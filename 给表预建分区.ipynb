{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8c1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime,timedelta\n",
    "start=datetime(2020,12,30)\n",
    "end=datetime(2022,1,1)\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "import pyspark.sql.functions as F\n",
    "# \",\n",
    "# tablemap={\"arkham_stream_etl\":\"arkham\",\"credinex_account_stream_etl\":\"credinex_account\",\n",
    "#           \"credinex_hive_stream_etl\":\"credinex_hive\",\"credinex_repeater_stream_etl\":\"credinex_repeater\",\"protoss_stream_etl\":\"protoss\"} #,\n",
    "tablemap={\n",
    "    \"banda_stream_etl\":\"banda\",\n",
    "#     \"lovina_stream_etl\":\"lovina\",\n",
    "#     \"coupon_stream_etl\":\"coupon\",\n",
    "#     \"notification_stream_etl\":\"notification\",\n",
    "#     \"telemarket_stream_etl\":\"telemarket\",\n",
    "#     \"arkham_stream_etl\":\"arkham\",\n",
    "#     \"credinex_account_stream_etl\":\"credinex_account\",\n",
    "#     \"credinex_hive_stream_etl\":\"credinex_hive\",\n",
    "#     \"credinex_repeater_stream_etl\":\"credinex_repeater\",\n",
    "#     \"protoss_stream_etl\":\"protoss\"\n",
    "} #,\n",
    "databases=spark.sql(\"show databases\")\n",
    "dateList=[]\n",
    "def gen_dates(b_date, days):\n",
    "    day = timedelta(days=1)\n",
    "    for i in range(days):\n",
    "        yield b_date + day*i\n",
    "def getDateList(start=None, end=None):\n",
    "    \"\"\"\n",
    "    获取日期列表\n",
    "    :param start: 开始日期\n",
    "    :param end: 结束日期\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if start is None:\n",
    "        start = datetime.strptime(\"2000-01-01\", \"%Y-%m-%d\")\n",
    "    if end is None:\n",
    "        end = datetime.now()\n",
    "    data = []\n",
    "#     partitionsql = \"ALTER TABLE `\" + database+ \"`.\" + tableNm+ \"  ADD IF NOT EXISTS \"\n",
    "    for d in gen_dates(start, (end-start).days):\n",
    "        data.append(d.strftime('%Y-%m-%d'))\n",
    "#         datelist=d.strftime('%Y-%m-%d').split(\"-\")\n",
    "#         partition=\" PARTITION (year=\" + datelist[0] + \",month=\" + datelist[1] + \",day=\" + datelist[2] + \") \"\n",
    "#         partitionsql=partitionsql+partition\n",
    "#         spark.sql(partitionsql)\n",
    "    return data\n",
    "dateList=getDateList(start,end)\n",
    "databases=databases.collect()\n",
    "def getpartitionSql(database,tableNm):\n",
    "    partitionsql = \"ALTER TABLE `\" + database+ \"`.\" + tableNm+ \"  ADD IF NOT EXISTS \"\n",
    "    for date in dateList:\n",
    "        datels=date.split(\"-\")\n",
    "        partition=\" PARTITION (year=\" + datels[0] + \",month=\" + datels[1] + \",day=\" + datels[2] + \") \"\n",
    "        partitionsql=partitionsql+partition\n",
    "    return partitionsql\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Demo\") \\\n",
    "    .config(\"hive.metastore.client.factory.class\",\n",
    "            \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"hive.exec.dynamic.partition\",\"true\");\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "for databaseName in tablemap:\n",
    "    databasesql=\"show tables in \"+databaseName\n",
    "    tables=spark.sql(databasesql)\n",
    "    tablelist=tables.collect();\n",
    "    for row in tablelist:\n",
    "        tableName=row[\"tableName\"]\n",
    "        if tableName=='t_password_update_record':\n",
    "            print(tableName)\n",
    "    #             tableName=row[\"tableName\"]\n",
    "            sql=getpartitionSql(databaseName,tableName)\n",
    "    #         print(sql)\n",
    "            spark.sql(sql)\n",
    "    #             setPartitions(start,end,\"partitions_etl\",tableName)\n",
    "    #                 spark.catalog.refreshTable(databaseName+\".\"+tableName)\n",
    "    #                 sql1=\"SELECT * FROM \"+databaseName+\".\"+tableName\n",
    "    #                 path=\"s3://rupiahplus-data-warehouse/stream/demo/\"+tableName;\n",
    "    #                 spark.sql(sql1).write.mode(\"overwrite\").partitionBy(\"year\",\"month\",\"day\").orc(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
