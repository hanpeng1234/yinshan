{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2e7739335443beb0a85cebe53d74f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "import copy\n",
    "dbmap={\n",
    "#     \"banda\":{\n",
    "#         \"increment_database\":\"banda_stream_etl\",\n",
    "#         \"database\":\"banda-etl-s3\",\n",
    "#         \"realTime_path\":\"banda\"\n",
    "#     },\n",
    "#     \"luzon\":{\n",
    "#          \"increment_database\":\"luzon_stream_etl\",\n",
    "#          \"database\":\"luzon_etl_s3\",\n",
    "#          \"realTime_path\":\"luzon\"\n",
    "#     },\n",
    "#     \"telemarket\":{\n",
    "#          \"increment_database\":\"telemarket_stream_etl\",\n",
    "#          \"database\":\"telemarket_etl_s3\",\n",
    "#          \"realTime_path\":\"telemarket\"\n",
    "#     },\n",
    "#      \"coupon\":{\n",
    "#         \"increment_database\":\"coupon_stream_etl\",\n",
    "#         \"database\":\"coupon_etl_s3\",\n",
    "#         \"realTime_path\":\"coupon\"\n",
    "#     },\n",
    "#     \"notification\":{\n",
    "#         \"increment_database\":\"notification_stream_etl\",\n",
    "#         \"database\":\"notification_etl_s3\",\n",
    "#         \"realTime_path\":\"notification\"\n",
    "#     },\n",
    "#     \"arkham\":{\n",
    "#         \"increment_database\":\"arkham_stream_etl\",\n",
    "#         \"database\":\"arkham_etl_s3\",\n",
    "#         \"realTime_path\":\"arkham\"\n",
    "#     },\n",
    "#     \"protoss\":{\n",
    "#         \"increment_database\":\"protoss_stream_etl\",\n",
    "#         \"database\":\"protoss_etl_s3\",\n",
    "#         \"realTime_path\":\"protoss\"\n",
    "#     },\n",
    "#     \"credinex_account\":{\n",
    "#         \"increment_database\":\"credinex_account_stream_etl\",\n",
    "#         \"database\":\"credinex_account_etl_s3\",\n",
    "#         \"realTime_path\":\"credinex_account\"\n",
    "#     },\n",
    "#     \"credinex_repeater\":{\n",
    "#         \"increment_database\":\"credinex_repeater_stream_etl\",\n",
    "#         \"database\":\"credinex_repeater_etl_s3\",\n",
    "#         \"realTime_path\":\"credinex_repeaters\"\n",
    "#     },\n",
    "#     \"credinex_hive\":{\n",
    "#         \"increment_database\":\"credinex_hive_stream_etl\",\n",
    "#         \"database\":\"credinex_hive_etl_s3\",\n",
    "#         \"realTime_path\":\"credinex_hive\"\n",
    "#     },\n",
    "#     \"lovina\":{\n",
    "#         \"increment_database\":\"lovina_stream_etl\",\n",
    "#         \"database\":\"lovina_etl_s3\",\n",
    "#         \"realTime_path\":\"lovina\"\n",
    "#     }\n",
    "}\n",
    "# increment_database=\"banda_stream_etl\"\n",
    "# # tableNm=\"t_loan_app\"\n",
    "# database=\"banda-etl-s3\"\n",
    "nowdate=(datetime.now()).strftime('%Y-%m-%d')\n",
    "yesterday=(datetime.now()+ timedelta(-1)).strftime('%Y-%m-%d')\n",
    "sqltemp=\"\"\"\n",
    "CREATE OR REPLACE VIEW {tableNm}_stream AS \n",
    "with {update_snapshot_table} as (\n",
    "  select *\n",
    "    from (\n",
    "      SELECT * ,\n",
    "         row_number() OVER (PARTITION BY id ORDER BY parse_datetime(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "      FROM \"{increment_database}\".\"{tableNm}\"\n",
    "      where date(year || '-' || month || '-' || day) = current_date\n",
    "      )\n",
    "    where row_num = 1\n",
    "  )\n",
    "select  {select_sql} \n",
    "from \"{database}\".\"{tableNm}\" l\n",
    "left join {update_snapshot_table}\n",
    "on l.id = {update_snapshot_table}.id\n",
    "where if({update_snapshot_table}.kind is null ,'', {update_snapshot_table}.kind )<> 'delete'\n",
    "union\n",
    "select {update_snapshot_sql} from (\n",
    "  select *\n",
    "  from \"{increment_database}\".\"{tableNm}\"\n",
    "  where\n",
    "    date(year || '-' || month || '-' || day) = current_date \n",
    "    and kind = 'insert'\n",
    "  ) new\n",
    "left join {update_snapshot_table}\n",
    "on new.id = {update_snapshot_table}.id  where if({update_snapshot_table}.kind is null ,'', {update_snapshot_table}.kind )<> 'delete'; \"\"\"\n",
    "def getTableColum(b,update_snapshot_table):\n",
    "    colum=\"\"\n",
    "    snapshot_colum=\"\"\n",
    "    sqlstrtemp=\"if({update_snapshot_table}.id is null, {colNm}     , {snapshot_colNm})  {ascolNm} ,\"\n",
    "    for index in range(len(b)):\n",
    "        if(index==1 or (index>2 and index<len(b)-8)):   \n",
    "            sqlstr=copy.copy(sqlstrtemp)\n",
    "            colum=colum+sqlstr.format(colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],False),snapshot_colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],True),ascolNm=b[index][\"col_name\"],update_snapshot_table=update_snapshot_table)\n",
    "            snapshot_colum=snapshot_colum+setSnapshotDef(b[index][\"data_type\"],b[index][\"col_name\"]).format(update_snapshot_table=update_snapshot_table)+\" ,\"\n",
    "    return colum[0:len(colum)-2],snapshot_colum[0:len(snapshot_colum)-2]\n",
    "def setDef(type,table_col,is_snapshot):\n",
    "    if is_snapshot:\n",
    "        return \"{update_snapshot_table}.\"+table_col\n",
    "    else:\n",
    "        return \"l.\"+table_col\n",
    "def setSnapshotDef(type,table_col):\n",
    "    return \"{update_snapshot_table}.\"+table_col\n",
    "def execute(tablerow,dbtype):\n",
    "#     print(tablerow,dbtype)\n",
    "    tableNm=tablerow[\"tableName\"]\n",
    "#     if tableNm=='t_user_install_info':\n",
    "    update_snapshot_table=\"update_snapshot_\"+dbtype+\"_\"+tableNm\n",
    "    spark.catalog.refreshTable(\"`\"+dbmap[dbtype][\"database\"]+\"`.\"+tableNm)\n",
    "    col=spark.sql(\"desc `\"+dbmap[dbtype][\"increment_database\"]+\"`.\"+tableNm).collect()\n",
    "    #     #真正的列,增量的列\n",
    "    real_col,snapshot_col=getTableColum(col,update_snapshot_table)\n",
    "#     sql=copy.copy(sqltemp)\n",
    "#     real_sql=sql.format(increment_database=dbmap[dbtype][\"increment_database\"],tableNm=tableNm,nowdate=nowdate,select_sql=real_col.format(update_snapshot_table=update_snapshot_table),update_snapshot_sql=snapshot_col.format(update_snapshot_table=update_snapshot_table),database=dbmap[dbtype][\"database\"],yesterday=yesterday,update_snapshot_table=update_snapshot_table)\n",
    "#     tablepath=\"s3://rupiahplus-data-warehouse/aliyun/\"+dbmap[dbtype][\"realTime_path\"]+\"/\"+tableNm\n",
    "#     print(real_sql)\n",
    "#         spark.sql(real_sql).drop(\"etlindex\").write.mode(\"overwrite\").orc(tablepath)\n",
    "#         spark.sql(real_sql).where(\"id==1731052\").show()\n",
    "#     spark.sql(real_sql).drop(\"etlindex\").write.mode(\"overwrite\").format(\"orc\").saveAsTable(\"etl_s3_temp.\"+dbtype+\"_\"+tableNm);\n",
    "#     spark.sql(\"select  * from  \"+\"etl_s3_temp.\"+dbtype+\"_\"+tableNm).write.mode(\"overwrite\").orc(tablepath)\n",
    "# __main__\n",
    "if __name__ == \"builtins\":\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Python Demo\")\\\n",
    "        .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "    spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "    spark.conf.set(\"spark.files.overwrite\",\"true\")\n",
    "#     ,Apache Arrow：一个跨平台的在内存中以列式存储的数据层，用来加速大数据分析速度。其可以一次性传入更大块的数据，pyspark中已经有载入该模块，需要打开该设置：\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "#     创建表时自动删除已存在的目录\n",
    "    spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "    for dbtype in dbmap:\n",
    "        databasesql=\"show tables in \"+dbmap[dbtype][\"increment_database\"]\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "    #         for row in tablelist:\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(execute, table,dbtype) for table in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e3b7000170400f9d749bc651f6ad23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1607423952566_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-6-114.ap-southeast-1.compute.internal:20888/proxy/application_1607423952566_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-3-235.ap-southeast-1.compute.internal:8042/node/containerlogs/container_1607423952566_0004_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Collecting pyathena\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/27/53fc42c07c8ccee31173e37599bb63dc6d53ade00660d12cf3cb83e1d1e3/PyAthena-2.0.0-py3-none-any.whl\n",
      "Collecting tenacity>=4.1.0 (from pyathena)\n",
      "  Downloading https://files.pythonhosted.org/packages/b5/05/ff089032442058bd3386f9cd991cd88ccac81dca1494d78751621ee35e62/tenacity-6.2.0-py2.py3-none-any.whl\n",
      "Collecting boto3>=1.4.4 (from pyathena)\n",
      "Collecting botocore>=1.5.52 (from pyathena)\n",
      "  Using cached https://files.pythonhosted.org/packages/95/df/b4268d55a03654ca97a33b58cd681fb8ccbc7040c7604a5b465c38f17bc4/botocore-1.19.31-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/site-packages (from tenacity>=4.1.0->pyathena)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3>=1.4.4->pyathena)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3>=1.4.4->pyathena)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore>=1.5.52->pyathena)\n",
      "  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl\n",
      "Collecting urllib3<1.27,>=1.25.4; python_version != \"3.4\" (from botocore>=1.5.52->pyathena)\n",
      "  Using cached https://files.pythonhosted.org/packages/f5/71/45d36a8df68f3ebb098d6861b2c017f3d094538c0fb98fa61d4dc43e69b9/urllib3-1.26.2-py2.py3-none-any.whl\n",
      "Installing collected packages: tenacity, python-dateutil, urllib3, botocore, s3transfer, boto3, pyathena\n",
      "Successfully installed boto3-1.16.31 botocore-1.19.31 pyathena-2.0.0 python-dateutil-2.8.1 s3transfer-0.3.3 tenacity-6.2.0 urllib3-1.26.2\n",
      "\n",
      "Collecting pandas\n",
      "  Downloading https://files.pythonhosted.org/packages/fd/70/e8eee0cbddf926bf51958c7d6a86bc69167c300fa2ba8e592330a2377d1b/pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5MB)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib64/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /mnt/tmp/1607424639557-0/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.1.5\n",
      "\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498352\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498386\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498397\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498406\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498416\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498426\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498435\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498445\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498454\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498464\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498473\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498483\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498493\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498502\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498518\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498537\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498554\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498571\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498581\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498590\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498600\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498609\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498618\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498627\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498636\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498645\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498654\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498663\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498672\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498681\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498691\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498700\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498709\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498718\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498727\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498736\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498745\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498754\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498763\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498773\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498782\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498791\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498804\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498822\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498842\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498853\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498862\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498871\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498881\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498890\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498899\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498908\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498917\n",
      "None\n",
      "end----- 2020-12-08 10:52:24.498927\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648531\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648557\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648568\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648578\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648591\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648601\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648610\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648619\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648629\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648638\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648648\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648657\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648666\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648676\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648685\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648694\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648704\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648713\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648722\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648732\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648741\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648750\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648760\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648770\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648779\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648789\n",
      "None\n",
      "end----- 2020-12-08 10:52:57.648798\n",
      "None\n",
      "end----- 2020-12-08 10:53:08.705973\n",
      "None\n",
      "end----- 2020-12-08 10:53:08.705997\n",
      "None\n",
      "end----- 2020-12-08 10:53:08.706008\n",
      "None\n",
      "end----- 2020-12-08 10:53:08.706018\n",
      "None\n",
      "end----- 2020-12-08 10:53:08.706027\n",
      "None\n",
      "end----- 2020-12-08 10:53:08.706037\n",
      "None\n",
      "end----- 2020-12-08 10:53:08.706050\n",
      "None\n",
      "end----- 2020-12-08 10:53:08.706059\n",
      "None\n",
      "end----- 2020-12-08 10:53:13.746291\n",
      "None\n",
      "end----- 2020-12-08 10:53:13.746315\n",
      "None\n",
      "end----- 2020-12-08 10:53:13.746326\n",
      "None\n",
      "end----- 2020-12-08 10:53:22.321652\n",
      "None\n",
      "end----- 2020-12-08 10:53:22.321677\n",
      "None\n",
      "end----- 2020-12-08 10:53:22.321688\n",
      "None\n",
      "end----- 2020-12-08 10:53:22.321698\n",
      "None\n",
      "end----- 2020-12-08 10:53:22.321708\n",
      "None\n",
      "end----- 2020-12-08 10:53:22.321717\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.718927\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.718957\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.718968\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.718978\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.718987\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.718997\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.719006\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.719016\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.719025\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.719035\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.719044\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.719054\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.719064\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.719073\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.719083\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.719092\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.719102\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.719112\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.719121\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.719130\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.719140\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.719149\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.719159\n",
      "None\n",
      "end----- 2020-12-08 10:53:51.719169\n",
      "None\n",
      "end----- 2020-12-08 10:53:58.765618\n",
      "None\n",
      "end----- 2020-12-08 10:53:58.765641\n",
      "None\n",
      "end----- 2020-12-08 10:53:58.765652\n",
      "None\n",
      "end----- 2020-12-08 10:54:11.162156\n",
      "None\n",
      "end----- 2020-12-08 10:54:11.162181\n",
      "None\n",
      "end----- 2020-12-08 10:54:11.162191\n",
      "None\n",
      "end----- 2020-12-08 10:54:11.162201\n",
      "None\n",
      "end----- 2020-12-08 10:54:11.162211\n",
      "None\n",
      "end----- 2020-12-08 10:54:11.162220\n",
      "None\n",
      "end----- 2020-12-08 10:54:11.162229\n",
      "None\n",
      "end----- 2020-12-08 10:54:11.162239\n",
      "None\n",
      "end----- 2020-12-08 10:54:11.162248\n",
      "None\n",
      "end----- 2020-12-08 10:54:17.319307\n",
      "None\n",
      "end----- 2020-12-08 10:54:17.319332\n",
      "None\n",
      "end----- 2020-12-08 10:54:17.319343\n",
      "None\n",
      "end----- 2020-12-08 10:54:17.319352\n",
      "None\n",
      "end----- 2020-12-08 10:54:24.638329\n",
      "None\n",
      "end----- 2020-12-08 10:54:24.638356\n",
      "None\n",
      "end----- 2020-12-08 10:54:24.638367\n",
      "None\n",
      "end----- 2020-12-08 10:54:24.638377\n",
      "None\n",
      "end----- 2020-12-08 10:54:24.638387\n",
      "None\n",
      "end----- 2020-12-08 10:54:28.384023\n",
      "None\n",
      "end----- 2020-12-08 10:54:28.384048\n",
      "Cannot uninstall requirement pyathena, not installed\n",
      "\n",
      "Cannot uninstall requirement pandas, not installed"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime,timedelta\n",
    "import copy\n",
    "import json\n",
    "import pytz\n",
    "import hmac\n",
    "def install():\n",
    "    sc.uninstall_package('pyathena')\n",
    "    sc.uninstall_package('pandas')\n",
    "    sc.install_pypi_package(\"pyathena\")\n",
    "    sc.install_pypi_package(\"pandas\")\n",
    "install()\n",
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "from hashlib import sha256 \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import types as T\n",
    "conn = connect(aws_access_key_id='AKIA4OO3YD6F6H3BAHEZ',\n",
    "               aws_secret_access_key='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',\n",
    "               s3_staging_dir='s3://aws-athena-query-results-ap-southeast-1-855696220043',\n",
    "               region_name='ap-southeast-1')\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Python Demo\")\\\n",
    "    .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "spark.conf.set(\"spark.files.overwrite\",\"true\")\n",
    "#     ,Apache Arrow：一个跨平台的在内存中以列式存储的数据层，用来加速大数据分析速度。其可以一次性传入更大块的数据，pyspark中已经有载入该模块，需要打开该设置：\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "#     创建表时自动删除已存在的目录\n",
    "spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "dbmap={\n",
    "      \"banda\":{\n",
    "        \"increment_database\":\"banda_stream_etl\",\n",
    "        \"database\":\"banda-etl-s3\",\n",
    "        \"realTime_path\":\"banda\"\n",
    "    },\n",
    "    \"luzon\":{\n",
    "         \"increment_database\":\"luzon_stream_etl\",\n",
    "         \"database\":\"luzon_etl_s3\",\n",
    "         \"realTime_path\":\"luzon\"\n",
    "    },\n",
    "    \"telemarket\":{\n",
    "         \"increment_database\":\"telemarket_stream_etl\",\n",
    "         \"database\":\"telemarket_etl_s3\",\n",
    "         \"realTime_path\":\"telemarket\"\n",
    "    },\n",
    "     \"coupon\":{\n",
    "        \"increment_database\":\"coupon_stream_etl\",\n",
    "        \"database\":\"coupon_etl_s3\",\n",
    "        \"realTime_path\":\"coupon\"\n",
    "    },\n",
    "    \"notification\":{\n",
    "        \"increment_database\":\"notification_stream_etl\",\n",
    "        \"database\":\"notification_etl_s3\",\n",
    "        \"realTime_path\":\"notification\"\n",
    "    },\n",
    "    \"arkham\":{\n",
    "        \"increment_database\":\"arkham_stream_etl\",\n",
    "        \"database\":\"arkham_etl_s3\",\n",
    "        \"realTime_path\":\"arkham\"\n",
    "    },\n",
    "    \"protoss\":{\n",
    "        \"increment_database\":\"protoss_stream_etl\",\n",
    "        \"database\":\"protoss_etl_s3\",\n",
    "        \"realTime_path\":\"protoss\"\n",
    "    },\n",
    "    \"credinex_account\":{\n",
    "        \"increment_database\":\"credinex_account_stream_etl\",\n",
    "        \"database\":\"credinex_account_etl_s3\",\n",
    "        \"realTime_path\":\"credinex_account\"\n",
    "    },\n",
    "    \"credinex_repeater\":{\n",
    "        \"increment_database\":\"credinex_repeater_stream_etl\",\n",
    "        \"database\":\"credinex_repeater_etl_s3\",\n",
    "        \"realTime_path\":\"credinex_repeaters\"\n",
    "    },\n",
    "    \"credinex_hive\":{\n",
    "        \"increment_database\":\"credinex_hive_stream_etl\",\n",
    "        \"database\":\"credinex_hive_etl_s3\",\n",
    "        \"realTime_path\":\"credinex_hive\"\n",
    "    },\n",
    "    \"lovina\":{\n",
    "        \"increment_database\":\"lovina_stream_etl\",\n",
    "        \"database\":\"lovina_etl_s3\",\n",
    "        \"realTime_path\":\"lovina\"\n",
    "    }\n",
    "}\n",
    "nowdate=(datetime.now()).strftime('%Y-%m-%d')\n",
    "yesterday=(datetime.now()+ timedelta(-1)).strftime('%Y-%m-%d')\n",
    "# where date(year || '-' || month || '-' || day) <=date(now()) and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "sqltemp=\"\"\"\n",
    "CREATE OR REPLACE VIEW \"{database}\".{tableNm}_stream AS \n",
    "with {update_snapshot_table} as (\n",
    "  select *\n",
    "    from (\n",
    "      SELECT * ,\n",
    "         row_number() OVER (PARTITION BY id ORDER BY parse_datetime(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "         FROM \"{increment_database}\".\"{tableNm}\"\n",
    "         where date(year || '-' || month || '-' || day) <=date(now()) and date(year || '-' || month || '-' || day) >= date(now() - INTERVAL  '1' DAY)\n",
    "      )\n",
    "    where row_num = 1\n",
    "  )\n",
    "select  {select_sql} \n",
    "from \"{database}\".\"{tableNm}\" l\n",
    "left join {update_snapshot_table}\n",
    "on l.id = {update_snapshot_table}.id\n",
    "where if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete'\n",
    "union\n",
    "select {update_snapshot_sql} from (\n",
    "  select *\n",
    "  from \"{increment_database}\".\"{tableNm}\"\n",
    "  where\n",
    "    date(year || '-' || month || '-' || day) <=date(now()) and date(year || '-' || month || '-' || day) >= date(now() - INTERVAL  '1' DAY)\n",
    "    and kind = 'insert'\n",
    "  ) new\n",
    "left join {update_snapshot_table}\n",
    "on new.id = {update_snapshot_table}.id where if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete' \"\"\"\n",
    "def get_secret_obj():\n",
    "    df=spark.read.text(\"s3://rupiahplus-configs/etl/data_secrt/col.json\").collect()\n",
    "    keymap=''\n",
    "    for  row in df:\n",
    "        keymap=keymap+row['value']\n",
    "    json_content=json.loads(keymap)\n",
    "    return json_content\n",
    "colmap= get_secret_obj()   \n",
    "def getTableColum(b,update_snapshot_table,dbtype,tableNm):\n",
    "    colum=\"\"\n",
    "    snapshot_colum=\"\"\n",
    "    sqlstrtemp=\"if({update_snapshot_table}.id is null, {colNm}     , {snapshot_colNm})  {ascolNm} ,\"\n",
    "    for index in range(len(b)):\n",
    "        if((index==1 or (index>2 and index<len(b)-7)) and (colmap.get(dbtype)==None or colmap.get(dbtype)!=None and (colmap[dbtype].get(tableNm)==None or colmap[dbtype].get(tableNm)!=None  and \n",
    "                                                          b[index][\"col_name\"] not in colmap[dbtype].get(tableNm)))) :\n",
    "            sqlstr=copy.copy(sqlstrtemp)\n",
    "            colum=colum+sqlstr.format(colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],False),snapshot_colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],True),ascolNm=b[index][\"col_name\"],update_snapshot_table=update_snapshot_table)\n",
    "            snapshot_colum=snapshot_colum+setSnapshotDef(b[index][\"data_type\"],b[index][\"col_name\"]).format(update_snapshot_table=update_snapshot_table)+\" ,\"\n",
    "    return colum[0:len(colum)-2],snapshot_colum[0:len(snapshot_colum)-2]\n",
    "def setDef(type,table_col,is_snapshot):\n",
    "    if is_snapshot:\n",
    "        return \"{update_snapshot_table}.\"+table_col\n",
    "    else:\n",
    "        return \"l.\"+table_col\n",
    "def setSnapshotDef(type,table_col):\n",
    "    return \"{update_snapshot_table}.\"+table_col\n",
    "def execute(tablerow,dbtype):\n",
    "    tableName=tablerow\n",
    "    update_snapshot_table=\"update_snapshot_\"+dbtype+\"_\"+tableName\n",
    "    col=spark.sql(\"desc `\"+dbmap[dbtype][\"increment_database\"]+\"`.\"+tableName).collect()\n",
    "    real_col,snapshot_col=getTableColum(col,update_snapshot_table,dbtype,tableName)\n",
    "    sql=copy.copy(sqltemp)\n",
    "    real_sql=sql.format(increment_database=dbmap[dbtype][\"increment_database\"],tableNm=tableName,nowdate=nowdate,select_sql=real_col.format(update_snapshot_table=update_snapshot_table),update_snapshot_sql=snapshot_col.format(update_snapshot_table=update_snapshot_table),database=dbmap[dbtype][\"database\"],yesterday=yesterday,update_snapshot_table=update_snapshot_table)\n",
    "#     print(real_sql)\n",
    "    pd.read_sql(real_sql, conn)\n",
    "if __name__ == \"builtins\":\n",
    "    for dbtype in dbmap:\n",
    "        databasesql=\"show tables in \"+dbmap[dbtype][\"increment_database\"]\n",
    "        df=pd.read_sql(databasesql, conn)\n",
    "        tablelist=df[\"tab_name\"]\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(execute, table,dbtype) for table in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())\n",
    "                print(\"end-----\",datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485dcb03b74d495a859b449e7084e2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling pyathena-2.0.0:\n",
      "  Successfully uninstalled pyathena-2.0.0\n",
      "\n",
      "Uninstalling pandas-1.1.4:\n",
      "  Successfully uninstalled pandas-1.1.4\n",
      "\n",
      "Collecting pyathena\n",
      "  Using cached https://files.pythonhosted.org/packages/f5/27/53fc42c07c8ccee31173e37599bb63dc6d53ade00660d12cf3cb83e1d1e3/PyAthena-2.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: tenacity>=4.1.0 in /mnt/tmp/1606810797628-0/lib/python3.7/site-packages (from pyathena)\n",
      "Requirement already satisfied: boto3>=1.4.4 in /mnt/tmp/1606810797628-0/lib/python3.7/site-packages (from pyathena)\n",
      "Requirement already satisfied: botocore>=1.5.52 in /mnt/tmp/1606810797628-0/lib/python3.7/site-packages (from pyathena)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/site-packages (from tenacity>=4.1.0->pyathena)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3>=1.4.4->pyathena)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /mnt/tmp/1606810797628-0/lib/python3.7/site-packages (from boto3>=1.4.4->pyathena)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /mnt/tmp/1606810797628-0/lib/python3.7/site-packages (from botocore>=1.5.52->pyathena)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /mnt/tmp/1606810797628-0/lib/python3.7/site-packages (from botocore>=1.5.52->pyathena)\n",
      "Installing collected packages: pyathena\n",
      "Successfully installed pyathena-2.0.0\n",
      "\n",
      "Collecting pandas\n",
      "  Using cached https://files.pythonhosted.org/packages/bf/4c/cb7da76f3a5e077e545f9cf8575b8f488a4e8ad60490838f89c5cdd5bb57/pandas-1.1.4-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib64/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /mnt/tmp/1606810797628-0/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.1.4\n",
      "\n",
      "                    etldate        id  ...               instance  year\n",
      "0   2020-11-30 17:05:13.168  20662649  ...  job-instance-adapundi  2020\n",
      "1   2020-11-30 17:05:39.131  20662676  ...  job-instance-adapundi  2020\n",
      "2   2020-11-30 17:05:44.133  20662681  ...  job-instance-adapundi  2020\n",
      "3   2020-11-30 17:05:45.124  20662682  ...  job-instance-adapundi  2020\n",
      "4   2020-11-30 17:06:29.771  20662706  ...     job-instance-1-245  2020\n",
      "..                      ...       ...  ...                    ...   ...\n",
      "95  2020-12-01 03:02:30.312  20673508  ...     job-instance-1-245  2020\n",
      "96  2020-12-01 03:05:09.161  20673531  ...  job-instance-adapundi  2020\n",
      "97  2020-12-01 03:05:19.216  20673542  ...  job-instance-adapundi  2020\n",
      "98  2020-12-01 03:05:37.180  20673561  ...  job-instance-adapundi  2020\n",
      "99  2020-12-01 03:06:29.720  20673591  ...     job-instance-1-245  2020\n",
      "\n",
      "[100 rows x 11 columns]"
     ]
    }
   ],
   "source": [
    "def install():\n",
    "    sc.uninstall_package('pyathena')\n",
    "    sc.uninstall_package('pandas')\n",
    "    sc.install_pypi_package(\"pyathena\")\n",
    "    sc.install_pypi_package(\"pandas\")\n",
    "install()\n",
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "conn = connect(aws_access_key_id='AKIA4OO3YD6F6H3BAHEZ',\n",
    "               aws_secret_access_key='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',\n",
    "               s3_staging_dir='s3://aws-athena-query-results-ap-southeast-1-855696220043',\n",
    "               region_name='ap-southeast-1')\n",
    "sql_01=\"\"\"\n",
    "select  * from \"banda-etl-s3\".t_job_metric  limit 100\n",
    "\"\"\"\n",
    "pd.read_sql(sql_01, conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3d406db9bc4a96b6e5f730b60ef5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{'t_ai_rudder_schedule_mobile': ['mobile'], 't_mobile_tag_rel': ['mobile'], 't_code': ['send_to'], 't_personal_info': ['credential_no', 'backup_mobile'], 't_review_blacklist': ['value'], 't_contact': ['mobile'], 't_customer': ['mobile'], 't_employment': ['company_phone'], 't_loan_app': ['credential_no'], 't_login_log': ['mobile'], 't_record_personal_info': ['credential_no', 'backup_mobile'], 't_record_employment': ['company_phone'], 't_record_contact': ['mobile'], 't_record_file': ['path'], 't_sms': ['sendto']}"
     ]
    }
   ],
   "source": [
    "a=['3','4','5']\n",
    "print('6' not in a )\n",
    "\n",
    "print(colmap.get(\"banda\"))\n",
    "b=[0 for i in range(20)]\n",
    "index=3\n",
    "dbtype='banda'\n",
    "tableNm='t_admin'\n",
    "if((index==1 or (index>2 and index<len(b)-7)) and (colmap.get(dbtype)==None or\n",
    "                                                         ( colmap.get(dbtype)!=None and colmap[dbtype].get(tableNm)!=None  and \n",
    "                                                          b[index][\"col_name\"] not in colmap[dbtype].get(tableNm)))):\n",
    "    \n",
    "    print(true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
