{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9a445e2135459cac6e57e3ca5b61f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling boto3-1.17.16:\n",
      "  Successfully uninstalled boto3-1.17.16\n",
      "\n",
      "Collecting boto3\n",
      "  Using cached https://files.pythonhosted.org/packages/c2/03/18184037cb21cab227e392962e0ba9a7596d777a08d7c07c2d3640f939bf/boto3-1.17.16-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.16 in /mnt/tmp/1614332935128-0/lib/python3.7/site-packages (from boto3)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /mnt/tmp/1614332935128-0/lib/python3.7/site-packages (from boto3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /mnt/tmp/1614332935128-0/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.16->boto3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /mnt/tmp/1614332935128-0/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.16->boto3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.16->boto3)\n",
      "Installing collected packages: boto3\n",
      "Successfully installed boto3-1.17.16\n",
      "\n",
      "<function hmac_sha256 at 0x7effdd2f9440>\n",
      "notification_stream_etl\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#notification导表到notification_etl_s3\n",
    "def install():\n",
    "    sc.uninstall_package('boto3')\n",
    "    sc.install_pypi_package(\"boto3\")\n",
    "install()\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "import boto3\n",
    "import json\n",
    "import pytz\n",
    "import hmac\n",
    "import pyspark.sql.functions as F\n",
    "from hashlib import sha256 \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# print(datetime.now(pytz.timezone(\"Asia/Shanghai\")).strftime( '%Y-%m-%d'))\n",
    "today=(datetime.now()+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "yesterday=(datetime.now(pytz.timezone(\"Asia/Shanghai\"))+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "tablemap={\"notification_stream_etl\":\"notification\"} \n",
    "# {\"notification_stream_etl\":\"notification\"} #,\n",
    "# {\"telemarket_stream_etl\":\"telemarket\"} #,\n",
    "# ,\"banda_stream_etl\":\"1\"\n",
    "tablesql_1=\" FROM  (   SELECT    *  , row_number() OVER (PARTITION BY id  ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC  , if(etlindex is null ,0,etlindex)  desc)  row_num FROM  \"\n",
    "tablesql_2=\" ORDER BY id  ASC )  a  WHERE a.row_num = 1   AND a.kind <> 'delete' \"\n",
    "# 替换手机号\n",
    "def get_secret_obj():\n",
    "    df=spark.read.text(\"s3://rupiahplus-configs/etl/data_secrt/col.json\").collect()\n",
    "    keymap=''\n",
    "    for  row in df:\n",
    "        keymap=keymap+row['value']\n",
    "    json_content=json.loads(keymap)\n",
    "    return json_content\n",
    "colmap= get_secret_obj()\n",
    "mobileType=colmap[\"mobileType\"]\n",
    "def getRelmobile(colNm,str):\n",
    "    if colNm in mobileType:\n",
    "        relmobile = str.strip().replace(\"+\", \"\").replace(\"-\", \"\").replace(\" \", \"\").replace('\"', '')\n",
    "        if (relmobile.startswith(\"62\")):\n",
    "            relmobile = relmobile.replace(\"62\", \"0\",1)\n",
    "        if relmobile.startswith(\"0\") == False:\n",
    "            relmobile = \"0\" + relmobile;\n",
    "        return relmobile\n",
    "    return str\n",
    "# hmac_256加密\n",
    "def hmac_sha256(key,colNm,value):\n",
    "    if value!=None:\n",
    "#         处理手机号\n",
    "        rtn=getRelmobile(colNm,value)\n",
    "        h = hmac.new(key.encode('utf-8'),digestmod=sha256)\n",
    "        h.update(rtn.encode('utf-8'))\n",
    "        h_str = h.hexdigest()\n",
    "        return h_str\n",
    "spark.udf.register(\"hmac_sha256\",hmac_sha256,T.StringType())\n",
    "def getTableColum(b):\n",
    "    colum=\"\"\n",
    "    for index in range(len(b)):\n",
    "        if(index>0 and index<len(b)-8):   \n",
    "            colum=colum+setDef(b[index][\"data_type\"],b[index][\"col_name\"])+\", \"\n",
    "    return colum[0:len(colum)-2]\n",
    "def setDef(type,table_col):\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull(\"+table_col+\",0) as \"+table_col\n",
    "    else:\n",
    "        return table_col\n",
    "def add_secret_col(dbType,tableNm,col):\n",
    "    secret_Col=[]\n",
    "    if colmap[tablemap[databaseName]]!=None and colmap[tablemap[databaseName]].get(tableNm)!=None:\n",
    "        secret_Col=colmap[tablemap[databaseName]][tableNm]\n",
    "    sh256Col =col\n",
    "    hmac_key=colmap[\"hmac_key\"]\n",
    "#     sql=\"select hmac_sha256('{hmac_key}','123456789') as sh2\".format(hmac_key=hmac_key)\n",
    "    for i in secret_Col:\n",
    "        sh256Col=sh256Col+ \", hmac_sha256('{hmac_key}','\"+i+\"', \"+i+\") \"+ i+\"_x\"\n",
    "    return  sh256Col.format(hmac_key=hmac_key)\n",
    "def make_col_temptable(databaseName,row):\n",
    "    tableName=row[\"tableName\"]\n",
    "    if tableName=='t_message':\n",
    "        tablecolum=\"desc \" +databaseName+\".\"+tableName;\n",
    "        tableSchema= spark.sql(tablecolum).collect()\n",
    "        tablepath=\"s3://rupiahplus-data-warehouse/aliyun/\"+tablemap[databaseName]+\"/\"+row[\"tableName\"]\n",
    "        spark.catalog.refreshTable(databaseName+\".\"+tableName)\n",
    "        secret_sql=add_secret_col(tablemap[databaseName],tableName,getTableColum(tableSchema))+\", year\"\n",
    "        sql=\"select \"+secret_sql+ tablesql_1+databaseName+\".\"+tableName+tablesql_2\n",
    "        df=spark.sql(sql).drop(\"etlindex\")\n",
    "        if colmap[tablemap[databaseName]].get(tableName)!=None :\n",
    "            for i in colmap[tablemap[databaseName]][tableName]:\n",
    "                df=df.withColumn(\"col_x\",F.col(i+\"_x\")).withColumn(i+\"_x\",F.col(i)).withColumn(i,F.col(\"col_x\"))\n",
    "        df.drop(\"col_x\").write.mode(\"overwrite\").partitionBy(\"year\").orc(tablepath)\n",
    "if __name__ == \"builtins\":\n",
    "#     注册udf\n",
    "    spark.udf.register(\"hmac_sha256\",hmac_sha256,T.StringType())\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Python Demo\")\\\n",
    "        .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "#     spark.conf.set(\"spark.scheduler.mode\",\"FAIR\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "    for databaseName in tablemap:\n",
    "        print(databaseName)\n",
    "        databasesql=\"show tables in \"+databaseName\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(make_col_temptable,databaseName, tableNm) for tableNm in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164ef8307d494f04a0312353090c2e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start----- 2021-02-26 09:57:25.016937\n",
      "with update_snapshot_notification_t_message as (\n",
      "  select *\n",
      "    from (\n",
      "      SELECT * ,\n",
      "         row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
      "         FROM `notification_stream_etl`.`t_message`\n",
      "         where date(year || '-' || month || '-' || day) <=date('2021-02-26') and date(year || '-' || month || '-' || day) >= date('2021-02-25')\n",
      "      )\n",
      "    where row_num = 1\n",
      "  )\n",
      "select  if(update_snapshot_notification_t_message.id is null, l.etldate     , update_snapshot_notification_t_message.etldate)  etldate ,if(update_snapshot_notification_t_message.id is null, l.id     , update_snapshot_notification_t_message.id)  id ,if(update_snapshot_notification_t_message.id is null, l.product_id     , update_snapshot_notification_t_message.product_id)  product_id ,if(update_snapshot_notification_t_message.id is null, l.ref_id     , update_snapshot_notification_t_message.ref_id)  ref_id ,if(update_snapshot_notification_t_message.id is null, l.type     , update_snapshot_notification_t_message.type)  type ,if(update_snapshot_notification_t_message.id is null, l.sub_type     , update_snapshot_notification_t_message.sub_type)  sub_type ,if(update_snapshot_notification_t_message.id is null, l.channel     , update_snapshot_notification_t_message.channel)  channel ,if(update_snapshot_notification_t_message.id is null, l.send_to_x     , update_snapshot_notification_t_message.send_to)  send_to ,if(update_snapshot_notification_t_message.id is null, l.status     , update_snapshot_notification_t_message.status)  status ,if(update_snapshot_notification_t_message.id is null, l.title     , update_snapshot_notification_t_message.title)  title ,if(update_snapshot_notification_t_message.id is null, l.content     , update_snapshot_notification_t_message.content)  content ,if(update_snapshot_notification_t_message.id is null, l.result_description     , update_snapshot_notification_t_message.result_description)  result_description ,if(update_snapshot_notification_t_message.id is null, l.expect_send_time     , update_snapshot_notification_t_message.expect_send_time)  expect_send_time ,if(update_snapshot_notification_t_message.id is null, l.actual_send_time     , update_snapshot_notification_t_message.actual_send_time)  actual_send_time ,if(update_snapshot_notification_t_message.id is null, l.create_time     , update_snapshot_notification_t_message.create_time)  create_time ,if(update_snapshot_notification_t_message.id is null, l.update_time     , update_snapshot_notification_t_message.update_time)  update_time ,if(update_snapshot_notification_t_message.id is null, l.avatar     , update_snapshot_notification_t_message.avatar)  avatar ,if(update_snapshot_notification_t_message.id is null, l.creative     , update_snapshot_notification_t_message.creative)  creative ,if(update_snapshot_notification_t_message.id is null, l.trigger_id     , update_snapshot_notification_t_message.trigger_id)  trigger_id ,if(update_snapshot_notification_t_message.id is null, l.trigger_type_id     , update_snapshot_notification_t_message.trigger_type_id)  trigger_type_id ,if(update_snapshot_notification_t_message.id is null, l.params     , update_snapshot_notification_t_message.params)  params , if(update_snapshot_notification_t_message.id is null, l.send_to     , hmac_sha256('xUIAZ3grtPOxaPNK','send_to',update_snapshot_notification_t_message.send_to)) send_to_x , if(update_snapshot_notification_t_message.id is null, l.year     , update_snapshot_notification_t_message.year)  year \n",
      "from `notification_etl_s3`.`t_message` l\n",
      "left join update_snapshot_notification_t_message\n",
      "on l.id = update_snapshot_notification_t_message.id\n",
      "where if(update_snapshot_notification_t_message.kind is not null,update_snapshot_notification_t_message.kind,'') <> 'delete'\n",
      "union\n",
      "select update_snapshot_notification_t_message.etldate ,update_snapshot_notification_t_message.id ,update_snapshot_notification_t_message.product_id ,update_snapshot_notification_t_message.ref_id ,update_snapshot_notification_t_message.type ,update_snapshot_notification_t_message.sub_type ,update_snapshot_notification_t_message.channel ,update_snapshot_notification_t_message.send_to ,update_snapshot_notification_t_message.status ,update_snapshot_notification_t_message.title ,update_snapshot_notification_t_message.content ,update_snapshot_notification_t_message.result_description ,update_snapshot_notification_t_message.expect_send_time ,update_snapshot_notification_t_message.actual_send_time ,update_snapshot_notification_t_message.create_time ,update_snapshot_notification_t_message.update_time ,update_snapshot_notification_t_message.avatar ,update_snapshot_notification_t_message.creative ,update_snapshot_notification_t_message.trigger_id ,update_snapshot_notification_t_message.trigger_type_id ,update_snapshot_notification_t_message.params ,hmac_sha256('xUIAZ3grtPOxaPNK','send_to', update_snapshot_notification_t_message.send_to) send_to_x ,update_snapshot_notification_t_message.year from (\n",
      "  select *\n",
      "  from `notification_stream_etl`.`t_message`\n",
      "  where\n",
      "    date(year || '-' || month || '-' || day) <=date('2021-02-26') and date(year || '-' || month || '-' || day) >= date('2021-02-25')\n",
      "    and kind = 'insert'\n",
      "  ) new\n",
      "left join update_snapshot_notification_t_message\n",
      "on new.id = update_snapshot_notification_t_message.id where if(update_snapshot_notification_t_message.kind is not null,update_snapshot_notification_t_message.kind,'') <> 'delete' \n",
      "None\n",
      "end----- 2021-02-26 09:57:26.066944\n",
      "None\n",
      "end----- 2021-02-26 09:57:26.066970\n",
      "None\n",
      "end----- 2021-02-26 09:57:26.066981\n",
      "None\n",
      "end----- 2021-02-26 09:57:26.066991\n",
      "None\n",
      "end----- 2021-02-26 09:57:26.067000\n",
      "None\n",
      "end----- 2021-02-26 09:57:26.067010"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime,timedelta\n",
    "import copy\n",
    "import json\n",
    "import pytz\n",
    "import hmac\n",
    "from hashlib import sha256 \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import types as T\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Python Demo\")\\\n",
    "    .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "spark.conf.set(\"spark.files.overwrite\",\"true\")\n",
    "#     ,Apache Arrow：一个跨平台的在内存中以列式存储的数据层，用来加速大数据分析速度。其可以一次性传入更大块的数据，pyspark中已经有载入该模块，需要打开该设置：\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "#     创建表时自动删除已存在的目录\n",
    "spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "dbmap={\n",
    "    \"notification\":{\n",
    "        \"increment_database\":\"notification_stream_etl\",\n",
    "        \"database\":\"notification_etl_s3\",\n",
    "        \"realTime_path\":\"notification\"\n",
    "    },\n",
    "#      \"telemarket\":{\n",
    "#          \"increment_database\":\"telemarket_stream_etl\",\n",
    "#          \"database\":\"telemarket_etl_s3\",\n",
    "#          \"realTime_path\":\"telemarket\"\n",
    "#     }\n",
    "}\n",
    "nowdate=(datetime.now()).strftime('%Y-%m-%d')\n",
    "yesterday=(datetime.now()+ timedelta(-1)).strftime('%Y-%m-%d')\n",
    "sqltemp=\"\"\"with {update_snapshot_table} as (\n",
    "  select *\n",
    "    from (\n",
    "      SELECT * ,\n",
    "         row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "         FROM `{increment_database}`.`{tableNm}`\n",
    "         where date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "      )\n",
    "    where row_num = 1\n",
    "  )\n",
    "select  {select_sql} \n",
    "from `{database}`.`{tableNm}` l\n",
    "left join {update_snapshot_table}\n",
    "on l.id = {update_snapshot_table}.id\n",
    "where if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete'\n",
    "union\n",
    "select {update_snapshot_sql} from (\n",
    "  select *\n",
    "  from `{increment_database}`.`{tableNm}`\n",
    "  where\n",
    "    date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "    and kind = 'insert'\n",
    "  ) new\n",
    "left join {update_snapshot_table}\n",
    "on new.id = {update_snapshot_table}.id where if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete' \"\"\"\n",
    "def get_secret_obj():\n",
    "    df=spark.read.text(\"s3://rupiahplus-configs/etl/data_secrt/col.json\").collect()\n",
    "    keymap=''\n",
    "    for  row in df:\n",
    "        keymap=keymap+row['value']\n",
    "    json_content=json.loads(keymap)\n",
    "    return json_content\n",
    "colmap= get_secret_obj()   \n",
    "hmac_key=colmap[\"hmac_key\"]\n",
    "mobileType=colmap[\"mobileType\"]\n",
    "def getRelmobile(colNm,str):\n",
    "    if colNm in mobileType:\n",
    "        relmobile = str.strip().replace(\"+\", \"\").replace(\"-\", \"\").replace(\" \", \"\").replace('\"', '')\n",
    "        if (relmobile.startswith(\"62\")):\n",
    "            relmobile = relmobile.replace(\"62\", \"0\",1)\n",
    "        if relmobile.startswith(\"0\") == False:\n",
    "            relmobile = \"0\" + relmobile;\n",
    "        return relmobile\n",
    "    return str\n",
    "# hmac_256加密\n",
    "def hmac_sha256(key,colNm,value):\n",
    "    if value!=None and value!='':\n",
    "#         处理手机号\n",
    "        rtn=getRelmobile(colNm,value)\n",
    "        h = hmac.new(key.encode('utf-8'),digestmod=sha256)\n",
    "        h.update(rtn.encode('utf-8'))\n",
    "        h_str = h.hexdigest()\n",
    "        return h_str\n",
    "spark.udf.register(\"hmac_sha256\",hmac_sha256,T.StringType())\n",
    "def getTableColum(b,update_snapshot_table,dbtype,tableNm):\n",
    "    colum=\"\"\n",
    "    snapshot_colum=\"\"\n",
    "    sqlstrtemp=\"if({update_snapshot_table}.id is null, {colNm}     , {snapshot_colNm})  {ascolNm} ,\"\n",
    "    sqlstr_secret_temp=\" if({update_snapshot_table}.id is null, {colNm}     , hmac_sha256('{hmac_key}','{ascolNm}',{snapshot_colNm})) {ascolNm}_x , \"\n",
    "    for index in range(len(b)):\n",
    "        if(index==1 or (index>2 and index<len(b)-7)):\n",
    "#             print(index,b[index])\n",
    "#             在year前面加上加密列\n",
    "            if(index==len(b)-8)and  colmap.get(dbtype)!=None and colmap[dbtype]!=None and colmap[dbtype].get(tableNm)!=None:\n",
    "                for secret_col in colmap[dbtype][tableNm]:\n",
    "                    sqlstr=copy.copy(sqlstr_secret_temp)\n",
    "                    colum=colum+sqlstr.format(colNm=setDef(\"string\",secret_col,False),snapshot_colNm=setDef(\"string\",secret_col,True),ascolNm=secret_col,update_snapshot_table=update_snapshot_table,hmac_key=hmac_key)\n",
    "                    snapshot_colum=snapshot_colum+setsecret_SnapshotDef(secret_col).format(update_snapshot_table=update_snapshot_table,hmac_key=hmac_key)+\" ,\"\n",
    "            sqlstr=copy.copy(sqlstrtemp)\n",
    "            colum=colum+sqlstr.format(colNm=isX(dbtype,tableNm,setDef(b[index][\"data_type\"],b[index][\"col_name\"],False)),snapshot_colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],True),ascolNm=b[index][\"col_name\"],update_snapshot_table=update_snapshot_table)\n",
    "            snapshot_colum=snapshot_colum+setSnapshotDef(b[index][\"data_type\"],b[index][\"col_name\"]).format(update_snapshot_table=update_snapshot_table)+\" ,\"\n",
    "    return colum[0:len(colum)-2],snapshot_colum[0:len(snapshot_colum)-2]\n",
    "def isX(type,table,column):\n",
    "    columnList=column.split(\".\")\n",
    "    if len(columnList)>0 and  colmap.get(type)!=None and colmap[type].get(table)!=None and columnList[len(columnList)-1] in colmap[type][table]:\n",
    "        return column+\"_x\"\n",
    "    return column\n",
    "def setDef(type,table_col,is_snapshot):\n",
    "    table_col_x=copy.copy(table_col)\n",
    "    if(type[:7] == 'decimal'):\n",
    "        if is_snapshot:\n",
    "            return \"ifnull({update_snapshot_table}.\"+table_col_x+\",0)  \"\n",
    "        else:\n",
    "            return \"ifnull(l.\"+table_col_x+\",0) \"\n",
    "    else:\n",
    "        if is_snapshot:\n",
    "            return \"{update_snapshot_table}.\"+table_col_x\n",
    "        else:\n",
    "            return \"l.\"+table_col_x\n",
    "def setSnapshotDef(type,table_col):\n",
    "    table_col_x=copy.copy(table_col)\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull({update_snapshot_table}.\"+table_col_x+\",0) \"+table_col\n",
    "    else:\n",
    "        return \"{update_snapshot_table}.\"+table_col_x\n",
    "def setsecret_SnapshotDef(table_col):\n",
    "    table_col_x=copy.copy(table_col)\n",
    "#     print(\"hmac_sha256('{hmac_key}','\"+table_col_x+\"', {update_snapshot_table}.\"+table_col_x+\") \"+ table_col_x+\"_x\")\n",
    "    return \"hmac_sha256('{hmac_key}','\"+table_col_x+\"', {update_snapshot_table}.\"+table_col_x+\") \"+ table_col_x+\"_x\"\n",
    "#     return \"sha2({update_snapshot_table}.\"+table_col+\",256) \" +table_col+\"_x\"\n",
    "def execute(tablerow,dbtype):\n",
    "#     print(tablerow,dbtype)\n",
    "    tableName=tablerow[\"tableName\"]\n",
    "    if tableName=='t_message':\n",
    "        update_snapshot_table=\"update_snapshot_\"+dbtype+\"_\"+tableName\n",
    "        spark.catalog.refreshTable(\"`\"+dbmap[dbtype][\"database\"]+\"`.\"+tableName)\n",
    "        col=spark.sql(\"desc `\"+dbmap[dbtype][\"increment_database\"]+\"`.\"+tableName).collect()\n",
    "        #     #真正的列,增量的列\n",
    "        real_col,snapshot_col=getTableColum(col,update_snapshot_table,dbtype,tableName)\n",
    "        sql=copy.copy(sqltemp)\n",
    "        real_sql=sql.format(increment_database=dbmap[dbtype][\"increment_database\"],tableNm=tableName,nowdate=nowdate,select_sql=real_col.format(update_snapshot_table=update_snapshot_table),update_snapshot_sql=snapshot_col.format(update_snapshot_table=update_snapshot_table),database=dbmap[dbtype][\"database\"],yesterday=yesterday,update_snapshot_table=update_snapshot_table,hmac_key=hmac_key)\n",
    "#         df=spark.sql(real_sql).drop(\"etlindex\")\n",
    "#         if colmap.get(dbtype)!=None and colmap[dbtype].get(tableName)!=None :\n",
    "#             for i in colmap[dbtype][tableName]:\n",
    "#                 df=df.withColumn(\"col_x\",F.col(i+\"_x\")).withColumn(i+\"_x\",F.col(i)).withColumn(i,F.col(\"col_x\"))\n",
    "#         df.drop(\"col_x\").write.mode(\"overwrite\").partitionBy(\"year\").format(\"orc\").saveAsTable(\"etl_s3_temp.\"+dbtype+\"_\"+tableName);\n",
    "#         tablepath='s3://rupiahplus-data-warehouse/aliyun/'+dbmap[dbtype][\"realTime_path\"]+'/'+tableName+\"/\"\n",
    "#         spark.sql(\"select  * from  \"+\"etl_s3_temp.\"+dbtype+\"_\"+tableName).write.mode(\"overwrite\").partitionBy(\"year\").orc(tablepath)\n",
    "        print(real_sql)\n",
    "if __name__ == \"builtins\":\n",
    "    print(\"start-----\",datetime.now())\n",
    "    for dbtype in dbmap:\n",
    "        databasesql=\"show tables in \"+dbmap[dbtype][\"increment_database\"]\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "    #         for row in tablelist:\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(execute, table,dbtype) for table in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())\n",
    "                print(\"end-----\",datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb1d7b6f38f45f8ae7893565fba359f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+----------+--------------------+----------+--------+----------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+---------------+--------------------+----+\n",
      "|            etldate|      id|product_id|              ref_id|      type|sub_type|   channel|             send_to| status|   title|             content|  result_description|    expect_send_time|    actual_send_time|         create_time|         update_time|              avatar|            creative|trigger_id|trigger_type_id|              params|year|\n",
      "+-------------------+--------+----------+--------------------+----------+--------+----------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+---------------+--------------------+----+\n",
      "|2021-01-01 23:59:59|  810103|         0|            36004930|      PUSH|    null|       GCM|cqkME4AHe3c:APA91...|SUCCESS|ADAPUNDI|Maaf, kualifikasi...|PUSH, lovina send...|2020-01-29 11:26:...|2020-01-29 11:26:...|2020-01-29 11:26:...|2020-01-29 11:26:...|                null|                null|        49|              2|                null|2021|\n",
      "|2021-01-01 23:59:59| 2591863|         0|            44387909|       SMS|    null|       STI|        081295943595|SUCCESS|    null|(AdaPundi) Maaf, ...|SMS, lovina send ...|2020-03-10 15:15:...|2020-03-10 15:15:...|2020-03-10 15:15:...|2020-03-10 15:15:...|                null|                null|        49|              2|                null|2021|\n",
      "|2021-01-01 23:59:59| 7206583|         0|            48522306|WHATS_APPS|    null|SOCIAL_HAT|        082232891375|SUCCESS|    null|🥰 🥰 🥰 Kabar ge...|WHATS_APPS, lovin...| 2020-04-27 05:00:00|2020-04-27 05:00:...|2020-04-27 04:02:...|2020-04-27 05:00:...|https://assets.ad...|https://assets.ad...|       166|              3|                null|2021|\n",
      "|2021-01-01 23:59:59| 8246566|         0|            49276598|WHATS_APPS|    null|SOCIAL_HAT|        089695414727|SUCCESS|    null|Pinjaman Anda tel...|WHATS_APPS, lovin...| 2020-05-15 02:35:00|2020-05-15 02:35:...|2020-05-15 01:48:...|2020-05-15 02:35:...|https://assets.ad...|https://assets.ad...|        96|              3|  {\"loanId\":7495607}|2021|\n",
      "|2021-01-01 23:59:59| 8660626|         0|            38974486|      PUSH|    null|       GCM|dpnv0e07-q8:APA91...|SUCCESS|AdaPundi|Maaf, ulasan kual...|PUSH, lovina send...|2020-05-20 08:28:...|2020-05-20 08:28:...|2020-05-20 08:28:...|2020-05-20 08:28:...|                null|                null|        49|              2|  {\"loanId\":8287523}|2021|\n",
      "|2021-01-01 23:59:59| 8862571|         0|            49763036|WHATS_APPS|    null|SOCIAL_HAT|        081236660007|SUCCESS|    null|Pinjaman Anda tel...|WHATS_APPS, lovin...| 2020-05-23 02:35:00|2020-05-23 02:37:...|2020-05-23 00:27:...|2020-05-23 02:37:...|https://assets.ad...|https://assets.ad...|        96|              3|  {\"loanId\":7818262}|2021|\n",
      "|2021-01-01 23:59:59| 9155750|         0|2770d836-e6ac-471...|    IN_BOX|    null|     INBOX|             4201241|SUCCESS|ADAPUNDI|Maaf, kualifikasi...| Send inbox success!|2020-05-26 04:49:...|2020-05-26 04:49:...|2020-05-26 04:49:...|2020-05-26 04:49:...|                null|                null|        50|              2|  {\"loanId\":8302114}|2021|\n",
      "|2021-01-01 23:59:59| 9315289|         0|            50130815|WHATS_APPS|    null|SOCIAL_HAT|        085779450026|SUCCESS|    null|Pinjaman Anda tel...|WHATS_APPS, lovin...| 2020-05-28 02:35:00|2020-05-28 02:35:...|2020-05-28 01:48:...|2020-05-28 02:35:...|https://assets.ad...|https://assets.ad...|        96|              3|  {\"loanId\":7517546}|2021|\n",
      "|2021-01-01 23:59:59| 9704644|         0|            50414234|WHATS_APPS|    null|SOCIAL_HAT|        082282222000|SUCCESS|    null|Karena pelanggara...|WHATS_APPS, lovin...| 2020-06-02 02:40:00|2020-06-02 02:45:...|2020-06-01 04:08:...|2020-06-02 02:45:...|https://assets.ad...|https://assets.ad...|        97|              3|  {\"loanId\":6131064}|2021|\n",
      "|2021-01-01 23:59:59|11688778|         0|            51765760|WHATS_APPS|    null|SOCIAL_HAT|        089527324303|SENDING|    null|Karena pelanggara...|WHATS_APPS, lovin...| 2020-06-24 02:40:00|2020-06-24 02:50:...|2020-06-23 03:42:...|2020-06-24 02:50:...|https://assets.ad...|https://assets.ad...|        97|              3|  {\"loanId\":6971878}|2021|\n",
      "|2021-01-01 23:59:59|14474468|         0|948f4851-d8a1-5f0...|      PUSH|    null|       GCM|dxyh-IIa4lw:APA91...|SUCCESS|AdaPundi|Kredit Anda baik,...|PUSH, lovina send...| 2020-07-26 11:00:00|2020-07-26 11:00:...|2020-07-25 13:55:...|2020-07-26 11:00:...|                null|                null|       331|              1|{\"orderID\":855712...|2021|\n",
      "|2021-01-01 23:59:59|15997481|         0|993fa034-eecb-5ba...|    IN_BOX|    null|     INBOX|             4299734|   READ|AdaPundi|Data Anda baik, a...| Send inbox success!| 2020-08-12 03:00:00|2020-08-12 03:00:...|2020-08-11 23:51:...|2020-08-17 02:06:...|                null|                null|       436|              1|{\"orderID\":0,\"cus...|2021|\n",
      "|2021-01-01 23:59:59|16436786|         0|e4d17482-89f7-499...|      PUSH|    null|       GCM|cgD1KJr5z1Y:APA91...|SUCCESS|AdaPundi|Anda memiliki dan...|PUSH, lovina send...| 2020-08-17 03:00:00|2020-08-17 03:14:...|2020-08-16 09:33:...|2020-08-17 03:14:...|                null|                null|        13|              1|{\"customerId\":431...|2021|\n",
      "|2021-01-01 23:59:59|18511800|         0|c9055991-1e73-4ad...|      PUSH|    null|       GCM|cqSdlPnSwso:APA91...|SUCCESS|AdaPundi|Maaf, ulasan kual...|PUSH, lovina send...|2020-09-04 08:07:...|2020-09-04 08:07:...|2020-09-04 08:07:...|2020-09-04 08:07:...|                null|                null|        49|              2|{\"customerId\":437...|2021|\n",
      "|2021-01-01 23:59:59|19438963|         0|a6b32750-b978-5ce...|    IN_BOX|    null|     INBOX|             4342749|SUCCESS|AdaPundi|Anda telah mendap...| Send inbox success!| 2020-09-13 03:00:00|2020-09-13 03:03:...|2020-09-12 09:20:...|2020-09-13 03:03:...|                null|                null|       628|              1|{\"orderID\":884393...|2021|\n",
      "|2021-01-01 23:59:59|21026581|         0|a4d4d9bb-7b76-48e...|      PUSH|    null|       GCM|cRtG5nupkAI:APA91...|SUCCESS|AdaPundi|Selamat, Anda tel...|PUSH, lovina send...|2020-09-28 07:06:...|2020-09-28 07:06:...|2020-09-28 07:06:...|2020-09-28 07:06:...|                null|                null|        45|              2|{\"SCORE_AMOUNT\":\"...|2021|\n",
      "|2021-01-01 23:59:59|21746059|         0|0bd0d9e0-6ea7-5a4...|    IN_BOX|    null|     INBOX|             4323203|   READ|AdaPundi|Tarik tunai hari ...| Send inbox success!| 2020-10-04 10:00:00|2020-10-04 10:00:...|2020-10-04 09:14:...|2020-10-07 08:26:...|                null|                null|       331|              1|{\"orderID\":899154...|2021|\n",
      "|2021-01-01 23:59:59|21913304|         0|4cc203e3-2999-530...|    IN_BOX|    null|     INBOX|             2397343|SUCCESS|ADAPUNDI|Tarik tunai hari ...| Send inbox success!| 2020-10-06 03:32:00|2020-10-06 03:32:...|2020-10-05 23:04:...|2020-10-06 03:32:...|                null|                null|       430|              1|{\"orderID\":899247...|2021|\n",
      "|2021-01-01 23:59:59|22415772|         0|a47a221e-0f7c-4ac...|      PUSH|    null|       GCM|dzNZkQZdXC8:APA91...|SUCCESS|AdaPundi|Kredit Anda baik,...|PUSH, lovina send...| 2020-10-11 03:00:00|2020-10-11 03:05:...|2020-10-10 09:05:...|2020-10-11 03:05:...|                null|                null|        21|              1|{\"customerId\":803...|2021|\n",
      "|2021-01-01 23:59:59|26278980|         0|a9b62a61-f5d9-46a...|      PUSH|    null|       GCM|dDW057h0Eng:APA91...|SUCCESS|AdaPundi|Maaf, kualifikasi...|PUSH, lovina send...|2020-11-03 07:33:...|2020-11-03 07:33:...|2020-11-03 07:33:...|2020-11-03 07:33:...|                null|                null|        50|              2|{\"customerId\":450...|2021|\n",
      "+-------------------+--------+----------+--------------------+----------+--------+----------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+---------------+--------------------+----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df=spark.sql(\"\"\"\n",
    "with update_snapshot_notification_t_message as (\n",
    "  select *\n",
    "    from (\n",
    "      SELECT * ,\n",
    "         row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "         FROM `notification_stream_etl`.`t_message`\n",
    "         where date(year || '-' || month || '-' || day) <=date('2021-02-25') and date(year || '-' || month || '-' || day) >= date('2021-02-24')\n",
    "      )\n",
    "    where row_num = 1\n",
    "  )\n",
    "select update_snapshot_notification_t_message.etldate ,update_snapshot_notification_t_message.id ,update_snapshot_notification_t_message.product_id ,update_snapshot_notification_t_message.ref_id ,update_snapshot_notification_t_message.type ,update_snapshot_notification_t_message.sub_type ,update_snapshot_notification_t_message.channel ,update_snapshot_notification_t_message.send_to ,update_snapshot_notification_t_message.status ,update_snapshot_notification_t_message.title ,update_snapshot_notification_t_message.content ,update_snapshot_notification_t_message.result_description ,update_snapshot_notification_t_message.expect_send_time ,update_snapshot_notification_t_message.actual_send_time ,update_snapshot_notification_t_message.create_time ,update_snapshot_notification_t_message.update_time ,update_snapshot_notification_t_message.avatar ,update_snapshot_notification_t_message.creative ,update_snapshot_notification_t_message.trigger_id ,update_snapshot_notification_t_message.trigger_type_id ,update_snapshot_notification_t_message.params  ,update_snapshot_notification_t_message.year from (\n",
    "  select *\n",
    "  from `notification_stream_etl`.`t_message`\n",
    "  where\n",
    "    date(year || '-' || month || '-' || day) <=date('2021-02-25') and date(year || '-' || month || '-' || day) >= date('2021-02-24')\n",
    "    and kind = 'insert'\n",
    "  ) new\n",
    "left join update_snapshot_notification_t_message\n",
    "on new.id = update_snapshot_notification_t_message.id where if(update_snapshot_notification_t_message.kind is not null,update_snapshot_notification_t_message.kind,'') <> 'delete'\n",
    "and  update_snapshot_notification_t_message.id=48326289\n",
    "\"\"\").show()\n",
    "# df=df.drop(\"etlindex\")\n",
    "# df.drop(\"col_x\").write.mode(\"overwrite\").partitionBy(\"year\").format(\"orc\").saveAsTable(\"etl_s3_temp.notification_t_message\")\n",
    "# spark.sql(\"select  * from  \"+\"etl_s3_temp.notification_t_message\").show()\n",
    "# print(real_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
