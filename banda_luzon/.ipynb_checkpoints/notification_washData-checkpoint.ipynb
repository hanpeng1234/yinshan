{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9a445e2135459cac6e57e3ca5b61f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling boto3-1.17.16:\n",
      "  Successfully uninstalled boto3-1.17.16\n",
      "\n",
      "Collecting boto3\n",
      "  Using cached https://files.pythonhosted.org/packages/c2/03/18184037cb21cab227e392962e0ba9a7596d777a08d7c07c2d3640f939bf/boto3-1.17.16-py2.py3-none-any.whl\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.16 in /mnt/tmp/1614332935128-0/lib/python3.7/site-packages (from boto3)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /mnt/tmp/1614332935128-0/lib/python3.7/site-packages (from boto3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /mnt/tmp/1614332935128-0/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.16->boto3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /mnt/tmp/1614332935128-0/lib/python3.7/site-packages (from botocore<1.21.0,>=1.20.16->boto3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.16->boto3)\n",
      "Installing collected packages: boto3\n",
      "Successfully installed boto3-1.17.16\n",
      "\n",
      "<function hmac_sha256 at 0x7effdd2f9440>\n",
      "notification_stream_etl\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#notificationÂØºË°®Âà∞notification_etl_s3\n",
    "def install():\n",
    "    sc.uninstall_package('boto3')\n",
    "    sc.install_pypi_package(\"boto3\")\n",
    "install()\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "import boto3\n",
    "import json\n",
    "import pytz\n",
    "import hmac\n",
    "import pyspark.sql.functions as F\n",
    "from hashlib import sha256 \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# print(datetime.now(pytz.timezone(\"Asia/Shanghai\")).strftime( '%Y-%m-%d'))\n",
    "today=(datetime.now()+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "yesterday=(datetime.now(pytz.timezone(\"Asia/Shanghai\"))+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "tablemap={\"notification_stream_etl\":\"notification\"} \n",
    "# {\"notification_stream_etl\":\"notification\"} #,\n",
    "# {\"telemarket_stream_etl\":\"telemarket\"} #,\n",
    "# ,\"banda_stream_etl\":\"1\"\n",
    "tablesql_1=\" FROM  (   SELECT    *  , row_number() OVER (PARTITION BY id  ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC  , if(etlindex is null ,0,etlindex)  desc)  row_num FROM  \"\n",
    "tablesql_2=\" ORDER BY id  ASC )  a  WHERE a.row_num = 1   AND a.kind <> 'delete' \"\n",
    "# ÊõøÊç¢ÊâãÊú∫Âè∑\n",
    "def get_secret_obj():\n",
    "    df=spark.read.text(\"s3://rupiahplus-configs/etl/data_secrt/col.json\").collect()\n",
    "    keymap=''\n",
    "    for  row in df:\n",
    "        keymap=keymap+row['value']\n",
    "    json_content=json.loads(keymap)\n",
    "    return json_content\n",
    "colmap= get_secret_obj()\n",
    "mobileType=colmap[\"mobileType\"]\n",
    "def getRelmobile(colNm,str):\n",
    "    if colNm in mobileType:\n",
    "        relmobile = str.strip().replace(\"+\", \"\").replace(\"-\", \"\").replace(\" \", \"\").replace('\"', '')\n",
    "        if (relmobile.startswith(\"62\")):\n",
    "            relmobile = relmobile.replace(\"62\", \"0\",1)\n",
    "        if relmobile.startswith(\"0\") == False:\n",
    "            relmobile = \"0\" + relmobile;\n",
    "        return relmobile\n",
    "    return str\n",
    "# hmac_256Âä†ÂØÜ\n",
    "def hmac_sha256(key,colNm,value):\n",
    "    if value!=None:\n",
    "#         Â§ÑÁêÜÊâãÊú∫Âè∑\n",
    "        rtn=getRelmobile(colNm,value)\n",
    "        h = hmac.new(key.encode('utf-8'),digestmod=sha256)\n",
    "        h.update(rtn.encode('utf-8'))\n",
    "        h_str = h.hexdigest()\n",
    "        return h_str\n",
    "spark.udf.register(\"hmac_sha256\",hmac_sha256,T.StringType())\n",
    "def getTableColum(b):\n",
    "    colum=\"\"\n",
    "    for index in range(len(b)):\n",
    "        if(index>0 and index<len(b)-8):   \n",
    "            colum=colum+setDef(b[index][\"data_type\"],b[index][\"col_name\"])+\", \"\n",
    "    return colum[0:len(colum)-2]\n",
    "def setDef(type,table_col):\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull(\"+table_col+\",0) as \"+table_col\n",
    "    else:\n",
    "        return table_col\n",
    "def add_secret_col(dbType,tableNm,col):\n",
    "    secret_Col=[]\n",
    "    if colmap[tablemap[databaseName]]!=None and colmap[tablemap[databaseName]].get(tableNm)!=None:\n",
    "        secret_Col=colmap[tablemap[databaseName]][tableNm]\n",
    "    sh256Col =col\n",
    "    hmac_key=colmap[\"hmac_key\"]\n",
    "#     sql=\"select hmac_sha256('{hmac_key}','123456789') as sh2\".format(hmac_key=hmac_key)\n",
    "    for i in secret_Col:\n",
    "        sh256Col=sh256Col+ \", hmac_sha256('{hmac_key}','\"+i+\"', \"+i+\") \"+ i+\"_x\"\n",
    "    return  sh256Col.format(hmac_key=hmac_key)\n",
    "def make_col_temptable(databaseName,row):\n",
    "    tableName=row[\"tableName\"]\n",
    "    if tableName=='t_message':\n",
    "        tablecolum=\"desc \" +databaseName+\".\"+tableName;\n",
    "        tableSchema= spark.sql(tablecolum).collect()\n",
    "        tablepath=\"s3://rupiahplus-data-warehouse/aliyun/\"+tablemap[databaseName]+\"/\"+row[\"tableName\"]\n",
    "        spark.catalog.refreshTable(databaseName+\".\"+tableName)\n",
    "        secret_sql=add_secret_col(tablemap[databaseName],tableName,getTableColum(tableSchema))+\", year\"\n",
    "        sql=\"select \"+secret_sql+ tablesql_1+databaseName+\".\"+tableName+tablesql_2\n",
    "        df=spark.sql(sql).drop(\"etlindex\")\n",
    "        if colmap[tablemap[databaseName]].get(tableName)!=None :\n",
    "            for i in colmap[tablemap[databaseName]][tableName]:\n",
    "                df=df.withColumn(\"col_x\",F.col(i+\"_x\")).withColumn(i+\"_x\",F.col(i)).withColumn(i,F.col(\"col_x\"))\n",
    "        df.drop(\"col_x\").write.mode(\"overwrite\").partitionBy(\"year\").orc(tablepath)\n",
    "if __name__ == \"builtins\":\n",
    "#     Ê≥®ÂÜåudf\n",
    "    spark.udf.register(\"hmac_sha256\",hmac_sha256,T.StringType())\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Python Demo\")\\\n",
    "        .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "#     spark.conf.set(\"spark.scheduler.mode\",\"FAIR\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "    for databaseName in tablemap:\n",
    "        print(databaseName)\n",
    "        databasesql=\"show tables in \"+databaseName\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(make_col_temptable,databaseName, tableNm) for tableNm in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164ef8307d494f04a0312353090c2e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start----- 2021-02-26 09:57:25.016937\n",
      "with update_snapshot_notification_t_message as (\n",
      "  select *\n",
      "    from (\n",
      "      SELECT * ,\n",
      "         row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
      "         FROM `notification_stream_etl`.`t_message`\n",
      "         where date(year || '-' || month || '-' || day) <=date('2021-02-26') and date(year || '-' || month || '-' || day) >= date('2021-02-25')\n",
      "      )\n",
      "    where row_num = 1\n",
      "  )\n",
      "select  if(update_snapshot_notification_t_message.id is null, l.etldate     , update_snapshot_notification_t_message.etldate)  etldate ,if(update_snapshot_notification_t_message.id is null, l.id     , update_snapshot_notification_t_message.id)  id ,if(update_snapshot_notification_t_message.id is null, l.product_id     , update_snapshot_notification_t_message.product_id)  product_id ,if(update_snapshot_notification_t_message.id is null, l.ref_id     , update_snapshot_notification_t_message.ref_id)  ref_id ,if(update_snapshot_notification_t_message.id is null, l.type     , update_snapshot_notification_t_message.type)  type ,if(update_snapshot_notification_t_message.id is null, l.sub_type     , update_snapshot_notification_t_message.sub_type)  sub_type ,if(update_snapshot_notification_t_message.id is null, l.channel     , update_snapshot_notification_t_message.channel)  channel ,if(update_snapshot_notification_t_message.id is null, l.send_to_x     , update_snapshot_notification_t_message.send_to)  send_to ,if(update_snapshot_notification_t_message.id is null, l.status     , update_snapshot_notification_t_message.status)  status ,if(update_snapshot_notification_t_message.id is null, l.title     , update_snapshot_notification_t_message.title)  title ,if(update_snapshot_notification_t_message.id is null, l.content     , update_snapshot_notification_t_message.content)  content ,if(update_snapshot_notification_t_message.id is null, l.result_description     , update_snapshot_notification_t_message.result_description)  result_description ,if(update_snapshot_notification_t_message.id is null, l.expect_send_time     , update_snapshot_notification_t_message.expect_send_time)  expect_send_time ,if(update_snapshot_notification_t_message.id is null, l.actual_send_time     , update_snapshot_notification_t_message.actual_send_time)  actual_send_time ,if(update_snapshot_notification_t_message.id is null, l.create_time     , update_snapshot_notification_t_message.create_time)  create_time ,if(update_snapshot_notification_t_message.id is null, l.update_time     , update_snapshot_notification_t_message.update_time)  update_time ,if(update_snapshot_notification_t_message.id is null, l.avatar     , update_snapshot_notification_t_message.avatar)  avatar ,if(update_snapshot_notification_t_message.id is null, l.creative     , update_snapshot_notification_t_message.creative)  creative ,if(update_snapshot_notification_t_message.id is null, l.trigger_id     , update_snapshot_notification_t_message.trigger_id)  trigger_id ,if(update_snapshot_notification_t_message.id is null, l.trigger_type_id     , update_snapshot_notification_t_message.trigger_type_id)  trigger_type_id ,if(update_snapshot_notification_t_message.id is null, l.params     , update_snapshot_notification_t_message.params)  params , if(update_snapshot_notification_t_message.id is null, l.send_to     , hmac_sha256('xUIAZ3grtPOxaPNK','send_to',update_snapshot_notification_t_message.send_to)) send_to_x , if(update_snapshot_notification_t_message.id is null, l.year     , update_snapshot_notification_t_message.year)  year \n",
      "from `notification_etl_s3`.`t_message` l\n",
      "left join update_snapshot_notification_t_message\n",
      "on l.id = update_snapshot_notification_t_message.id\n",
      "where if(update_snapshot_notification_t_message.kind is not null,update_snapshot_notification_t_message.kind,'') <> 'delete'\n",
      "union\n",
      "select update_snapshot_notification_t_message.etldate ,update_snapshot_notification_t_message.id ,update_snapshot_notification_t_message.product_id ,update_snapshot_notification_t_message.ref_id ,update_snapshot_notification_t_message.type ,update_snapshot_notification_t_message.sub_type ,update_snapshot_notification_t_message.channel ,update_snapshot_notification_t_message.send_to ,update_snapshot_notification_t_message.status ,update_snapshot_notification_t_message.title ,update_snapshot_notification_t_message.content ,update_snapshot_notification_t_message.result_description ,update_snapshot_notification_t_message.expect_send_time ,update_snapshot_notification_t_message.actual_send_time ,update_snapshot_notification_t_message.create_time ,update_snapshot_notification_t_message.update_time ,update_snapshot_notification_t_message.avatar ,update_snapshot_notification_t_message.creative ,update_snapshot_notification_t_message.trigger_id ,update_snapshot_notification_t_message.trigger_type_id ,update_snapshot_notification_t_message.params ,hmac_sha256('xUIAZ3grtPOxaPNK','send_to', update_snapshot_notification_t_message.send_to) send_to_x ,update_snapshot_notification_t_message.year from (\n",
      "  select *\n",
      "  from `notification_stream_etl`.`t_message`\n",
      "  where\n",
      "    date(year || '-' || month || '-' || day) <=date('2021-02-26') and date(year || '-' || month || '-' || day) >= date('2021-02-25')\n",
      "    and kind = 'insert'\n",
      "  ) new\n",
      "left join update_snapshot_notification_t_message\n",
      "on new.id = update_snapshot_notification_t_message.id where if(update_snapshot_notification_t_message.kind is not null,update_snapshot_notification_t_message.kind,'') <> 'delete' \n",
      "None\n",
      "end----- 2021-02-26 09:57:26.066944\n",
      "None\n",
      "end----- 2021-02-26 09:57:26.066970\n",
      "None\n",
      "end----- 2021-02-26 09:57:26.066981\n",
      "None\n",
      "end----- 2021-02-26 09:57:26.066991\n",
      "None\n",
      "end----- 2021-02-26 09:57:26.067000\n",
      "None\n",
      "end----- 2021-02-26 09:57:26.067010"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime,timedelta\n",
    "import copy\n",
    "import json\n",
    "import pytz\n",
    "import hmac\n",
    "from hashlib import sha256 \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import types as T\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Python Demo\")\\\n",
    "    .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "spark.conf.set(\"spark.files.overwrite\",\"true\")\n",
    "#     ,Apache ArrowÔºö‰∏Ä‰∏™Ë∑®Âπ≥Âè∞ÁöÑÂú®ÂÜÖÂ≠ò‰∏≠‰ª•ÂàóÂºèÂ≠òÂÇ®ÁöÑÊï∞ÊçÆÂ±ÇÔºåÁî®Êù•Âä†ÈÄüÂ§ßÊï∞ÊçÆÂàÜÊûêÈÄüÂ∫¶„ÄÇÂÖ∂ÂèØ‰ª•‰∏ÄÊ¨°ÊÄß‰º†ÂÖ•Êõ¥Â§ßÂùóÁöÑÊï∞ÊçÆÔºåpyspark‰∏≠Â∑≤ÁªèÊúâËΩΩÂÖ•ËØ•Ê®°ÂùóÔºåÈúÄË¶ÅÊâìÂºÄËØ•ËÆæÁΩÆÔºö\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "#     ÂàõÂª∫Ë°®Êó∂Ëá™Âä®Âà†Èô§Â∑≤Â≠òÂú®ÁöÑÁõÆÂΩï\n",
    "spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "dbmap={\n",
    "    \"notification\":{\n",
    "        \"increment_database\":\"notification_stream_etl\",\n",
    "        \"database\":\"notification_etl_s3\",\n",
    "        \"realTime_path\":\"notification\"\n",
    "    },\n",
    "#      \"telemarket\":{\n",
    "#          \"increment_database\":\"telemarket_stream_etl\",\n",
    "#          \"database\":\"telemarket_etl_s3\",\n",
    "#          \"realTime_path\":\"telemarket\"\n",
    "#     }\n",
    "}\n",
    "nowdate=(datetime.now()).strftime('%Y-%m-%d')\n",
    "yesterday=(datetime.now()+ timedelta(-1)).strftime('%Y-%m-%d')\n",
    "sqltemp=\"\"\"with {update_snapshot_table} as (\n",
    "  select *\n",
    "    from (\n",
    "      SELECT * ,\n",
    "         row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "         FROM `{increment_database}`.`{tableNm}`\n",
    "         where date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "      )\n",
    "    where row_num = 1\n",
    "  )\n",
    "select  {select_sql} \n",
    "from `{database}`.`{tableNm}` l\n",
    "left join {update_snapshot_table}\n",
    "on l.id = {update_snapshot_table}.id\n",
    "where if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete'\n",
    "union\n",
    "select {update_snapshot_sql} from (\n",
    "  select *\n",
    "  from `{increment_database}`.`{tableNm}`\n",
    "  where\n",
    "    date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "    and kind = 'insert'\n",
    "  ) new\n",
    "left join {update_snapshot_table}\n",
    "on new.id = {update_snapshot_table}.id where if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete' \"\"\"\n",
    "def get_secret_obj():\n",
    "    df=spark.read.text(\"s3://rupiahplus-configs/etl/data_secrt/col.json\").collect()\n",
    "    keymap=''\n",
    "    for  row in df:\n",
    "        keymap=keymap+row['value']\n",
    "    json_content=json.loads(keymap)\n",
    "    return json_content\n",
    "colmap= get_secret_obj()   \n",
    "hmac_key=colmap[\"hmac_key\"]\n",
    "mobileType=colmap[\"mobileType\"]\n",
    "def getRelmobile(colNm,str):\n",
    "    if colNm in mobileType:\n",
    "        relmobile = str.strip().replace(\"+\", \"\").replace(\"-\", \"\").replace(\" \", \"\").replace('\"', '')\n",
    "        if (relmobile.startswith(\"62\")):\n",
    "            relmobile = relmobile.replace(\"62\", \"0\",1)\n",
    "        if relmobile.startswith(\"0\") == False:\n",
    "            relmobile = \"0\" + relmobile;\n",
    "        return relmobile\n",
    "    return str\n",
    "# hmac_256Âä†ÂØÜ\n",
    "def hmac_sha256(key,colNm,value):\n",
    "    if value!=None and value!='':\n",
    "#         Â§ÑÁêÜÊâãÊú∫Âè∑\n",
    "        rtn=getRelmobile(colNm,value)\n",
    "        h = hmac.new(key.encode('utf-8'),digestmod=sha256)\n",
    "        h.update(rtn.encode('utf-8'))\n",
    "        h_str = h.hexdigest()\n",
    "        return h_str\n",
    "spark.udf.register(\"hmac_sha256\",hmac_sha256,T.StringType())\n",
    "def getTableColum(b,update_snapshot_table,dbtype,tableNm):\n",
    "    colum=\"\"\n",
    "    snapshot_colum=\"\"\n",
    "    sqlstrtemp=\"if({update_snapshot_table}.id is null, {colNm}     , {snapshot_colNm})  {ascolNm} ,\"\n",
    "    sqlstr_secret_temp=\" if({update_snapshot_table}.id is null, {colNm}     , hmac_sha256('{hmac_key}','{ascolNm}',{snapshot_colNm})) {ascolNm}_x , \"\n",
    "    for index in range(len(b)):\n",
    "        if(index==1 or (index>2 and index<len(b)-7)):\n",
    "#             print(index,b[index])\n",
    "#             Âú®yearÂâçÈù¢Âä†‰∏äÂä†ÂØÜÂàó\n",
    "            if(index==len(b)-8)and  colmap.get(dbtype)!=None and colmap[dbtype]!=None and colmap[dbtype].get(tableNm)!=None:\n",
    "                for secret_col in colmap[dbtype][tableNm]:\n",
    "                    sqlstr=copy.copy(sqlstr_secret_temp)\n",
    "                    colum=colum+sqlstr.format(colNm=setDef(\"string\",secret_col,False),snapshot_colNm=setDef(\"string\",secret_col,True),ascolNm=secret_col,update_snapshot_table=update_snapshot_table,hmac_key=hmac_key)\n",
    "                    snapshot_colum=snapshot_colum+setsecret_SnapshotDef(secret_col).format(update_snapshot_table=update_snapshot_table,hmac_key=hmac_key)+\" ,\"\n",
    "            sqlstr=copy.copy(sqlstrtemp)\n",
    "            colum=colum+sqlstr.format(colNm=isX(dbtype,tableNm,setDef(b[index][\"data_type\"],b[index][\"col_name\"],False)),snapshot_colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],True),ascolNm=b[index][\"col_name\"],update_snapshot_table=update_snapshot_table)\n",
    "            snapshot_colum=snapshot_colum+setSnapshotDef(b[index][\"data_type\"],b[index][\"col_name\"]).format(update_snapshot_table=update_snapshot_table)+\" ,\"\n",
    "    return colum[0:len(colum)-2],snapshot_colum[0:len(snapshot_colum)-2]\n",
    "def isX(type,table,column):\n",
    "    columnList=column.split(\".\")\n",
    "    if len(columnList)>0 and  colmap.get(type)!=None and colmap[type].get(table)!=None and columnList[len(columnList)-1] in colmap[type][table]:\n",
    "        return column+\"_x\"\n",
    "    return column\n",
    "def setDef(type,table_col,is_snapshot):\n",
    "    table_col_x=copy.copy(table_col)\n",
    "    if(type[:7] == 'decimal'):\n",
    "        if is_snapshot:\n",
    "            return \"ifnull({update_snapshot_table}.\"+table_col_x+\",0)  \"\n",
    "        else:\n",
    "            return \"ifnull(l.\"+table_col_x+\",0) \"\n",
    "    else:\n",
    "        if is_snapshot:\n",
    "            return \"{update_snapshot_table}.\"+table_col_x\n",
    "        else:\n",
    "            return \"l.\"+table_col_x\n",
    "def setSnapshotDef(type,table_col):\n",
    "    table_col_x=copy.copy(table_col)\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull({update_snapshot_table}.\"+table_col_x+\",0) \"+table_col\n",
    "    else:\n",
    "        return \"{update_snapshot_table}.\"+table_col_x\n",
    "def setsecret_SnapshotDef(table_col):\n",
    "    table_col_x=copy.copy(table_col)\n",
    "#     print(\"hmac_sha256('{hmac_key}','\"+table_col_x+\"', {update_snapshot_table}.\"+table_col_x+\") \"+ table_col_x+\"_x\")\n",
    "    return \"hmac_sha256('{hmac_key}','\"+table_col_x+\"', {update_snapshot_table}.\"+table_col_x+\") \"+ table_col_x+\"_x\"\n",
    "#     return \"sha2({update_snapshot_table}.\"+table_col+\",256) \" +table_col+\"_x\"\n",
    "def execute(tablerow,dbtype):\n",
    "#     print(tablerow,dbtype)\n",
    "    tableName=tablerow[\"tableName\"]\n",
    "    if tableName=='t_message':\n",
    "        update_snapshot_table=\"update_snapshot_\"+dbtype+\"_\"+tableName\n",
    "        spark.catalog.refreshTable(\"`\"+dbmap[dbtype][\"database\"]+\"`.\"+tableName)\n",
    "        col=spark.sql(\"desc `\"+dbmap[dbtype][\"increment_database\"]+\"`.\"+tableName).collect()\n",
    "        #     #ÁúüÊ≠£ÁöÑÂàó,Â¢ûÈáèÁöÑÂàó\n",
    "        real_col,snapshot_col=getTableColum(col,update_snapshot_table,dbtype,tableName)\n",
    "        sql=copy.copy(sqltemp)\n",
    "        real_sql=sql.format(increment_database=dbmap[dbtype][\"increment_database\"],tableNm=tableName,nowdate=nowdate,select_sql=real_col.format(update_snapshot_table=update_snapshot_table),update_snapshot_sql=snapshot_col.format(update_snapshot_table=update_snapshot_table),database=dbmap[dbtype][\"database\"],yesterday=yesterday,update_snapshot_table=update_snapshot_table,hmac_key=hmac_key)\n",
    "#         df=spark.sql(real_sql).drop(\"etlindex\")\n",
    "#         if colmap.get(dbtype)!=None and colmap[dbtype].get(tableName)!=None :\n",
    "#             for i in colmap[dbtype][tableName]:\n",
    "#                 df=df.withColumn(\"col_x\",F.col(i+\"_x\")).withColumn(i+\"_x\",F.col(i)).withColumn(i,F.col(\"col_x\"))\n",
    "#         df.drop(\"col_x\").write.mode(\"overwrite\").partitionBy(\"year\").format(\"orc\").saveAsTable(\"etl_s3_temp.\"+dbtype+\"_\"+tableName);\n",
    "#         tablepath='s3://rupiahplus-data-warehouse/aliyun/'+dbmap[dbtype][\"realTime_path\"]+'/'+tableName+\"/\"\n",
    "#         spark.sql(\"select  * from  \"+\"etl_s3_temp.\"+dbtype+\"_\"+tableName).write.mode(\"overwrite\").partitionBy(\"year\").orc(tablepath)\n",
    "        print(real_sql)\n",
    "if __name__ == \"builtins\":\n",
    "    print(\"start-----\",datetime.now())\n",
    "    for dbtype in dbmap:\n",
    "        databasesql=\"show tables in \"+dbmap[dbtype][\"increment_database\"]\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "    #         for row in tablelist:\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(execute, table,dbtype) for table in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())\n",
    "                print(\"end-----\",datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb1d7b6f38f45f8ae7893565fba359f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+----------+--------------------+----------+--------+----------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+---------------+--------------------+----+\n",
      "|            etldate|      id|product_id|              ref_id|      type|sub_type|   channel|             send_to| status|   title|             content|  result_description|    expect_send_time|    actual_send_time|         create_time|         update_time|              avatar|            creative|trigger_id|trigger_type_id|              params|year|\n",
      "+-------------------+--------+----------+--------------------+----------+--------+----------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+---------------+--------------------+----+\n",
      "|2021-01-01 23:59:59|  810103|         0|            36004930|      PUSH|    null|       GCM|cqkME4AHe3c:APA91...|SUCCESS|ADAPUNDI|Maaf, kualifikasi...|PUSH, lovina send...|2020-01-29 11:26:...|2020-01-29 11:26:...|2020-01-29 11:26:...|2020-01-29 11:26:...|                null|                null|        49|              2|                null|2021|\n",
      "|2021-01-01 23:59:59| 2591863|         0|            44387909|       SMS|    null|       STI|        081295943595|SUCCESS|    null|(AdaPundi) Maaf, ...|SMS, lovina send ...|2020-03-10 15:15:...|2020-03-10 15:15:...|2020-03-10 15:15:...|2020-03-10 15:15:...|                null|                null|        49|              2|                null|2021|\n",
      "|2021-01-01 23:59:59| 7206583|         0|            48522306|WHATS_APPS|    null|SOCIAL_HAT|        082232891375|SUCCESS|    null|ü•∞ ü•∞ ü•∞ Kabar ge...|WHATS_APPS, lovin...| 2020-04-27 05:00:00|2020-04-27 05:00:...|2020-04-27 04:02:...|2020-04-27 05:00:...|https://assets.ad...|https://assets.ad...|       166|              3|                null|2021|\n",
      "|2021-01-01 23:59:59| 8246566|         0|            49276598|WHATS_APPS|    null|SOCIAL_HAT|        089695414727|SUCCESS|    null|Pinjaman Anda tel...|WHATS_APPS, lovin...| 2020-05-15 02:35:00|2020-05-15 02:35:...|2020-05-15 01:48:...|2020-05-15 02:35:...|https://assets.ad...|https://assets.ad...|        96|              3|  {\"loanId\":7495607}|2021|\n",
      "|2021-01-01 23:59:59| 8660626|         0|            38974486|      PUSH|    null|       GCM|dpnv0e07-q8:APA91...|SUCCESS|AdaPundi|Maaf, ulasan kual...|PUSH, lovina send...|2020-05-20 08:28:...|2020-05-20 08:28:...|2020-05-20 08:28:...|2020-05-20 08:28:...|                null|                null|        49|              2|  {\"loanId\":8287523}|2021|\n",
      "|2021-01-01 23:59:59| 8862571|         0|            49763036|WHATS_APPS|    null|SOCIAL_HAT|        081236660007|SUCCESS|    null|Pinjaman Anda tel...|WHATS_APPS, lovin...| 2020-05-23 02:35:00|2020-05-23 02:37:...|2020-05-23 00:27:...|2020-05-23 02:37:...|https://assets.ad...|https://assets.ad...|        96|              3|  {\"loanId\":7818262}|2021|\n",
      "|2021-01-01 23:59:59| 9155750|         0|2770d836-e6ac-471...|    IN_BOX|    null|     INBOX|             4201241|SUCCESS|ADAPUNDI|Maaf, kualifikasi...| Send inbox success!|2020-05-26 04:49:...|2020-05-26 04:49:...|2020-05-26 04:49:...|2020-05-26 04:49:...|                null|                null|        50|              2|  {\"loanId\":8302114}|2021|\n",
      "|2021-01-01 23:59:59| 9315289|         0|            50130815|WHATS_APPS|    null|SOCIAL_HAT|        085779450026|SUCCESS|    null|Pinjaman Anda tel...|WHATS_APPS, lovin...| 2020-05-28 02:35:00|2020-05-28 02:35:...|2020-05-28 01:48:...|2020-05-28 02:35:...|https://assets.ad...|https://assets.ad...|        96|              3|  {\"loanId\":7517546}|2021|\n",
      "|2021-01-01 23:59:59| 9704644|         0|            50414234|WHATS_APPS|    null|SOCIAL_HAT|        082282222000|SUCCESS|    null|Karena pelanggara...|WHATS_APPS, lovin...| 2020-06-02 02:40:00|2020-06-02 02:45:...|2020-06-01 04:08:...|2020-06-02 02:45:...|https://assets.ad...|https://assets.ad...|        97|              3|  {\"loanId\":6131064}|2021|\n",
      "|2021-01-01 23:59:59|11688778|         0|            51765760|WHATS_APPS|    null|SOCIAL_HAT|        089527324303|SENDING|    null|Karena pelanggara...|WHATS_APPS, lovin...| 2020-06-24 02:40:00|2020-06-24 02:50:...|2020-06-23 03:42:...|2020-06-24 02:50:...|https://assets.ad...|https://assets.ad...|        97|              3|  {\"loanId\":6971878}|2021|\n",
      "|2021-01-01 23:59:59|14474468|         0|948f4851-d8a1-5f0...|      PUSH|    null|       GCM|dxyh-IIa4lw:APA91...|SUCCESS|AdaPundi|Kredit Anda baik,...|PUSH, lovina send...| 2020-07-26 11:00:00|2020-07-26 11:00:...|2020-07-25 13:55:...|2020-07-26 11:00:...|                null|                null|       331|              1|{\"orderID\":855712...|2021|\n",
      "|2021-01-01 23:59:59|15997481|         0|993fa034-eecb-5ba...|    IN_BOX|    null|     INBOX|             4299734|   READ|AdaPundi|Data Anda baik, a...| Send inbox success!| 2020-08-12 03:00:00|2020-08-12 03:00:...|2020-08-11 23:51:...|2020-08-17 02:06:...|                null|                null|       436|              1|{\"orderID\":0,\"cus...|2021|\n",
      "|2021-01-01 23:59:59|16436786|         0|e4d17482-89f7-499...|      PUSH|    null|       GCM|cgD1KJr5z1Y:APA91...|SUCCESS|AdaPundi|Anda memiliki dan...|PUSH, lovina send...| 2020-08-17 03:00:00|2020-08-17 03:14:...|2020-08-16 09:33:...|2020-08-17 03:14:...|                null|                null|        13|              1|{\"customerId\":431...|2021|\n",
      "|2021-01-01 23:59:59|18511800|         0|c9055991-1e73-4ad...|      PUSH|    null|       GCM|cqSdlPnSwso:APA91...|SUCCESS|AdaPundi|Maaf, ulasan kual...|PUSH, lovina send...|2020-09-04 08:07:...|2020-09-04 08:07:...|2020-09-04 08:07:...|2020-09-04 08:07:...|                null|                null|        49|              2|{\"customerId\":437...|2021|\n",
      "|2021-01-01 23:59:59|19438963|         0|a6b32750-b978-5ce...|    IN_BOX|    null|     INBOX|             4342749|SUCCESS|AdaPundi|Anda telah mendap...| Send inbox success!| 2020-09-13 03:00:00|2020-09-13 03:03:...|2020-09-12 09:20:...|2020-09-13 03:03:...|                null|                null|       628|              1|{\"orderID\":884393...|2021|\n",
      "|2021-01-01 23:59:59|21026581|         0|a4d4d9bb-7b76-48e...|      PUSH|    null|       GCM|cRtG5nupkAI:APA91...|SUCCESS|AdaPundi|Selamat, Anda tel...|PUSH, lovina send...|2020-09-28 07:06:...|2020-09-28 07:06:...|2020-09-28 07:06:...|2020-09-28 07:06:...|                null|                null|        45|              2|{\"SCORE_AMOUNT\":\"...|2021|\n",
      "|2021-01-01 23:59:59|21746059|         0|0bd0d9e0-6ea7-5a4...|    IN_BOX|    null|     INBOX|             4323203|   READ|AdaPundi|Tarik tunai hari ...| Send inbox success!| 2020-10-04 10:00:00|2020-10-04 10:00:...|2020-10-04 09:14:...|2020-10-07 08:26:...|                null|                null|       331|              1|{\"orderID\":899154...|2021|\n",
      "|2021-01-01 23:59:59|21913304|         0|4cc203e3-2999-530...|    IN_BOX|    null|     INBOX|             2397343|SUCCESS|ADAPUNDI|Tarik tunai hari ...| Send inbox success!| 2020-10-06 03:32:00|2020-10-06 03:32:...|2020-10-05 23:04:...|2020-10-06 03:32:...|                null|                null|       430|              1|{\"orderID\":899247...|2021|\n",
      "|2021-01-01 23:59:59|22415772|         0|a47a221e-0f7c-4ac...|      PUSH|    null|       GCM|dzNZkQZdXC8:APA91...|SUCCESS|AdaPundi|Kredit Anda baik,...|PUSH, lovina send...| 2020-10-11 03:00:00|2020-10-11 03:05:...|2020-10-10 09:05:...|2020-10-11 03:05:...|                null|                null|        21|              1|{\"customerId\":803...|2021|\n",
      "|2021-01-01 23:59:59|26278980|         0|a9b62a61-f5d9-46a...|      PUSH|    null|       GCM|dDW057h0Eng:APA91...|SUCCESS|AdaPundi|Maaf, kualifikasi...|PUSH, lovina send...|2020-11-03 07:33:...|2020-11-03 07:33:...|2020-11-03 07:33:...|2020-11-03 07:33:...|                null|                null|        50|              2|{\"customerId\":450...|2021|\n",
      "+-------------------+--------+----------+--------------------+----------+--------+----------+--------------------+-------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+---------------+--------------------+----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df=spark.sql(\"\"\"\n",
    "with update_snapshot_notification_t_message as (\n",
    "  select *\n",
    "    from (\n",
    "      SELECT * ,\n",
    "         row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "         FROM `notification_stream_etl`.`t_message`\n",
    "         where date(year || '-' || month || '-' || day) <=date('2021-02-25') and date(year || '-' || month || '-' || day) >= date('2021-02-24')\n",
    "      )\n",
    "    where row_num = 1\n",
    "  )\n",
    "select update_snapshot_notification_t_message.etldate ,update_snapshot_notification_t_message.id ,update_snapshot_notification_t_message.product_id ,update_snapshot_notification_t_message.ref_id ,update_snapshot_notification_t_message.type ,update_snapshot_notification_t_message.sub_type ,update_snapshot_notification_t_message.channel ,update_snapshot_notification_t_message.send_to ,update_snapshot_notification_t_message.status ,update_snapshot_notification_t_message.title ,update_snapshot_notification_t_message.content ,update_snapshot_notification_t_message.result_description ,update_snapshot_notification_t_message.expect_send_time ,update_snapshot_notification_t_message.actual_send_time ,update_snapshot_notification_t_message.create_time ,update_snapshot_notification_t_message.update_time ,update_snapshot_notification_t_message.avatar ,update_snapshot_notification_t_message.creative ,update_snapshot_notification_t_message.trigger_id ,update_snapshot_notification_t_message.trigger_type_id ,update_snapshot_notification_t_message.params  ,update_snapshot_notification_t_message.year from (\n",
    "  select *\n",
    "  from `notification_stream_etl`.`t_message`\n",
    "  where\n",
    "    date(year || '-' || month || '-' || day) <=date('2021-02-25') and date(year || '-' || month || '-' || day) >= date('2021-02-24')\n",
    "    and kind = 'insert'\n",
    "  ) new\n",
    "left join update_snapshot_notification_t_message\n",
    "on new.id = update_snapshot_notification_t_message.id where if(update_snapshot_notification_t_message.kind is not null,update_snapshot_notification_t_message.kind,'') <> 'delete'\n",
    "and  update_snapshot_notification_t_message.id=48326289\n",
    "\"\"\").show()\n",
    "# df=df.drop(\"etlindex\")\n",
    "# df.drop(\"col_x\").write.mode(\"overwrite\").partitionBy(\"year\").format(\"orc\").saveAsTable(\"etl_s3_temp.notification_t_message\")\n",
    "# spark.sql(\"select  * from  \"+\"etl_s3_temp.notification_t_message\").show()\n",
    "# print(real_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
