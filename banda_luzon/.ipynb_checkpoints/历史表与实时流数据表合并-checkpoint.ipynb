{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ce6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "tablemap={\"banda_emr_etl\":\"banda\"} #,\n",
    "first=\" select 'insert' as kind, '2021-01-01 23:59:59' as etldate,0 as  etlindex, \"\n",
    "end=\" , '2021' as year,   '01' as month,  '01' as day from  \"\n",
    "def getTableColum(tableNm,b):\n",
    "    colum=\"\"\n",
    "    for index in range(len(b)):\n",
    "        colum=colum+setDef(b[index][\"data_type\"],b[index][\"col_name\"],tableNm)+\", \"\n",
    "    return colum[0:len(colum)-2]\n",
    "def setDef(type,table_col,tableNm):\n",
    "    if(type[:7] == 'boolean'):\n",
    "        return \"cast (\"+table_col+\" as string ) as \"+table_col\n",
    "    else:\n",
    "        return table_col\n",
    "def make_col_temptable(databaseName,row,stream_tableList):\n",
    "    tableName=row[\"tableName\"]\n",
    "    if tableName=='t_trigger_group'  :\n",
    "        tablecolum=\"desc \" +databaseName+\".\"+tableName;\n",
    "        tableSchema= spark.sql(tablecolum).collect()\n",
    "        sql=first+ getTableColum(tableName,tableSchema)+end+databaseName+\".\"+tableName\n",
    "        path=\"s3://rupiahplus-data-warehouse/stream/\"+tablemap[databaseName]+\"_etl/\"+tableName+\"/\"\n",
    "    #         print(sql)\n",
    "        spark.sql(sql).write.mode(\"append\").partitionBy(\"year\",\"month\",\"day\").orc(path)\n",
    "if __name__ == \"builtins\":\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Python Demo\")\\\n",
    "        .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate() \n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "    df=spark.sql(\"\"\"show tables in banda_stream_etl\"\"\").collect()\n",
    "    stream_tableList=[]\n",
    "    for i in df:\n",
    "        stream_tableList.append(i['tableName'])\n",
    "    for databaseName in tablemap:\n",
    "        databasesql=\"show tables in \"+\"banda_emr_etl\"\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(make_col_temptable,databaseName, tableNm,stream_tableList) for tableNm in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
