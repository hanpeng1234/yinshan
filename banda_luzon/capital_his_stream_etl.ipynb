{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf49149f914649d3bb833e36d73913de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1623318899558_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-3-0-25.ap-southeast-1.compute.internal:20888/proxy/application_1623318899558_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-3-0-50.ap-southeast-1.compute.internal:8042/node/containerlogs/container_1623318899558_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_capital_account\n",
      "t_capital_account_record\n",
      "t_debt_record\n",
      "t_interest_record\n",
      "t_mq_listener_original\n",
      "t_order_original\n",
      "t_package\n",
      "t_package_details\n",
      "t_package_details_record\n",
      "t_repay_record\n",
      "t_settlement_record\n",
      "t_subject\n",
      "t_workday_schedule\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "tablemap={\"capital_emr_etl\":\"capital\"} #,\n",
    "first=\" select 'insert' as kind, '2021-01-01 23:59:59' as etldate,0 as  etlindex, \"\n",
    "end=\" , '2021' as year,   '01' as month,  '01' as day from  \"\n",
    "def getTableColum(tableNm,b):\n",
    "    colum=\"\"\n",
    "    for index in range(len(b)):\n",
    "        colum=colum+setDef(b[index][\"data_type\"],b[index][\"col_name\"],tableNm)+\", \"\n",
    "    return colum[0:len(colum)-2]\n",
    "def setDef(type,table_col,tableNm):\n",
    "    if(type[:7] == 'boolean' ) :\n",
    "        return \"cast (\"+table_col+\" as string ) as \"+table_col\n",
    "#     elif tableNm=='t_package_details' and  type == 'timestamp' and (table_col=='start_time' or table_col=='end_time') :\n",
    "#         return \"substring (\"+table_col+\",11,9 ) as \"+table_col\n",
    "    else:\n",
    "        return table_col\n",
    "def make_col_temptable(databaseName,row,stream_tableList):\n",
    "    tableName=row[\"tableName\"]\n",
    "    print(tableName)\n",
    "#     if tableName in stream_tableList and (tableName=='t_thirdparty_data' or tableName=='t_clear_detail_log') :\n",
    "    if  tableName=='t_case_distribution_log' :\n",
    "        tablecolum=\"desc \" +databaseName+\".\"+tableName;\n",
    "        tableSchema= spark.sql(tablecolum).collect()\n",
    "        sql=first+ getTableColum(tableName,tableSchema)+end+databaseName+\".\"+tableName\n",
    "        path=\"s3://rupiahplus-data-warehouse/stream/\"+tablemap[databaseName]+\"_etl/\"+tableName+\"/\"\n",
    "        spark.sql(sql).write.mode(\"append\").partitionBy(\"year\",\"month\",\"day\").orc(path)\n",
    "if __name__ == \"builtins\":\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Python Demo\")\\\n",
    "        .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "#     spark.conf.set(\"spark.scheduler.mode\",\"FAIR\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "    df=spark.sql(\"\"\"show tables in banda_stream_etl\"\"\").collect()\n",
    "    stream_tableList=[]\n",
    "    for i in df:\n",
    "        stream_tableList.append(i['tableName'])\n",
    "    for databaseName in tablemap:\n",
    "        databasesql=\"show tables in \"+\"capital_emr_etl\"\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(make_col_temptable,databaseName, tableNm,stream_tableList) for tableNm in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348f0183f7a048c3bcc6e93546fe3b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1611540126777_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-3-0-4.ap-southeast-1.compute.internal:20888/proxy/application_1611540126777_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-3-0-20.ap-southeast-1.compute.internal:8042/node/containerlogs/container_1611540126777_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_time: string (nullable = true)\n",
      " |-- end_time: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- package_id: long (nullable = true)\n",
      " |-- capital_account_record_id: long (nullable = true)\n",
      " |-- charge_amount: decimal(18,2) (nullable = true)\n",
      " |-- available_amount: decimal(18,2) (nullable = true)\n",
      " |-- subject_type: string (nullable = true)\n",
      " |-- match_priority: short (nullable = true)\n",
      " |-- opened: short (nullable = true)\n",
      " |-- workday: short (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- end_time: timestamp (nullable = true)\n",
      " |-- buy_back_rule: string (nullable = true)\n",
      " |-- day_rate: decimal(16,6) (nullable = true)\n",
      " |-- create_time: timestamp (nullable = true)\n",
      " |-- update_time: timestamp (nullable = true)\n",
      " |-- limit_amount: decimal(18,2) (nullable = true)\n",
      " |-- swap_package_details_id: long (nullable = true)\n",
      " |-- user_type: short (nullable = true)\n",
      " |-- interest_type: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select  substring(start_time,) as start_time, substring(end_time,11,9) as end_time ,* from capital_emr_etl.t_package_details\n",
    "\n",
    "\"\"\").printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
