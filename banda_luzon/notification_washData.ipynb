{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e513b8e2e32400ab88b95bb01a4c51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1620358127926_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-3-0-35.ap-southeast-1.compute.internal:20888/proxy/application_1620358127926_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-3-0-112.ap-southeast-1.compute.internal:8042/node/containerlogs/container_1620358127926_0003_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting boto3\n",
      "  Downloading https://files.pythonhosted.org/packages/11/2d/804332ee1eaf36c8be737e9c44da2f2aa449339c220a96b9a15ae7f61443/boto3-1.17.69-py2.py3-none-any.whl (131kB)\n",
      "Collecting s3transfer<0.5.0,>=0.4.0 (from boto3)\n",
      "  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3)\n",
      "Collecting botocore<1.21.0,>=1.20.69 (from boto3)\n",
      "  Downloading https://files.pythonhosted.org/packages/10/bb/06710dce8770adf852785785df7e15fc1363596b712766898b1c529e358e/botocore-1.20.69-py2.py3-none-any.whl (7.5MB)\n",
      "Collecting urllib3<1.27,>=1.25.4 (from botocore<1.21.0,>=1.20.69->boto3)\n",
      "  Downloading https://files.pythonhosted.org/packages/09/c6/d3e3abe5b4f4f16cf0dfc9240ab7ce10c2baa0e268989a4e3ec19e90c84e/urllib3-1.26.4-py2.py3-none-any.whl (153kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.21.0,>=1.20.69->boto3)\n",
      "  Downloading https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl (227kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.69->boto3)\n",
      "Installing collected packages: urllib3, python-dateutil, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.17.69 botocore-1.20.69 python-dateutil-2.8.1 s3transfer-0.4.2 urllib3-1.26.4\n",
      "\n",
      "<function hmac_sha256 at 0x7fe2c9a1dd40>\n",
      "notification_stream_etl\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Cannot uninstall requirement boto3, not installed"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#notification导表到notification_etl_s3\n",
    "def install():\n",
    "    sc.uninstall_package('boto3')\n",
    "    sc.install_pypi_package(\"boto3\")\n",
    "install()\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "import boto3\n",
    "import json\n",
    "import pytz\n",
    "import hmac\n",
    "import pyspark.sql.functions as F\n",
    "from hashlib import sha256 \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# print(datetime.now(pytz.timezone(\"Asia/Shanghai\")).strftime( '%Y-%m-%d'))\n",
    "today=(datetime.now()+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "yesterday=(datetime.now(pytz.timezone(\"Asia/Shanghai\"))+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "tablemap={\"notification_stream_etl\":\"notification\"} \n",
    "# {\"notification_stream_etl\":\"notification\"} #,\n",
    "# {\"telemarket_stream_etl\":\"telemarket\"} #,\n",
    "# ,\"banda_stream_etl\":\"1\"\n",
    "tablesql_1=\" FROM  (   SELECT    *  , row_number() OVER (PARTITION BY id  ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC  , if(etlindex is null ,0,etlindex)  desc)  row_num FROM  \"\n",
    "tablesql_2=\" ORDER BY id  ASC )  a  WHERE a.row_num = 1   AND a.kind <> 'delete' \"\n",
    "# 替换手机号\n",
    "def get_secret_obj():\n",
    "    df=spark.read.text(\"s3://rupiahplus-configs/etl/data_secrt/col.json\").collect()\n",
    "    keymap=''\n",
    "    for  row in df:\n",
    "        keymap=keymap+row['value']\n",
    "    json_content=json.loads(keymap)\n",
    "    return json_content\n",
    "colmap= get_secret_obj()\n",
    "mobileType=colmap[\"mobileType\"]\n",
    "def getRelmobile(colNm,str):\n",
    "    if colNm in mobileType:\n",
    "        relmobile = str.strip().replace(\"+\", \"\").replace(\"-\", \"\").replace(\" \", \"\").replace('\"', '')\n",
    "        if (relmobile.startswith(\"62\")):\n",
    "            relmobile = relmobile.replace(\"62\", \"0\",1)\n",
    "        if relmobile.startswith(\"0\") == False:\n",
    "            relmobile = \"0\" + relmobile;\n",
    "        return relmobile\n",
    "    return str\n",
    "# hmac_256加密\n",
    "def hmac_sha256(key,colNm,value):\n",
    "    if value!=None:\n",
    "#         处理手机号\n",
    "        rtn=getRelmobile(colNm,value)\n",
    "        h = hmac.new(key.encode('utf-8'),digestmod=sha256)\n",
    "        h.update(rtn.encode('utf-8'))\n",
    "        h_str = h.hexdigest()\n",
    "        return h_str\n",
    "spark.udf.register(\"hmac_sha256\",hmac_sha256,T.StringType())\n",
    "def getTableColum(b):\n",
    "    colum=\"\"\n",
    "    for index in range(len(b)):\n",
    "        if(index>0 and index<len(b)-8):   \n",
    "            colum=colum+setDef(b[index][\"data_type\"],b[index][\"col_name\"])+\", \"\n",
    "    return colum[0:len(colum)-2]\n",
    "def setDef(type,table_col):\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull(\"+table_col+\",0) as \"+table_col\n",
    "    else:\n",
    "        return table_col\n",
    "def add_secret_col(dbType,tableNm,col):\n",
    "    secret_Col=[]\n",
    "    if colmap[tablemap[databaseName]]!=None and colmap[tablemap[databaseName]].get(tableNm)!=None:\n",
    "        secret_Col=colmap[tablemap[databaseName]][tableNm]\n",
    "    sh256Col =col\n",
    "    hmac_key=colmap[\"hmac_key\"]\n",
    "#     sql=\"select hmac_sha256('{hmac_key}','123456789') as sh2\".format(hmac_key=hmac_key)\n",
    "    for i in secret_Col:\n",
    "        sh256Col=sh256Col+ \", hmac_sha256('{hmac_key}','\"+i+\"', \"+i+\") \"+ i+\"_x\"\n",
    "    return  sh256Col.format(hmac_key=hmac_key)\n",
    "def make_col_temptable(databaseName,row):\n",
    "    tableName=row[\"tableName\"]\n",
    "    if tableName=='t_message':\n",
    "        tablecolum=\"desc \" +databaseName+\".\"+tableName;\n",
    "        tableSchema= spark.sql(tablecolum).collect()\n",
    "        tablepath=\"s3://rupiahplus-data-warehouse/aliyun/\"+tablemap[databaseName]+\"/\"+row[\"tableName\"]\n",
    "        spark.catalog.refreshTable(databaseName+\".\"+tableName)\n",
    "        secret_sql=add_secret_col(tablemap[databaseName],tableName,getTableColum(tableSchema))+\", year\"\n",
    "        sql=\"select \"+secret_sql+ tablesql_1+databaseName+\".\"+tableName+tablesql_2\n",
    "        df=spark.sql(sql).drop(\"etlindex\")\n",
    "        if colmap[tablemap[databaseName]].get(tableName)!=None :\n",
    "            for i in colmap[tablemap[databaseName]][tableName]:\n",
    "                df=df.withColumn(\"col_x\",F.col(i+\"_x\")).withColumn(i+\"_x\",F.col(i)).withColumn(i,F.col(\"col_x\"))\n",
    "        df.drop(\"col_x\").write.mode(\"overwrite\").partitionBy(\"year\").orc(tablepath)\n",
    "if __name__ == \"builtins\":\n",
    "#     注册udf\n",
    "    spark.udf.register(\"hmac_sha256\",hmac_sha256,T.StringType())\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Python Demo\")\\\n",
    "        .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "#     spark.conf.set(\"spark.scheduler.mode\",\"FAIR\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "    for databaseName in tablemap:\n",
    "        print(databaseName)\n",
    "        databasesql=\"show tables in \"+databaseName\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(make_col_temptable,databaseName, tableNm) for tableNm in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164ef8307d494f04a0312353090c2e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start----- 2021-02-26 09:57:25.016937\n",
      "with update_snapshot_notification_t_message as (\n",
      "  select *\n",
      "    from (\n",
      "      SELECT * ,\n",
      "         row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
      "         FROM `notification_stream_etl`.`t_message`\n",
      "         where date(year || '-' || month || '-' || day) <=date('2021-02-26') and date(year || '-' || month || '-' || day) >= date('2021-02-25')\n",
      "      )\n",
      "    where row_num = 1\n",
      "  )\n",
      "select  if(update_snapshot_notification_t_message.id is null, l.etldate     , update_snapshot_notification_t_message.etldate)  etldate ,if(update_snapshot_notification_t_message.id is null, l.id     , update_snapshot_notification_t_message.id)  id ,if(update_snapshot_notification_t_message.id is null, l.product_id     , update_snapshot_notification_t_message.product_id)  product_id ,if(update_snapshot_notification_t_message.id is null, l.ref_id     , update_snapshot_notification_t_message.ref_id)  ref_id ,if(update_snapshot_notification_t_message.id is null, l.type     , update_snapshot_notification_t_message.type)  type ,if(update_snapshot_notification_t_message.id is null, l.sub_type     , update_snapshot_notification_t_message.sub_type)  sub_type ,if(update_snapshot_notification_t_message.id is null, l.channel     , update_snapshot_notification_t_message.channel)  channel ,if(update_snapshot_notification_t_message.id is null, l.send_to_x     , update_snapshot_notification_t_message.send_to)  send_to ,if(update_snapshot_notification_t_message.id is null, l.status     , update_snapshot_notification_t_message.status)  status ,if(update_snapshot_notification_t_message.id is null, l.title     , update_snapshot_notification_t_message.title)  title ,if(update_snapshot_notification_t_message.id is null, l.content     , update_snapshot_notification_t_message.content)  content ,if(update_snapshot_notification_t_message.id is null, l.result_description     , update_snapshot_notification_t_message.result_description)  result_description ,if(update_snapshot_notification_t_message.id is null, l.expect_send_time     , update_snapshot_notification_t_message.expect_send_time)  expect_send_time ,if(update_snapshot_notification_t_message.id is null, l.actual_send_time     , update_snapshot_notification_t_message.actual_send_time)  actual_send_time ,if(update_snapshot_notification_t_message.id is null, l.create_time     , update_snapshot_notification_t_message.create_time)  create_time ,if(update_snapshot_notification_t_message.id is null, l.update_time     , update_snapshot_notification_t_message.update_time)  update_time ,if(update_snapshot_notification_t_message.id is null, l.avatar     , update_snapshot_notification_t_message.avatar)  avatar ,if(update_snapshot_notification_t_message.id is null, l.creative     , update_snapshot_notification_t_message.creative)  creative ,if(update_snapshot_notification_t_message.id is null, l.trigger_id     , update_snapshot_notification_t_message.trigger_id)  trigger_id ,if(update_snapshot_notification_t_message.id is null, l.trigger_type_id     , update_snapshot_notification_t_message.trigger_type_id)  trigger_type_id ,if(update_snapshot_notification_t_message.id is null, l.params     , update_snapshot_notification_t_message.params)  params , if(update_snapshot_notification_t_message.id is null, l.send_to     , hmac_sha256('xUIAZ3grtPOxaPNK','send_to',update_snapshot_notification_t_message.send_to)) send_to_x , if(update_snapshot_notification_t_message.id is null, l.year     , update_snapshot_notification_t_message.year)  year \n",
      "from `notification_etl_s3`.`t_message` l\n",
      "left join update_snapshot_notification_t_message\n",
      "on l.id = update_snapshot_notification_t_message.id\n",
      "where if(update_snapshot_notification_t_message.kind is not null,update_snapshot_notification_t_message.kind,'') <> 'delete'\n",
      "union\n",
      "select update_snapshot_notification_t_message.etldate ,update_snapshot_notification_t_message.id ,update_snapshot_notification_t_message.product_id ,update_snapshot_notification_t_message.ref_id ,update_snapshot_notification_t_message.type ,update_snapshot_notification_t_message.sub_type ,update_snapshot_notification_t_message.channel ,update_snapshot_notification_t_message.send_to ,update_snapshot_notification_t_message.status ,update_snapshot_notification_t_message.title ,update_snapshot_notification_t_message.content ,update_snapshot_notification_t_message.result_description ,update_snapshot_notification_t_message.expect_send_time ,update_snapshot_notification_t_message.actual_send_time ,update_snapshot_notification_t_message.create_time ,update_snapshot_notification_t_message.update_time ,update_snapshot_notification_t_message.avatar ,update_snapshot_notification_t_message.creative ,update_snapshot_notification_t_message.trigger_id ,update_snapshot_notification_t_message.trigger_type_id ,update_snapshot_notification_t_message.params ,hmac_sha256('xUIAZ3grtPOxaPNK','send_to', update_snapshot_notification_t_message.send_to) send_to_x ,update_snapshot_notification_t_message.year from (\n",
      "  select *\n",
      "  from `notification_stream_etl`.`t_message`\n",
      "  where\n",
      "    date(year || '-' || month || '-' || day) <=date('2021-02-26') and date(year || '-' || month || '-' || day) >= date('2021-02-25')\n",
      "    and kind = 'insert'\n",
      "  ) new\n",
      "left join update_snapshot_notification_t_message\n",
      "on new.id = update_snapshot_notification_t_message.id where if(update_snapshot_notification_t_message.kind is not null,update_snapshot_notification_t_message.kind,'') <> 'delete' \n",
      "None\n",
      "end----- 2021-02-26 09:57:26.066944\n",
      "None\n",
      "end----- 2021-02-26 09:57:26.066970\n",
      "None\n",
      "end----- 2021-02-26 09:57:26.066981\n",
      "None\n",
      "end----- 2021-02-26 09:57:26.066991\n",
      "None\n",
      "end----- 2021-02-26 09:57:26.067000\n",
      "None\n",
      "end----- 2021-02-26 09:57:26.067010"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime,timedelta\n",
    "import copy\n",
    "import json\n",
    "import pytz\n",
    "import hmac\n",
    "from hashlib import sha256 \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import types as T\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Python Demo\")\\\n",
    "    .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "spark.conf.set(\"spark.files.overwrite\",\"true\")\n",
    "#     ,Apache Arrow：一个跨平台的在内存中以列式存储的数据层，用来加速大数据分析速度。其可以一次性传入更大块的数据，pyspark中已经有载入该模块，需要打开该设置：\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "#     创建表时自动删除已存在的目录\n",
    "spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "dbmap={\n",
    "    \"notification\":{\n",
    "        \"increment_database\":\"notification_stream_etl\",\n",
    "        \"database\":\"notification_etl_s3\",\n",
    "        \"realTime_path\":\"notification\"\n",
    "    },\n",
    "#      \"telemarket\":{\n",
    "#          \"increment_database\":\"telemarket_stream_etl\",\n",
    "#          \"database\":\"telemarket_etl_s3\",\n",
    "#          \"realTime_path\":\"telemarket\"\n",
    "#     }\n",
    "}\n",
    "nowdate=(datetime.now()).strftime('%Y-%m-%d')\n",
    "yesterday=(datetime.now()+ timedelta(-1)).strftime('%Y-%m-%d')\n",
    "sqltemp=\"\"\"with {update_snapshot_table} as (\n",
    "  select *\n",
    "    from (\n",
    "      SELECT * ,\n",
    "         row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "         FROM `{increment_database}`.`{tableNm}`\n",
    "         where date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "      )\n",
    "    where row_num = 1\n",
    "  )\n",
    "select  {select_sql} \n",
    "from `{database}`.`{tableNm}` l\n",
    "left join {update_snapshot_table}\n",
    "on l.id = {update_snapshot_table}.id\n",
    "where if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete'\n",
    "union\n",
    "select {update_snapshot_sql} from (\n",
    "  select *\n",
    "  from `{increment_database}`.`{tableNm}`\n",
    "  where\n",
    "    date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "    and kind = 'insert'\n",
    "  ) new\n",
    "left join {update_snapshot_table}\n",
    "on new.id = {update_snapshot_table}.id where if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete' \"\"\"\n",
    "def get_secret_obj():\n",
    "    df=spark.read.text(\"s3://rupiahplus-configs/etl/data_secrt/col.json\").collect()\n",
    "    keymap=''\n",
    "    for  row in df:\n",
    "        keymap=keymap+row['value']\n",
    "    json_content=json.loads(keymap)\n",
    "    return json_content\n",
    "colmap= get_secret_obj()   \n",
    "hmac_key=colmap[\"hmac_key\"]\n",
    "mobileType=colmap[\"mobileType\"]\n",
    "def getRelmobile(colNm,str):\n",
    "    if colNm in mobileType:\n",
    "        relmobile = str.strip().replace(\"+\", \"\").replace(\"-\", \"\").replace(\" \", \"\").replace('\"', '')\n",
    "        if (relmobile.startswith(\"62\")):\n",
    "            relmobile = relmobile.replace(\"62\", \"0\",1)\n",
    "        if relmobile.startswith(\"0\") == False:\n",
    "            relmobile = \"0\" + relmobile;\n",
    "        return relmobile\n",
    "    return str\n",
    "# hmac_256加密\n",
    "def hmac_sha256(key,colNm,value):\n",
    "    if value!=None and value!='':\n",
    "#         处理手机号\n",
    "        rtn=getRelmobile(colNm,value)\n",
    "        h = hmac.new(key.encode('utf-8'),digestmod=sha256)\n",
    "        h.update(rtn.encode('utf-8'))\n",
    "        h_str = h.hexdigest()\n",
    "        return h_str\n",
    "spark.udf.register(\"hmac_sha256\",hmac_sha256,T.StringType())\n",
    "def getTableColum(b,update_snapshot_table,dbtype,tableNm):\n",
    "    colum=\"\"\n",
    "    snapshot_colum=\"\"\n",
    "    sqlstrtemp=\"if({update_snapshot_table}.id is null, {colNm}     , {snapshot_colNm})  {ascolNm} ,\"\n",
    "    sqlstr_secret_temp=\" if({update_snapshot_table}.id is null, {colNm}     , hmac_sha256('{hmac_key}','{ascolNm}',{snapshot_colNm})) {ascolNm}_x , \"\n",
    "    for index in range(len(b)):\n",
    "        if(index==1 or (index>2 and index<len(b)-7)):\n",
    "#             print(index,b[index])\n",
    "#             在year前面加上加密列\n",
    "            if(index==len(b)-8)and  colmap.get(dbtype)!=None and colmap[dbtype]!=None and colmap[dbtype].get(tableNm)!=None:\n",
    "                for secret_col in colmap[dbtype][tableNm]:\n",
    "                    sqlstr=copy.copy(sqlstr_secret_temp)\n",
    "                    colum=colum+sqlstr.format(colNm=setDef(\"string\",secret_col,False),snapshot_colNm=setDef(\"string\",secret_col,True),ascolNm=secret_col,update_snapshot_table=update_snapshot_table,hmac_key=hmac_key)\n",
    "                    snapshot_colum=snapshot_colum+setsecret_SnapshotDef(secret_col).format(update_snapshot_table=update_snapshot_table,hmac_key=hmac_key)+\" ,\"\n",
    "            sqlstr=copy.copy(sqlstrtemp)\n",
    "            colum=colum+sqlstr.format(colNm=isX(dbtype,tableNm,setDef(b[index][\"data_type\"],b[index][\"col_name\"],False)),snapshot_colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],True),ascolNm=b[index][\"col_name\"],update_snapshot_table=update_snapshot_table)\n",
    "            snapshot_colum=snapshot_colum+setSnapshotDef(b[index][\"data_type\"],b[index][\"col_name\"]).format(update_snapshot_table=update_snapshot_table)+\" ,\"\n",
    "    return colum[0:len(colum)-2],snapshot_colum[0:len(snapshot_colum)-2]\n",
    "def isX(type,table,column):\n",
    "    columnList=column.split(\".\")\n",
    "    if len(columnList)>0 and  colmap.get(type)!=None and colmap[type].get(table)!=None and columnList[len(columnList)-1] in colmap[type][table]:\n",
    "        return column+\"_x\"\n",
    "    return column\n",
    "def setDef(type,table_col,is_snapshot):\n",
    "    table_col_x=copy.copy(table_col)\n",
    "    if(type[:7] == 'decimal'):\n",
    "        if is_snapshot:\n",
    "            return \"ifnull({update_snapshot_table}.\"+table_col_x+\",0)  \"\n",
    "        else:\n",
    "            return \"ifnull(l.\"+table_col_x+\",0) \"\n",
    "    else:\n",
    "        if is_snapshot:\n",
    "            return \"{update_snapshot_table}.\"+table_col_x\n",
    "        else:\n",
    "            return \"l.\"+table_col_x\n",
    "def setSnapshotDef(type,table_col):\n",
    "    table_col_x=copy.copy(table_col)\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull({update_snapshot_table}.\"+table_col_x+\",0) \"+table_col\n",
    "    else:\n",
    "        return \"{update_snapshot_table}.\"+table_col_x\n",
    "def setsecret_SnapshotDef(table_col):\n",
    "    table_col_x=copy.copy(table_col)\n",
    "#     print(\"hmac_sha256('{hmac_key}','\"+table_col_x+\"', {update_snapshot_table}.\"+table_col_x+\") \"+ table_col_x+\"_x\")\n",
    "    return \"hmac_sha256('{hmac_key}','\"+table_col_x+\"', {update_snapshot_table}.\"+table_col_x+\") \"+ table_col_x+\"_x\"\n",
    "#     return \"sha2({update_snapshot_table}.\"+table_col+\",256) \" +table_col+\"_x\"\n",
    "def execute(tablerow,dbtype):\n",
    "#     print(tablerow,dbtype)\n",
    "    tableName=tablerow[\"tableName\"]\n",
    "    if tableName=='t_message':\n",
    "        update_snapshot_table=\"update_snapshot_\"+dbtype+\"_\"+tableName\n",
    "        spark.catalog.refreshTable(\"`\"+dbmap[dbtype][\"database\"]+\"`.\"+tableName)\n",
    "        col=spark.sql(\"desc `\"+dbmap[dbtype][\"increment_database\"]+\"`.\"+tableName).collect()\n",
    "        #     #真正的列,增量的列\n",
    "        real_col,snapshot_col=getTableColum(col,update_snapshot_table,dbtype,tableName)\n",
    "        sql=copy.copy(sqltemp)\n",
    "        real_sql=sql.format(increment_database=dbmap[dbtype][\"increment_database\"],tableNm=tableName,nowdate=nowdate,select_sql=real_col.format(update_snapshot_table=update_snapshot_table),update_snapshot_sql=snapshot_col.format(update_snapshot_table=update_snapshot_table),database=dbmap[dbtype][\"database\"],yesterday=yesterday,update_snapshot_table=update_snapshot_table,hmac_key=hmac_key)\n",
    "#         df=spark.sql(real_sql).drop(\"etlindex\")\n",
    "#         if colmap.get(dbtype)!=None and colmap[dbtype].get(tableName)!=None :\n",
    "#             for i in colmap[dbtype][tableName]:\n",
    "#                 df=df.withColumn(\"col_x\",F.col(i+\"_x\")).withColumn(i+\"_x\",F.col(i)).withColumn(i,F.col(\"col_x\"))\n",
    "#         df.drop(\"col_x\").write.mode(\"overwrite\").partitionBy(\"year\").format(\"orc\").saveAsTable(\"etl_s3_temp.\"+dbtype+\"_\"+tableName);\n",
    "#         tablepath='s3://rupiahplus-data-warehouse/aliyun/'+dbmap[dbtype][\"realTime_path\"]+'/'+tableName+\"/\"\n",
    "#         spark.sql(\"select  * from  \"+\"etl_s3_temp.\"+dbtype+\"_\"+tableName).write.mode(\"overwrite\").partitionBy(\"year\").orc(tablepath)\n",
    "        print(real_sql)\n",
    "if __name__ == \"builtins\":\n",
    "    print(\"start-----\",datetime.now())\n",
    "    for dbtype in dbmap:\n",
    "        databasesql=\"show tables in \"+dbmap[dbtype][\"increment_database\"]\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "    #         for row in tablelist:\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(execute, table,dbtype) for table in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())\n",
    "                print(\"end-----\",datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43e6d12861d45cc924081725d1dfb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+--------------------+----+--------+-------+--------------------+-------+--------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+------+--------+----------+---------------+--------------------+----+\n",
      "|             etldate|      id|product_id|              ref_id|type|sub_type|channel|             send_to| status|   title|             content|  result_description|   expect_send_time|    actual_send_time|         create_time|         update_time|avatar|creative|trigger_id|trigger_type_id|              params|year|\n",
      "+--------------------+--------+----------+--------------------+----+--------+-------+--------------------+-------+--------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+------+--------+----------+---------------+--------------------+----+\n",
      "|2021-02-25 03:06:...|48326289|         0|a47cb909-4b5a-521...|PUSH|    null|    GCM|e50iiDFZcZE:APA91...|SUCCESS|ADAPUNDI|\"body\":\"Ajukan pi...|PUSH, lovina send...|2021-02-25 03:00:00|2021-02-25 03:06:...|2021-02-24 23:04:...|2021-02-25 03:06:...|  null|    null|      1448|              1|{\"orderID\":104035...|2021|\n",
      "+--------------------+--------+----------+--------------------+----+--------+-------+--------------------+-------+--------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+------+--------+----------+---------------+--------------------+----+"
     ]
    }
   ],
   "source": [
    "df=spark.sql(\"\"\"\n",
    "with update_snapshot_notification_t_message as (\n",
    "  select *\n",
    "    from (\n",
    "      SELECT * ,\n",
    "         row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "         FROM `notification_stream_etl`.`t_message`\n",
    "         where date(year || '-' || month || '-' || day) <=date('2021-02-25') and date(year || '-' || month || '-' || day) >= date('2021-02-24')\n",
    "      )\n",
    "    where row_num = 1\n",
    "  )\n",
    "select update_snapshot_notification_t_message.etldate ,update_snapshot_notification_t_message.id ,update_snapshot_notification_t_message.product_id ,update_snapshot_notification_t_message.ref_id ,update_snapshot_notification_t_message.type ,update_snapshot_notification_t_message.sub_type ,update_snapshot_notification_t_message.channel ,update_snapshot_notification_t_message.send_to ,update_snapshot_notification_t_message.status ,update_snapshot_notification_t_message.title ,update_snapshot_notification_t_message.content ,update_snapshot_notification_t_message.result_description ,update_snapshot_notification_t_message.expect_send_time ,update_snapshot_notification_t_message.actual_send_time ,update_snapshot_notification_t_message.create_time ,update_snapshot_notification_t_message.update_time ,update_snapshot_notification_t_message.avatar ,update_snapshot_notification_t_message.creative ,update_snapshot_notification_t_message.trigger_id ,update_snapshot_notification_t_message.trigger_type_id ,update_snapshot_notification_t_message.params  ,update_snapshot_notification_t_message.year from (\n",
    "  select *\n",
    "  from `notification_stream_etl`.`t_message`\n",
    "  where\n",
    "    date(year || '-' || month || '-' || day) <=date('2021-02-25') and date(year || '-' || month || '-' || day) >= date('2021-02-24')\n",
    "    and kind = 'insert'\n",
    "  ) new\n",
    "left join update_snapshot_notification_t_message\n",
    "on new.id = update_snapshot_notification_t_message.id where if(update_snapshot_notification_t_message.kind is not null,update_snapshot_notification_t_message.kind,'') <> 'delete'\n",
    "and  update_snapshot_notification_t_message.id=48326289\n",
    "\"\"\").show()\n",
    "# df=df.drop(\"etlindex\")\n",
    "# df.drop(\"col_x\").write.mode(\"overwrite\").partitionBy(\"year\").format(\"orc\").saveAsTable(\"etl_s3_temp.notification_t_message\")\n",
    "# spark.sql(\"select  * from  \"+\"etl_s3_temp.notification_t_message\").show()\n",
    "# print(real_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select  if(update_snapshot_notification_t_message.id is null, l.etldate     , update_snapshot_notification_t_message.etldate)  etldate ,if(update_snapshot_notification_t_message.id is null, l.id     , update_snapshot_notification_t_message.id)  id ,if(update_snapshot_notification_t_message.id is null, l.product_id     , update_snapshot_notification_t_message.product_id)  product_id ,if(update_snapshot_notification_t_message.id is null, l.ref_id     , update_snapshot_notification_t_message.ref_id)  ref_id ,if(update_snapshot_notification_t_message.id is null, l.type     , update_snapshot_notification_t_message.type)  type ,if(update_snapshot_notification_t_message.id is null, l.sub_type     , update_snapshot_notification_t_message.sub_type)  sub_type ,if(update_snapshot_notification_t_message.id is null, l.channel     , update_snapshot_notification_t_message.channel)  channel ,if(update_snapshot_notification_t_message.id is null, l.send_to_x     , update_snapshot_notification_t_message.send_to)  send_to ,if(update_snapshot_notification_t_message.id is null, l.status     , update_snapshot_notification_t_message.status)  status ,if(update_snapshot_notification_t_message.id is null, l.title     , update_snapshot_notification_t_message.title)  title ,if(update_snapshot_notification_t_message.id is null, l.content     , update_snapshot_notification_t_message.content)  content ,if(update_snapshot_notification_t_message.id is null, l.result_description     , update_snapshot_notification_t_message.result_description)  result_description ,if(update_snapshot_notification_t_message.id is null, l.expect_send_time     , update_snapshot_notification_t_message.expect_send_time)  expect_send_time ,if(update_snapshot_notification_t_message.id is null, l.actual_send_time     , update_snapshot_notification_t_message.actual_send_time)  actual_send_time ,if(update_snapshot_notification_t_message.id is null, l.create_time     , update_snapshot_notification_t_message.create_time)  create_time ,if(update_snapshot_notification_t_message.id is null, l.update_time     , update_snapshot_notification_t_message.update_time)  update_time ,if(update_snapshot_notification_t_message.id is null, l.avatar     , update_snapshot_notification_t_message.avatar)  avatar ,if(update_snapshot_notification_t_message.id is null, l.creative     , update_snapshot_notification_t_message.creative)  creative ,if(update_snapshot_notification_t_message.id is null, l.trigger_id     , update_snapshot_notification_t_message.trigger_id)  trigger_id ,if(update_snapshot_notification_t_message.id is null, l.trigger_type_id     , update_snapshot_notification_t_message.trigger_type_id)  trigger_type_id ,if(update_snapshot_notification_t_message.id is null, l.params     , update_snapshot_notification_t_message.params)  params , if(update_snapshot_notification_t_message.id is null, l.year     , update_snapshot_notification_t_message.year)  year \n",
    "from `notification_etl_s3`.`t_message` l\n",
    "left join update_snapshot_notification_t_message\n",
    "on l.id = update_snapshot_notification_t_message.id\n",
    "where if(update_snapshot_notification_t_message.kind is not null,update_snapshot_notification_t_message.kind,'') <> 'delete'\n",
    "and l.id<>48326289\n",
    "union"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
