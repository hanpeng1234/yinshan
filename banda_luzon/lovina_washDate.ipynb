{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5691a7f11c1c42e1902c06f8c88ec04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1622692591997_0006</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-3-0-41.ap-southeast-1.compute.internal:20888/proxy/application_1622692591997_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-3-0-94.ap-southeast-1.compute.internal:8042/node/containerlogs/container_1622692591997_0006_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "builtins\n",
      "<function hmac_sha256 at 0x7fe627c9f5f0>\n",
      "lovina_stream_etl\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#notification导表到notification_etl_s3\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "import json\n",
    "import pytz\n",
    "import hmac\n",
    "import pyspark.sql.functions as F\n",
    "from hashlib import sha256 \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# print(datetime.now(pytz.timezone(\"Asia/Shanghai\")).strftime( '%Y-%m-%d'))\n",
    "today=(datetime.now()+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "yesterday=(datetime.now(pytz.timezone(\"Asia/Shanghai\"))+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "tablemap={\"lovina_stream_etl\":\"lovina\"} #,\n",
    "# ,\"banda_stream_etl\":\"1\"\n",
    "tablesql_1=\" FROM  (   SELECT    *  , row_number() OVER (PARTITION BY id  ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC  , if(etlindex is null ,0,etlindex)  desc)  row_num FROM  \"\n",
    "tablesql_2=\" ORDER BY id  ASC )  a  WHERE a.row_num = 1   AND a.kind <> 'delete' \"\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Python Demo\")\\\n",
    "    .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "#     spark.conf.set(\"spark.scheduler.mode\",\"FAIR\")\n",
    "# 替换手机号\n",
    "def get_secret_obj():\n",
    "    df=spark.read.text(\"s3://rupiahplus-configs/etl/data_secrt/col.json\").collect()\n",
    "    keymap=''\n",
    "    for  row in df:\n",
    "        keymap=keymap+row['value']\n",
    "    json_content=json.loads(keymap)\n",
    "    return json_content\n",
    "colmap= get_secret_obj()\n",
    "mobileType=colmap[\"mobileType\"]\n",
    "def getRelmobile(colNm,str):\n",
    "    if colNm in mobileType:\n",
    "        relmobile = str.strip().replace(\"+\", \"\").replace(\"-\", \"\").replace(\" \", \"\").replace('\"', '')\n",
    "        if (relmobile.startswith(\"62\")):\n",
    "            relmobile = relmobile.replace(\"62\", \"0\",1)\n",
    "        if relmobile.startswith(\"0\") == False:\n",
    "            relmobile = \"0\" + relmobile;\n",
    "        return relmobile\n",
    "    return str\n",
    "# hmac_256加密\n",
    "def hmac_sha256(key,colNm,value):\n",
    "    if value!=None:\n",
    "#         处理手机号\n",
    "        rtn=getRelmobile(colNm,value)\n",
    "        h = hmac.new(key.encode('utf-8'),digestmod=sha256)\n",
    "        h.update(rtn.encode('utf-8'))\n",
    "        h_str = h.hexdigest()\n",
    "        return h_str\n",
    "    return \"\"\n",
    "spark.udf.register(\"hmac_sha256\",hmac_sha256,T.StringType())\n",
    "def getTableColum(b):\n",
    "    colum=\"\"\n",
    "    for index in range(len(b)):\n",
    "        if(index>0 and index<len(b)-8):   \n",
    "            colum=colum+setDef(b[index][\"data_type\"],b[index][\"col_name\"])+\", \"\n",
    "    return colum[0:len(colum)-2]\n",
    "def setDef(type,table_col):\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull(\"+table_col+\",0) as \"+table_col\n",
    "    else:\n",
    "        return table_col\n",
    "def add_secret_col(dbType,tableNm,col):\n",
    "    secret_Col=[]\n",
    "    if colmap.get(dbType)!=None and colmap[dbType]!=None and colmap[dbType].get(tableNm)!=None:\n",
    "        secret_Col=colmap[tablemap[databaseName]][tableNm]\n",
    "    sh256Col =col\n",
    "    hmac_key=colmap[\"hmac_key\"]\n",
    "#     sql=\"select hmac_sha256('{hmac_key}','123456789') as sh2\".format(hmac_key=hmac_key)\n",
    "    for i in secret_Col:\n",
    "        sh256Col=sh256Col+ \", hmac_sha256('{hmac_key}','\"+i+\"', \"+i+\") \"+ i+\"_x\"\n",
    "    return sh256Col.format(hmac_key=hmac_key)\n",
    "def make_col_temptable(databaseName,row):\n",
    "    tableName=row[\"tableName\"]\n",
    "    if tableName=='t_channel' or tableName=='t_department':\n",
    "        tablecolum=\"desc \" +databaseName+\".\"+tableName;\n",
    "        tableSchema= spark.sql(tablecolum).collect()\n",
    "        tablepath=\"s3://rupiahplus-data-warehouse/aliyun/\"+tablemap[databaseName]+\"/\"+row[\"tableName\"]\n",
    "        spark.catalog.refreshTable(databaseName+\".\"+tableName)\n",
    "        dbtype=tablemap[databaseName]\n",
    "        secret_sql=add_secret_col(dbtype,tableName,getTableColum(tableSchema))+\", year\"\n",
    "        sql=\"select \"+secret_sql+ tablesql_1+databaseName+\".\"+tableName+tablesql_2\n",
    "        df=spark.sql(sql).drop(\"etlindex\")\n",
    "        if colmap.get(dbtype) and colmap[dbtype].get(tableName)!=None :\n",
    "            for i in colmap[tablemap[databaseName]][tableName]:\n",
    "                df=df.withColumn(\"col_x\",F.col(i+\"_x\")).withColumn(i+\"_x\",F.col(i)).withColumn(i,F.col(\"col_x\"))\n",
    "        if colmap[\"extraColumn\"]!=None and colmap[\"extraColumn\"].get(dbtype)!=None and colmap[\"extraColumn\"][dbtype].get(tableName)!=None and len(colmap[\"extraColumn\"][dbtype][tableName])>0:\n",
    "            colmap[\"extraColumn\"][dbtype][tableName].insert(0,\"*\")\n",
    "            df=df.selectExpr(colmap[\"extraColumn\"][dbtype][tableName])\n",
    "            df=df.withColumn(\"year_new\",F.col(\"year\")).drop(\"year\").withColumnRenamed(\"year_new\",\"year\")\n",
    "    #     df.drop(\"col_x\").show()\n",
    "        df.drop(\"col_x\").write.mode(\"overwrite\").partitionBy(\"year\").orc(tablepath)\n",
    "print(__name__)\n",
    "if __name__ == \"builtins\":\n",
    "#     注册udf\n",
    "    spark.udf.register(\"hmac_sha256\",hmac_sha256,T.StringType())\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "    for databaseName in tablemap:\n",
    "        print(databaseName)\n",
    "        databasesql=\"show tables in \"+databaseName\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(make_col_temptable,databaseName, tableNm) for tableNm in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
