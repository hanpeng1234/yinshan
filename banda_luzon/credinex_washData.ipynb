{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\n",
    ">\n",
    ">\n",
    ">  带加密列的全量etl\n",
    "\n",
    ">\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85484b72507b4db3a60b49e2a6a76268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1623315754995_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-3-0-61.ap-southeast-1.compute.internal:20888/proxy/application_1623315754995_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-3-0-41.ap-southeast-1.compute.internal:8042/node/containerlogs/container_1623315754995_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069501e4234f4fce80557e9a2f6f5f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#notification导表到notification_etl_s3\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "import json\n",
    "import pytz\n",
    "import hmac\n",
    "import pyspark.sql.functions as F\n",
    "from hashlib import sha256 \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# print(datetime.now(pytz.timezone(\"Asia/Shanghai\")).strftime( '%Y-%m-%d'))\n",
    "today=(datetime.now()+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "yesterday=(datetime.now(pytz.timezone(\"Asia/Shanghai\"))+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "# \"arkham_stream_etl\":\"arkham\"\"protoss_stream_etl\":\"protoss\",\"credinex_account_stream_etl\":\"credinex_account\",\"credinex_repeater_stream_etl\":\"credinex_repeater\",\"credinex_hive_stream_etl\":\"credinex_hive\"\n",
    "tablemap={\"credinex_account_stream_etl\":\"credinex_account\"} \n",
    "# ,\"banda_stream_etl\":\"1\"\n",
    "tablesql_1=\" FROM  (   SELECT    *  , row_number() OVER (PARTITION BY id  ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC  , if(etlindex is null ,0,etlindex)  desc)  row_num FROM  \"\n",
    "tablesql_2=\" ORDER BY id  ASC )  a  WHERE a.row_num = 1   AND a.kind <> 'delete' \"\n",
    "# 替换手机号\n",
    "def get_secret_obj():\n",
    "    df=spark.read.text(\"s3://rupiahplus-configs/etl/data_secrt/col.json\").collect()\n",
    "    keymap=''\n",
    "    for  row in df:\n",
    "        keymap=keymap+row['value']\n",
    "    json_content=json.loads(keymap)\n",
    "    return json_content\n",
    "colmap= get_secret_obj()\n",
    "print(colmap)\n",
    "mobileType=colmap[\"mobileType\"]\n",
    "def getRelmobile(colNm,str):\n",
    "    if colNm in mobileType:\n",
    "        relmobile = str.strip().replace(\"+\", \"\").replace(\"-\", \"\").replace(\" \", \"\").replace('\"', '')\n",
    "        if (relmobile.startswith(\"62\")):\n",
    "            relmobile = relmobile.replace(\"62\", \"0\",1)\n",
    "        if relmobile.startswith(\"0\") == False:\n",
    "            relmobile = \"0\" + relmobile;\n",
    "        return relmobile\n",
    "    return str\n",
    "# hmac_256加密\n",
    "def hmac_sha256(key,colNm,value):\n",
    "    if value!=None:\n",
    "#         处理手机号\n",
    "        rtn=getRelmobile(colNm,value)\n",
    "        h = hmac.new(key.encode('utf-8'),digestmod=sha256)\n",
    "        h.update(rtn.encode('utf-8'))\n",
    "        h_str = h.hexdigest()\n",
    "        return h_str\n",
    "spark.udf.register(\"hmac_sha256\",hmac_sha256,T.StringType())\n",
    "def getTableColum(b):\n",
    "    colum=\"\"\n",
    "    for index in range(len(b)):\n",
    "        if(index>0 and index<len(b)-8):   \n",
    "            colum=colum+setDef(b[index][\"data_type\"],b[index][\"col_name\"])+\", \"\n",
    "    return colum[0:len(colum)-2]\n",
    "def setDef(type,table_col):\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull(\"+table_col+\",0) as \"+table_col\n",
    "    else:\n",
    "        return table_col\n",
    "def add_secret_col(dbType,tableNm,col):\n",
    "    print(dbType,tableNm)\n",
    "    secret_Col=[]\n",
    "    if colmap.get(dbType)!=None and colmap[dbType]!=None and colmap[dbType].get(tableNm)!=None:\n",
    "        secret_Col=colmap[tablemap[databaseName]][tableNm]\n",
    "    sh256Col =col\n",
    "    hmac_key=colmap[\"hmac_key\"]\n",
    "#     sql=\"select hmac_sha256('{hmac_key}','123456789') as sh2\".format(hmac_key=hmac_key)\n",
    "    for i in secret_Col:\n",
    "        sh256Col=sh256Col+ \", hmac_sha256('{hmac_key}','\"+i+\"', \"+i+\") \"+ i+\"_x\"\n",
    "    return  sh256Col.format(hmac_key=hmac_key)\n",
    "def make_col_temptable(databaseName,row):\n",
    "    tableName=row[\"tableName\"]\n",
    "#     if tableName=='t_pay_order':\n",
    "    tablecolum=\"desc \" +databaseName+\".\"+tableName;\n",
    "    tableSchema= spark.sql(tablecolum).collect()\n",
    "    tablepath=\"s3://rupiahplus-data-warehouse/aliyun/\"+tablemap[databaseName]+\"/\"+row[\"tableName\"]\n",
    "#         spark.catalog.refreshTable(databaseName+\".\"+tableName)\n",
    "    secret_sql=add_secret_col(tablemap[databaseName],tableName,getTableColum(tableSchema))+\", year\"\n",
    "    sql=\"select \"+secret_sql+ tablesql_1+databaseName+\".\"+tableName+tablesql_2\n",
    "    print(sql)\n",
    "    df=spark.sql(sql).drop(\"etlindex\")\n",
    "    if colmap.get(tablemap[databaseName]) and colmap[tablemap[databaseName]].get(tableName)!=None :\n",
    "        for i in colmap[tablemap[databaseName]][tableName]:\n",
    "            df=df.withColumn(\"col_x\",F.col(i+\"_x\")).withColumn(i+\"_x\",F.col(i)).withColumn(i,F.col(\"col_x\"))\n",
    "    df.drop(\"col_x\").write.mode(\"overwrite\").partitionBy(\"year\").orc(tablepath)\n",
    "if __name__ == \"builtins\":\n",
    "#     注册udf\n",
    "    spark.udf.register(\"hmac_sha256\",hmac_sha256,T.StringType())\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Python Demo\")\\\n",
    "        .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "#     spark.conf.set(\"spark.scheduler.mode\",\"FAIR\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "    for databaseName in tablemap:\n",
    "        print(databaseName)\n",
    "        databasesql=\"show tables in \"+databaseName\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(make_col_temptable,databaseName, tableNm) for tableNm in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "import copy\n",
    "dbmap={\n",
    "#     \"arkham\":{\n",
    "#         \"increment_database\":\"arkham_stream_etl\",\n",
    "#         \"database\":\"arkham_etl_s3\",\n",
    "#         \"realTime_path\":\"arkham\"        \n",
    "#     },\n",
    "    \"protoss\":{\n",
    "        \"increment_database\":\"protoss_stream_etl\",\n",
    "        \"database\":\"protoss_etl_s3\",\n",
    "        \"realTime_path\":\"protoss\"        \n",
    "    },\n",
    "#     \"credinex_hive\":{\n",
    "#         \"increment_database\":\"credinex_hive_stream_etl\",\n",
    "#         \"database\":\"credinex_hive_etl_s3\",\n",
    "#         \"realTime_path\":\"credinex_hive\"        \n",
    "#     },\n",
    "#     \"credinex_repeater\":{\n",
    "#         \"increment_database\":\"credinex_repeater_stream_etl\",\n",
    "#         \"database\":\"credinex_repeater_etl_s3\",\n",
    "#         \"realTime_path\":\"credinex_repeater\"        \n",
    "#     },\n",
    "#     \"credinex_account\":{\n",
    "#         \"increment_database\":\"credinex_account_stream_etl\",\n",
    "#         \"database\":\"credinex_account_etl_s3\",\n",
    "#         \"realTime_path\":\"credinex_account\"        \n",
    "#     },\n",
    "}\n",
    "nowdate=(datetime.now()).strftime('%Y-%m-%d')\n",
    "yesterday=(datetime.now()+ timedelta(-1)).strftime('%Y-%m-%d')\n",
    "sqltemp=\"\"\"with {update_snapshot_table} as (\n",
    "  select *\n",
    "    from (\n",
    "      SELECT * ,\n",
    "         row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "      FROM `{increment_database}`.`{tableNm}`\n",
    "      where date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "      )\n",
    "    where row_num = 1\n",
    "  )\n",
    "select  {select_sql} \n",
    "from `{database}`.`{tableNm}` l\n",
    "left join {update_snapshot_table}\n",
    "on l.id = {update_snapshot_table}.id\n",
    "where if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete'\n",
    "union\n",
    "select {update_snapshot_sql} from (\n",
    "  select *\n",
    "  from `{increment_database}`.`{tableNm}`\n",
    "  where\n",
    "    date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "    and kind = 'insert'\n",
    "  ) new\n",
    "left join {update_snapshot_table}\n",
    "on new.id = {update_snapshot_table}.id where if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete' \"\"\"\n",
    "def getTableColum(b,update_snapshot_table):\n",
    "    colum=\" \"\n",
    "    snapshot_colum=\" \"\n",
    "    sqlstrtemp=\"if({update_snapshot_table}.id is null, {colNm}     , {snapshot_colNm})  {ascolNm} ,\"\n",
    "    for index in range(len(b)):\n",
    "        if(index==1 or (index>2 and index<len(b)-7)):   \n",
    "            sqlstr=copy.copy(sqlstrtemp)\n",
    "            colum=colum+sqlstr.format(colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],False),snapshot_colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],True),ascolNm=b[index][\"col_name\"],update_snapshot_table=update_snapshot_table)\n",
    "            snapshot_colum=snapshot_colum+setSnapshotDef(b[index][\"data_type\"],b[index][\"col_name\"]).format(update_snapshot_table=update_snapshot_table)+\" ,\"\n",
    "    return colum[0:len(colum)-2],snapshot_colum[0:len(snapshot_colum)-2]\n",
    "def setDef(type,table_col,is_snapshot):\n",
    "    if(type[:7] == 'decimal'):\n",
    "        if is_snapshot:\n",
    "            return \"ifnull({update_snapshot_table}.\"+table_col+\",0)  \"\n",
    "        else:\n",
    "            return \"ifnull(l.\"+table_col+\",0) \"\n",
    "    else:\n",
    "        if is_snapshot:\n",
    "            return \"{update_snapshot_table}.\"+table_col\n",
    "        else:\n",
    "            return \"l.\"+table_col\n",
    "def setSnapshotDef(type,table_col):\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull({update_snapshot_table}.\"+table_col+\",0) \"+table_col\n",
    "    else:\n",
    "        return \"{update_snapshot_table}.\"+table_col\n",
    "def execute(tablerow,dbtype):\n",
    "#     print(tablerow,dbtype)\n",
    "    tableNm=tablerow[\"tableName\"]\n",
    "    update_snapshot_table=\"update_snapshot_\"+dbtype+\"_\"+tableNm\n",
    "    spark.catalog.refreshTable(\"`\"+dbmap[dbtype][\"database\"]+\"`.\"+tableNm)\n",
    "    col=spark.sql(\"desc `\"+dbmap[dbtype][\"increment_database\"]+\"`.\"+tableNm).collect()\n",
    "    #     #真正的列,增量的列\n",
    "    real_col,snapshot_col=getTableColum(col,update_snapshot_table)\n",
    "    sql=copy.copy(sqltemp)\n",
    "    real_sql=sql.format(increment_database=dbmap[dbtype][\"increment_database\"],tableNm=tableNm,nowdate=nowdate,select_sql=real_col.format(update_snapshot_table=update_snapshot_table),update_snapshot_sql=snapshot_col.format(update_snapshot_table=update_snapshot_table),database=dbmap[dbtype][\"database\"],yesterday=yesterday,update_snapshot_table=update_snapshot_table)\n",
    "    spark.sql(real_sql).drop(\"etlindex\").write.option(\"path\", \"s3://rupiahplus-data-warehouse/stream/etl_s3_temp/\"+dbtype+\"_\"+tableName).mode(\"overwrite\").partitionBy(\"year\").format(\"orc\").saveAsTable(\"etl_s3_temp.\"+dbtype+\"_\"+tableNm);\n",
    "    tablepath='s3://rupiahplus-data-warehouse/aliyun/'+dbmap[dbtype][\"realTime_path\"]+'/'+tableNm+\"/\"\n",
    "    spark.sql(\"select  * from  \"+\"etl_s3_temp.\"+dbtype+\"_\"+tableNm).write.mode(\"overwrite\").partitionBy(\"year\").orc(tablepath)\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Python Demo\")\\\n",
    "    .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "    spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "    spark.conf.set(\"spark.files.overwrite\",\"true\")\n",
    "#     ,Apache Arrow：一个跨平台的在内存中以列式存储的数据层，用来加速大数据分析速度。其可以一次性传入更大块的数据，pyspark中已经有载入该模块，需要打开该设置：\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "#     创建表时自动删除已存在的目录\n",
    "    spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "    for dbtype in dbmap:\n",
    "        databasesql=\"show tables in \"+dbmap[dbtype][\"increment_database\"]\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "    #         for row in tablelist:\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(execute, table,dbtype) for table in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>\n",
    ">\n",
    ">\n",
    ">正式credinex全量\n",
    ">\n",
    ">\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208d4da0f1b04615915317e833feb81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start----- 2021-06-10 09:25:56.106397\n",
      "None\n",
      "end----- 2021-06-10 09:27:39.151138\n",
      "None\n",
      "end----- 2021-06-10 09:27:39.151184\n",
      "None\n",
      "end----- 2021-06-10 09:27:39.151208\n",
      "None\n",
      "end----- 2021-06-10 09:27:39.151231\n",
      "None\n",
      "end----- 2021-06-10 09:27:39.151273\n",
      "None\n",
      "end----- 2021-06-10 09:27:39.151294\n",
      "None\n",
      "end----- 2021-06-10 09:27:39.151315\n",
      "None\n",
      "end----- 2021-06-10 09:27:39.151338\n",
      "None\n",
      "end----- 2021-06-10 09:27:39.151359\n",
      "None\n",
      "end----- 2021-06-10 09:27:39.151382\n",
      "None\n",
      "end----- 2021-06-10 09:27:39.151404"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime,timedelta\n",
    "import copy\n",
    "import json\n",
    "import pytz\n",
    "import hmac\n",
    "from hashlib import sha256 \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import types as T\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Python Demo\")\\\n",
    "    .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "spark.conf.set(\"spark.files.overwrite\",\"true\")\n",
    "#     ,Apache Arrow：一个跨平台的在内存中以列式存储的数据层，用来加速大数据分析速度。其可以一次性传入更大块的数据，pyspark中已经有载入该模块，需要打开该设置：\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "#     创建表时自动删除已存在的目录\n",
    "dbmap={\n",
    "#     \"arkham\":{\n",
    "#         \"increment_database\":\"arkham_stream_etl\",\n",
    "#         \"database\":\"arkham_etl_s3\",\n",
    "#         \"realTime_path\":\"arkham\"\n",
    "#     },\n",
    "#     \"protoss\":{\n",
    "#         \"increment_database\":\"protoss_stream_etl\",\n",
    "#         \"database\":\"protoss_etl_s3\",\n",
    "#         \"realTime_path\":\"protoss\"\n",
    "#     },\n",
    "#     \"credinex_repeater\":{\n",
    "#         \"increment_database\":\"credinex_repeater_stream_etl\",\n",
    "#         \"database\":\"credinex_repeater_etl_s3\",\n",
    "#         \"realTime_path\":\"credinex_repeater\"\n",
    "#     },\n",
    "    \"credinex_account\":{\n",
    "        \"increment_database\":\"credinex_account_stream_etl\",\n",
    "        \"database\":\"credinex_account_etl_s3\",\n",
    "        \"realTime_path\":\"credinex_account\"\n",
    "    },\n",
    "#     \"credinex_hive\":{\n",
    "#         \"increment_database\":\"credinex_hive_stream_etl\",\n",
    "#         \"database\":\"credinex_hive_etl_s3\",\n",
    "#         \"realTime_path\":\"credinex_hive\"\n",
    "#     }\n",
    "}\n",
    "nowdate=(datetime.now()).strftime('%Y-%m-%d')\n",
    "yesterday=(datetime.now()+ timedelta(-1)).strftime('%Y-%m-%d')\n",
    "sqltemp=\"\"\"with {update_snapshot_table} as (\n",
    "  select *\n",
    "    from (\n",
    "      SELECT * ,\n",
    "         row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "         FROM `{increment_database}`.`{tableNm}`\n",
    "         where date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "      )\n",
    "    where row_num = 1\n",
    "  )\n",
    "select  {select_sql} \n",
    "from `{database}`.`{tableNm}` l\n",
    "left join {update_snapshot_table}\n",
    "on l.id = {update_snapshot_table}.id\n",
    "where if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete'\n",
    "union\n",
    "select {update_snapshot_sql} from (\n",
    "  select *\n",
    "  from `{increment_database}`.`{tableNm}`\n",
    "  where\n",
    "    date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "    and kind = 'insert'\n",
    "  ) new\n",
    "left join {update_snapshot_table}\n",
    "on new.id = {update_snapshot_table}.id where if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete' \"\"\"\n",
    "def get_secret_obj():\n",
    "    df=spark.read.text(\"s3://rupiahplus-configs/etl/data_secrt/col.json\").collect()\n",
    "    keymap=''\n",
    "    for  row in df:\n",
    "        keymap=keymap+row['value']\n",
    "    json_content=json.loads(keymap)\n",
    "    return json_content\n",
    "colmap= get_secret_obj()   \n",
    "hmac_key=colmap[\"hmac_key\"]\n",
    "mobileType=colmap[\"mobileType\"]\n",
    "def getRelmobile(colNm,str):\n",
    "    if colNm in mobileType:\n",
    "        relmobile = str.strip().replace(\"+\", \"\").replace(\"-\", \"\").replace(\" \", \"\").replace('\"', '')\n",
    "        if (relmobile.startswith(\"62\")):\n",
    "            relmobile = relmobile.replace(\"62\", \"0\",1)\n",
    "        if relmobile.startswith(\"0\") == False:\n",
    "            relmobile = \"0\" + relmobile;\n",
    "        return relmobile\n",
    "    return str\n",
    "# hmac_256加密\n",
    "def hmac_sha256(key,colNm,value):\n",
    "    if value!=None and value!='':\n",
    "#         处理手机号\n",
    "        rtn=getRelmobile(colNm,value)\n",
    "        h = hmac.new(key.encode('utf-8'),digestmod=sha256)\n",
    "        h.update(rtn.encode('utf-8'))\n",
    "        h_str = h.hexdigest()\n",
    "        return h_str\n",
    "spark.udf.register(\"hmac_sha256\",hmac_sha256,T.StringType())\n",
    "def getTableColum(b,update_snapshot_table,dbtype,tableNm):\n",
    "    colum=\"\"\n",
    "    snapshot_colum=\"\"\n",
    "    sqlstrtemp=\"if({update_snapshot_table}.id is null, {colNm}     , {snapshot_colNm})  {ascolNm} ,\"\n",
    "    sqlstr_secret_temp=\" if({update_snapshot_table}.id is null, {colNm}     , hmac_sha256('{hmac_key}','{ascolNm}',{snapshot_colNm})) {ascolNm}_x , \"\n",
    "    for index in range(len(b)):\n",
    "        if(index==1 or (index>2 and index<len(b)-7)):\n",
    "#             print(index,b[index])\n",
    "#             在year前面加上加密列\n",
    "            if(index==len(b)-8)and  colmap.get(dbtype)!=None and colmap[dbtype]!=None and colmap[dbtype].get(tableNm)!=None:\n",
    "                for secret_col in colmap[dbtype][tableNm]:\n",
    "                    sqlstr=copy.copy(sqlstr_secret_temp)\n",
    "                    colum=colum+sqlstr.format(colNm=setDef(\"string\",secret_col,False),snapshot_colNm=setDef(\"string\",secret_col,True),ascolNm=secret_col,update_snapshot_table=update_snapshot_table,hmac_key=hmac_key)\n",
    "                    snapshot_colum=snapshot_colum+setsecret_SnapshotDef(secret_col).format(update_snapshot_table=update_snapshot_table,hmac_key=hmac_key)+\" ,\"\n",
    "            sqlstr=copy.copy(sqlstrtemp)\n",
    "            colum=colum+sqlstr.format(colNm=isX(dbtype,tableNm,setDef(b[index][\"data_type\"],b[index][\"col_name\"],False)),snapshot_colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],True),ascolNm=b[index][\"col_name\"],update_snapshot_table=update_snapshot_table)\n",
    "            snapshot_colum=snapshot_colum+setSnapshotDef(b[index][\"data_type\"],b[index][\"col_name\"]).format(update_snapshot_table=update_snapshot_table)+\" ,\"\n",
    "    return colum[0:len(colum)-2],snapshot_colum[0:len(snapshot_colum)-2]\n",
    "def isX(type,table,column):\n",
    "    columnList=column.split(\".\")\n",
    "    if len(columnList)>0 and  colmap.get(type)!=None and colmap[type].get(table)!=None and columnList[len(columnList)-1] in colmap[type][table]:\n",
    "        return column+\"_x\"\n",
    "    return column\n",
    "def setDef(type,table_col,is_snapshot):\n",
    "    table_col_x=copy.copy(table_col)\n",
    "    if(type[:7] == 'decimal'):\n",
    "        if is_snapshot:\n",
    "            return \"ifnull({update_snapshot_table}.\"+table_col_x+\",0)  \"\n",
    "        else:\n",
    "            return \"ifnull(l.\"+table_col_x+\",0) \"\n",
    "    else:\n",
    "        if is_snapshot:\n",
    "            return \"{update_snapshot_table}.\"+table_col_x\n",
    "        else:\n",
    "            return \"l.\"+table_col_x\n",
    "def setSnapshotDef(type,table_col):\n",
    "    table_col_x=copy.copy(table_col)\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull({update_snapshot_table}.\"+table_col_x+\",0) \"+table_col\n",
    "    else:\n",
    "        return \"{update_snapshot_table}.\"+table_col_x\n",
    "def setsecret_SnapshotDef(table_col):\n",
    "    table_col_x=copy.copy(table_col)\n",
    "#     print(\"hmac_sha256('{hmac_key}','\"+table_col_x+\"', {update_snapshot_table}.\"+table_col_x+\") \"+ table_col_x+\"_x\")\n",
    "    return \"hmac_sha256('{hmac_key}','\"+table_col_x+\"', {update_snapshot_table}.\"+table_col_x+\") \"+ table_col_x+\"_x\"\n",
    "#     return \"sha2({update_snapshot_table}.\"+table_col+\",256) \" +table_col+\"_x\"\n",
    "def execute(tablerow,dbtype):\n",
    "#     print(tablerow,dbtype)\n",
    "    tableName=tablerow[\"tableName\"]\n",
    "    update_snapshot_table=\"update_snapshot_\"+dbtype+\"_\"+tableName\n",
    "    spark.catalog.refreshTable(\"`\"+dbmap[dbtype][\"database\"]+\"`.\"+tableName)\n",
    "    col=spark.sql(\"desc `\"+dbmap[dbtype][\"increment_database\"]+\"`.\"+tableName).collect()\n",
    "    #     #真正的列,增量的列\n",
    "    real_col,snapshot_col=getTableColum(col,update_snapshot_table,dbtype,tableName)\n",
    "    sql=copy.copy(sqltemp)\n",
    "    real_sql=sql.format(increment_database=dbmap[dbtype][\"increment_database\"],tableNm=tableName,nowdate=nowdate,select_sql=real_col.format(update_snapshot_table=update_snapshot_table),update_snapshot_sql=snapshot_col.format(update_snapshot_table=update_snapshot_table),database=dbmap[dbtype][\"database\"],yesterday=yesterday,update_snapshot_table=update_snapshot_table,hmac_key=hmac_key)\n",
    "    df=spark.sql(real_sql).drop(\"etlindex\")\n",
    "    if colmap.get(dbtype)!=None and colmap[dbtype].get(tableName)!=None :\n",
    "        for i in colmap[dbtype][tableName]:\n",
    "            df=df.withColumn(\"col_x\",F.col(i+\"_x\")).withColumn(i+\"_x\",F.col(i)).withColumn(i,F.col(\"col_x\"))\n",
    "    df.drop(\"col_x\").write.option(\"path\", \"s3://rupiahplus-data-warehouse/stream/etl_s3_temp/\"+dbtype+\"_\"+tableName).mode(\"overwrite\").partitionBy(\"year\").format(\"orc\").saveAsTable(\"etl_s3_temp.\"+dbtype+\"_\"+tableName);\n",
    "    tablepath='s3://rupiahplus-data-warehouse/aliyun/'+dbmap[dbtype][\"realTime_path\"]+'/'+tableName+\"/\"\n",
    "    spark.sql(\"select  * from  \"+\"etl_s3_temp.\"+dbtype+\"_\"+tableName).write.mode(\"overwrite\").partitionBy(\"year\").orc(tablepath)\n",
    "if __name__ == \"builtins\":\n",
    "    print(\"start-----\",datetime.now())\n",
    "    for dbtype in dbmap:\n",
    "        databasesql=\"show tables in \"+dbmap[dbtype][\"increment_database\"]\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "    #         for row in tablelist:\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(execute, table,dbtype) for table in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())\n",
    "                print(\"end-----\",datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c040e93f38df44ec82652d220adc8f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1613731873121_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-3-0-86.ap-southeast-1.compute.internal:20888/proxy/application_1613731873121_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-3-0-5.ap-southeast-1.compute.internal:8042/node/containerlogs/container_1613731873121_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start----- 2021-02-20 03:05:51.723473\n",
      "with update_snapshot_arkham_t_user as (\n",
      "  select *\n",
      "    from (\n",
      "      SELECT * ,\n",
      "         row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
      "         FROM `arkham_stream_etl`.`t_user`\n",
      "         where date(year || '-' || month || '-' || day) <=date('2021-02-20') and date(year || '-' || month || '-' || day) >= date('2021-02-19')\n",
      "      and date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') >= (SELECT max(date_format(etldate, 'yyyy-MM-dd HH:mm:ss.SSS')) from  `arkham_etl_s3`.`t_user`) \n",
      "      )\n",
      "    where row_num = 1\n",
      "  )\n",
      "select  if(update_snapshot_arkham_t_user.id is null, l.etldate     , update_snapshot_arkham_t_user.etldate)  etldate ,if(update_snapshot_arkham_t_user.id is null, l.id     , update_snapshot_arkham_t_user.id)  id ,if(update_snapshot_arkham_t_user.id is null, l.uid     , update_snapshot_arkham_t_user.uid)  uid ,if(update_snapshot_arkham_t_user.id is null, l.idtype     , update_snapshot_arkham_t_user.idtype)  idtype ,if(update_snapshot_arkham_t_user.id is null, l.idno_x     , update_snapshot_arkham_t_user.idno)  idno ,if(update_snapshot_arkham_t_user.id is null, l.email     , update_snapshot_arkham_t_user.email)  email ,if(update_snapshot_arkham_t_user.id is null, l.mobile_x     , update_snapshot_arkham_t_user.mobile)  mobile ,if(update_snapshot_arkham_t_user.id is null, l.name     , update_snapshot_arkham_t_user.name)  name ,if(update_snapshot_arkham_t_user.id is null, l.password     , update_snapshot_arkham_t_user.password)  password ,if(update_snapshot_arkham_t_user.id is null, l.status     , update_snapshot_arkham_t_user.status)  status ,if(update_snapshot_arkham_t_user.id is null, l.user_status     , update_snapshot_arkham_t_user.user_status)  user_status ,if(update_snapshot_arkham_t_user.id is null, l.type     , update_snapshot_arkham_t_user.type)  type ,if(update_snapshot_arkham_t_user.id is null, l.completed_step     , update_snapshot_arkham_t_user.completed_step)  completed_step ,if(update_snapshot_arkham_t_user.id is null, l.is_deleted     , update_snapshot_arkham_t_user.is_deleted)  is_deleted ,if(update_snapshot_arkham_t_user.id is null, l.is_ktpauth     , update_snapshot_arkham_t_user.is_ktpauth)  is_ktpauth ,if(update_snapshot_arkham_t_user.id is null, l.create_time     , update_snapshot_arkham_t_user.create_time)  create_time ,if(update_snapshot_arkham_t_user.id is null, l.update_time     , update_snapshot_arkham_t_user.update_time)  update_time , if(update_snapshot_arkham_t_user.id is null, l.idno     , hmac_sha256('xUIAZ3grtPOxaPNK','idno',update_snapshot_arkham_t_user.idno)) idno_x ,  if(update_snapshot_arkham_t_user.id is null, l.mobile     , hmac_sha256('xUIAZ3grtPOxaPNK','mobile',update_snapshot_arkham_t_user.mobile)) mobile_x , if(update_snapshot_arkham_t_user.id is null, l.year     , update_snapshot_arkham_t_user.year)  year \n",
      "from `arkham_etl_s3`.`t_user` l\n",
      "left join update_snapshot_arkham_t_user\n",
      "on l.id = update_snapshot_arkham_t_user.id\n",
      "where if(update_snapshot_arkham_t_user.kind is not null,update_snapshot_arkham_t_user.kind,'') <> 'delete'\n",
      "union\n",
      "select update_snapshot_arkham_t_user.etldate ,update_snapshot_arkham_t_user.id ,update_snapshot_arkham_t_user.uid ,update_snapshot_arkham_t_user.idtype ,update_snapshot_arkham_t_user.idno ,update_snapshot_arkham_t_user.email ,update_snapshot_arkham_t_user.mobile ,update_snapshot_arkham_t_user.name ,update_snapshot_arkham_t_user.password ,update_snapshot_arkham_t_user.status ,update_snapshot_arkham_t_user.user_status ,update_snapshot_arkham_t_user.type ,update_snapshot_arkham_t_user.completed_step ,update_snapshot_arkham_t_user.is_deleted ,update_snapshot_arkham_t_user.is_ktpauth ,update_snapshot_arkham_t_user.create_time ,update_snapshot_arkham_t_user.update_time ,hmac_sha256('xUIAZ3grtPOxaPNK','idno', update_snapshot_arkham_t_user.idno) idno_x ,hmac_sha256('xUIAZ3grtPOxaPNK','mobile', update_snapshot_arkham_t_user.mobile) mobile_x ,update_snapshot_arkham_t_user.year from (\n",
      "  select *\n",
      "  from `arkham_stream_etl`.`t_user`\n",
      "  where\n",
      "    date(year || '-' || month || '-' || day) <=date('2021-02-20') and date(year || '-' || month || '-' || day) >= date('2021-02-19')\n",
      "    and date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') >= (SELECT max(date_format(etldate, 'yyyy-MM-dd HH:mm:ss.SSS')) from  `arkham_etl_s3`.`t_user`) \n",
      "    and kind = 'insert'\n",
      "  ) new\n",
      "left join update_snapshot_arkham_t_user\n",
      "on new.id = update_snapshot_arkham_t_user.id where if(update_snapshot_arkham_t_user.kind is not null,update_snapshot_arkham_t_user.kind,'') <> 'delete' \n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964503\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964528\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964539\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964549\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964558\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964568\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964577\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964587\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964596\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964606\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964615\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964626\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964635\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964645\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964654\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964664\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964673\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964682\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964692\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964701\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964710\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964719\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964728\n",
      "None\n",
      "end----- 2021-02-20 03:05:57.964738"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499ecdf0d5724b4698ad0903a392c021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o721.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 38.0 failed 4 times, most recent failure: Lost task 3.3 in stage 38.0 (TID 2328, ip-10-3-0-75.ap-southeast-1.compute.internal, executor 29): java.lang.ArrayIndexOutOfBoundsException\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2175)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2124)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2123)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2123)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:990)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:990)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:990)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2355)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2304)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2293)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:146)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:159)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:158)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3664)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2737)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3655)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:106)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:207)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:88)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3653)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2737)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2944)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.ArrayIndexOutOfBoundsException\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 441, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 131, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o721.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 38.0 failed 4 times, most recent failure: Lost task 3.3 in stage 38.0 (TID 2328, ip-10-3-0-75.ap-southeast-1.compute.internal, executor 29): java.lang.ArrayIndexOutOfBoundsException\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2175)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2124)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2123)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2123)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:990)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:990)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:990)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2355)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2304)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2293)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:146)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:159)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:158)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3664)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2737)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3655)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:106)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:207)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:88)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3653)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2737)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2944)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.ArrayIndexOutOfBoundsException\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select etldate, etlindex, id, user_id, platform_name, relation_id,\n",
    "business_id, channel_type, client_name, transaction_date,\n",
    "third_id, ifnull(amount,0) as amount, from_bank, from_account_number, \n",
    "from_account_name, to_bank, to_account_number, to_account_name, d\n",
    "irection, currency_unit, business_type, pay_type, status, message,\n",
    "remark1, remark2, groups, finish_time, create_time, update_time, year \n",
    "FROM  ( \n",
    ")  a  WHERE a.row_num = 1   AND a.kind <> 'delete'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f173b43bf14f2b84af954086beb765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>6</td><td>application_1623121284325_0007</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-3-0-35.ap-southeast-1.compute.internal:20888/proxy/application_1623121284325_0007/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-3-0-59.ap-southeast-1.compute.internal:8042/node/containerlogs/container_1623121284325_0007_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o85.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 4 times, most recent failure: Lost task 3.3 in stage 1.0 (TID 424, ip-10-3-0-36.ap-southeast-1.compute.internal, executor 7): java.lang.ArrayIndexOutOfBoundsException\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2175)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2124)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2123)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2123)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:990)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:990)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:990)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2355)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2304)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2293)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:146)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:159)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:158)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3664)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2737)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3655)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:106)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:207)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:88)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3653)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2737)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2944)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.ArrayIndexOutOfBoundsException\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 441, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\", line 1305, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 131, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o85.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 4 times, most recent failure: Lost task 3.3 in stage 1.0 (TID 424, ip-10-3-0-36.ap-southeast-1.compute.internal, executor 7): java.lang.ArrayIndexOutOfBoundsException\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2175)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2124)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2123)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2123)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:990)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:990)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:990)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2355)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2304)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2293)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:146)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:159)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:158)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3664)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2737)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3655)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:106)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:207)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:88)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3653)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2737)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2944)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.ArrayIndexOutOfBoundsException\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT    *  , \n",
    "row_number() OVER (PARTITION BY id  ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC  , if(etlindex is null ,0,etlindex)  desc)  row_num \n",
    "FROM  protoss_stream_etl.t_pay_order ORDER BY id  ASC \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15d4a3b347d43bb839586cf2726e60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1623141101698_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-3-0-22.ap-southeast-1.compute.internal:20888/proxy/application_1623141101698_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-3-0-28.ap-southeast-1.compute.internal:8042/node/containerlogs/container_1623141101698_0003_01_000002/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+--------+------------------+-------+-------------+-----------+-----------+------------+-----------+--------------------+--------------------+----------+---------+-------------------+------------------+--------+-----------------+---------------+---------+-------------+-------------+-----------+-------+-------+------------------+------------------+------+--------------------+--------------------+--------------------+----+-----+---+\n",
      "|  kind|            etldate|etlindex|                id|user_id|platform_name|relation_id|business_id|channel_type|client_name|    transaction_date|            third_id|    amount|from_bank|from_account_number| from_account_name| to_bank|to_account_number|to_account_name|direction|currency_unit|business_type|   pay_type| status|message|           remark1|           remark2|groups|         finish_time|         create_time|         update_time|year|month|day|\n",
      "+------+-------------------+--------+------------------+-------+-------------+-----------+-----------+------------+-----------+--------------------+--------------------+----------+---------+-------------------+------------------+--------+-----------------+---------------+---------+-------------+-------------+-----------+-------+-------+------------------+------------------+------+--------------------+--------------------+--------------------+----+-----+---+\n",
      "|insert|2021-01-01 23:59:59|       0|559249693412220928|5705561|     ADAPANDI|    6378671|   11928462|         BCA|  bca-10002|2021-05-31 14:45:...|    2021053110992063|1820000.00| CENAIDJA|         5485939609|               DDL|CENAIDJA|       5385857788|         ESCROW|       IN|          IDR|      FUNDING|        BCA|SUCCESS|   null|REMARK1:0006378671|REMARK2:0005705561|    {}|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "|insert|2021-01-01 23:59:59|       0|559249697882566656|4393255|     ADAPANDI|    6378672|   11916891|         BCA|  bca-10001|2021-05-31 14:45:...|    2021053110992064|5700000.00| CENAIDJA|         5385857788|              null|CENAIDJA|       6560054023|           null|      OUT|          IDR| TRANSFER_OUT|        BCA|SUCCESS|   null|REMARK1:0006378672|REMARK2:0004393255|    {}|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "|insert|2021-01-01 23:59:59|       0|559249699656757248|4534001|     ADAPANDI|    6378674|   11926144|         BCA|  bca-10002|2021-05-31 14:45:...|    2021053110992065|1800000.00| CENAIDJA|         5485939609|               DDL|CENAIDJA|       5385857788|         ESCROW|       IN|          IDR|      FUNDING|        BCA|SUCCESS|   null|REMARK1:0006378674|REMARK2:0004534001|    {}|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "|insert|2021-01-01 23:59:59|       0|559249700173438976|4111556|     ADAPANDI|    6378675|   11926097|         BCA|  bca-10002|2021-05-31 14:45:...|    2021053110992066|3000000.00| CENAIDJA|         5485939609|               DDL|CENAIDJA|       5385857788|         ESCROW|       IN|          IDR|      FUNDING|        BCA|SUCCESS|   null|REMARK1:0006378675|REMARK2:0004111556|    {}|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "|insert|2021-01-01 23:59:59|       0|559249700353794048|5375609|     ADAPANDI|    6378676|   11928091|         BCA|  bca-10002|2021-05-31 14:45:...|    2021053110992067|2000000.00| CENAIDJA|         5485939609|               DDL|CENAIDJA|       5385857788|         ESCROW|       IN|          IDR|      FUNDING|        BCA|SUCCESS|   null|REMARK1:0006378676|REMARK2:0005375609|    {}|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "|insert|2021-01-01 23:59:59|       0|559249700373983232|5487776|     ADAPANDI|    6378677|   11917206|         BCA|  bca-10002|2021-05-31 14:45:...|    2021053110992068| 600000.00| CENAIDJA|         5485939609|               DDL|CENAIDJA|       5385857788|         ESCROW|       IN|          IDR|      FUNDING|        BCA|SUCCESS|   null|REMARK1:0006378677|REMARK2:0005487776|    {}|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "|insert|2021-01-01 23:59:59|       0|559249704715870208|5657108|     ADAPANDI|    6378679|   11927789|         BCA|  bca-10002|2021-05-31 14:45:...|    2021053110992069| 420000.00| CENAIDJA|         5485939609|               DDL|CENAIDJA|       5385857788|         ESCROW|       IN|          IDR|      FUNDING|        BCA|SUCCESS|   null|REMARK1:0006378679|REMARK2:0005657108|    {}|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "|insert|2021-01-01 23:59:59|       0|559249705444896768|5302535|     ADAPANDI|    6378681|   11924868|         BCA|  bca-10002|2021-05-31 14:45:...|    2021053110992070|2180000.00| CENAIDJA|         5485939609|               DDL|CENAIDJA|       5385857788|         ESCROW|       IN|          IDR|      FUNDING|        BCA|SUCCESS|   null|REMARK1:0006378681|REMARK2:0005302535|    {}|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "|insert|2021-01-01 23:59:59|       0|559249706237620224|4002720|     ADAPANDI|       null|       null|         BCA|  bca-10002| 2021-05-31 07:45:29|20210531144512122...|  88000.00|     null|   1224500004002720|     DITA LISTIANI|    null|             null|          12245|       IN|          IDR|    REPAYMENT|        ATM|SUCCESS|   null|              null|              null|  null|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "|insert|2021-01-01 23:59:59|       0|559249706703187968|5103836|     ADAPANDI|    6378684|   11928385|         BCA|  bca-10002|2021-05-31 14:45:...|    2021053110992071|1310000.00| CENAIDJA|         5485939609|               DDL|CENAIDJA|       5385857788|         ESCROW|       IN|          IDR|      FUNDING|        BCA|SUCCESS|   null|REMARK1:0006378684|REMARK2:0005103836|    {}|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "|insert|2021-01-01 23:59:59|       0|559249706796244992|5356805|     ADAPANDI|    6378685|   11928267|         BCA|  bca-10002|2021-05-31 14:45:...|    2021053110992072|1000000.00| CENAIDJA|         5485939609|               DDL|CENAIDJA|       5385857788|         ESCROW|       IN|          IDR|      FUNDING|        BCA|SUCCESS|   null|REMARK1:0006378685|REMARK2:0005356805|    {}|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "|insert|2021-01-01 23:59:59|       0|559249709426073600|5627249|     ADAPANDI|    6378687|   11874208|         BCA|  bca-10002|2021-05-31 14:45:...|    2021053110992073| 700000.00| CENAIDJA|         5485939609|               DDL|CENAIDJA|       5385857788|         ESCROW|       IN|          IDR|      FUNDING|        BCA|SUCCESS|   null|REMARK1:0006378687|REMARK2:0005627249|    {}|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "|insert|2021-01-01 23:59:59|       0|559249713057558528|5512338|     ADAPANDI|    6378691|   11926758|         BCA|  bca-10001|2021-05-31 14:45:...|    2021053110992074| 500000.00| CENAIDJA|         5385857788|              null|CENAIDJA|       1310648180|           null|      OUT|          IDR| TRANSFER_OUT|        BCA|SUCCESS|   null|REMARK1:0006378691|REMARK2:0005512338|    {}|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "|insert|2021-01-01 23:59:59|       0|559249726949875712|5605456|     ADAPANDI|    6378693|   11928009|         BCA|  bca-10001|2021-05-31 14:45:...|    2021053110992075| 400000.00| CENAIDJA|         5385857788|              null|CENAIDJA|       5085053890|           null|      OUT|          IDR| TRANSFER_OUT|        BCA|SUCCESS|   null|REMARK1:0006378693|REMARK2:0005605456|    {}|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "|insert|2021-01-01 23:59:59|       0|559249734155689984|1361550|     ADAPANDI|       null|       null|         BCA|  bca-10002| 2021-05-31 07:45:18|20210531144517122...|3558400.00|     null|   1224500001361550|    Fariz Nur Aziz|    null|             null|          12245|       IN|          IDR|    REPAYMENT|MOBILE_BANK|SUCCESS|   null|              null|              null|  null|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "|insert|2021-01-01 23:59:59|       0|559249754674225152|4870551|     ADAPANDI|    6378698|   11928108|         BCA|  bca-10001|2021-05-31 14:45:...|    2021053110992076|3100000.00| CENAIDJA|         5385857788|              null|CENAIDJA|       2711399862|           null|      OUT|          IDR| TRANSFER_OUT|        BCA|SUCCESS|   null|REMARK1:0006378698|REMARK2:0004870551|    {}|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "|insert|2021-01-01 23:59:59|       0|559249765889794048| 713639|     ADAPANDI|    6378699|   11928112|         BCA|  bca-10001|2021-05-31 14:45:...|    2021053110992077|3010000.00| CENAIDJA|         5385857788|              null|CENAIDJA|       7310644433|           null|      OUT|          IDR| TRANSFER_OUT|        BCA|SUCCESS|   null|REMARK1:0006378699|REMARK2:0000713639|    {}|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "|insert|2021-01-01 23:59:59|       0|559249773338095616|5584274|     ADAPANDI|    6378700|   11928119|         BCA|  bca-10001|2021-05-31 14:45:...|    2021053110992078| 420000.00| CENAIDJA|         5385857788|              null|CENAIDJA|       2310199764|           null|      OUT|          IDR| TRANSFER_OUT|        BCA|SUCCESS|   null|REMARK1:0006378700|REMARK2:0005584274|    {}|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "|insert|2021-01-01 23:59:59|       0|559249787473682432|1095845|     ADAPANDI|    6378702|   11928329|         BCA|  bca-10001|2021-05-31 14:45:...|    2021053110992079|3000000.00| CENAIDJA|         5385857788|              null|CENAIDJA|       1011263564|           null|      OUT|          IDR| TRANSFER_OUT|        BCA|SUCCESS|   null|REMARK1:0006378702|REMARK2:0001095845|    {}|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "|insert|2021-01-01 23:59:59|       0|559249787993776128|5001090|     ADAPANDI|       null|       null|         BCA|  bca-10002| 2021-05-31 07:45:36|20210531144535122...|2182400.00|     null|   1224500005001090|Dandy Ari Ramadhan|    null|             null|          12245|       IN|          IDR|    REPAYMENT|MOBILE_BANK|SUCCESS|   null|              null|              null|  null|2021-05-31 07:45:...|2021-05-31 07:45:...|2021-05-31 07:45:...|2021|   01| 01|\n",
      "+------+-------------------+--------+------------------+-------+-------------+-----------+-----------+------------+-----------+--------------------+--------------------+----------+---------+-------------------+------------------+--------+-----------------+---------------+---------+-------------+-------------+-----------+-------+-------+------------------+------------------+------+--------------------+--------------------+--------------------+----+-----+---+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "select  * from protoss_stream_etl.t_pay_order\n",
    "\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
