{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars': 's3://rupiahplus-configs/hudi_jar/hudi-spark-bundle.jar,s3://rupiahplus-configs/hudi_jar/spark-avro.jar', 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer', 'spark.sql.hive.convertMetastoreParquet': 'false'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1606988695033_0001</td><td>pyspark</td><td>dead</td><td><a target=\"_blank\" href=\"http://ip-172-31-1-41.ap-southeast-1.compute.internal:20888/proxy/application_1606988695033_0001/\">Link</a></td><td></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{ \"conf\": {\n",
    "            \"spark.jars\":\"s3://rupiahplus-configs/hudi_jar/hudi-spark-bundle.jar,s3://rupiahplus-configs/hudi_jar/spark-avro.jar\",\n",
    "            \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "            \"spark.sql.hive.convertMetastoreParquet\":\"false\"\n",
    "          }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0b80cac8cb43139c38a0cc4d8255db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, array, ArrayType, DateType, DecimalType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"Product_Price_Tracking\") \\\n",
    "     .config(\"spark.jars\",\"s3://rupiahplus-configs/hudi_jar/hudi-spark-bundle.jar,s3://rupiahplus-configs/hudi_jar/spark-avro.jar\") \\\n",
    "     .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "     .config(\"spark.sql.hive.convertMetastoreParquet\", \"false\") \\\n",
    "     .getOrCreate()\n",
    "# import org.apache.spark.sql.SaveMode\n",
    "# import org.apache.spark.sql.functions._\n",
    "# import org.apache.hudi.DataSourceWriteOptions\n",
    "# import org.apache.hudi.config.HoodieWriteConfig\n",
    "# import org.apache.hudi.hive.MultiPartKeysValueExtractor\n",
    "df=spark.read.format(\"orc\").load('s3://rupiahplus-data-warehouse/stream/banda_etl/t_admin_audit/')\n",
    "# df.write.format(\"org.apache.hudi\") \\\n",
    "#             .option(\"hoodie.table.name\", \"t_admin_audit1\") \\\n",
    "#             .option(\"hoodie.datasource.write.storage.type\", \"COPY_ON_WRITE\") \\\n",
    "#             .option(\"hoodie.datasource.write.operation\", \"bulk_insert\") \\\n",
    "#             .option(\"hoodie.datasource.write.recordkey.field\",\"id\") \\\n",
    "#             .option(\"hoodie.datasource.write.precombine.field\", \"id\") \\\n",
    "#             .mode(\"overwrite\") \\\n",
    "#             .save(\"s3://rupiahplus-data-warehouse/aliyun/banda_real/t_admin_audit1/\")\n",
    "# inputDF = spark.createDataFrame(\n",
    "#     [\n",
    "#         (\"100\", \"2015-01-01\", \"2015-01-01T13:51:39.340396Z\"),\n",
    "#         (\"101\", \"2015-01-01\", \"2015-01-01T12:14:58.597216Z\"),\n",
    "#         (\"102\", \"2015-01-01\", \"2015-01-01T13:51:40.417052Z\"),\n",
    "#         (\"103\", \"2015-01-01\", \"2015-01-01T13:51:40.519832Z\"),\n",
    "#         (\"104\", \"2015-01-02\", \"2015-01-01T12:15:00.512679Z\"),\n",
    "#         (\"105\", \"2015-01-02\", \"2015-01-01T13:51:42.248818Z\"),\n",
    "#     ],\n",
    "#     [\"id\", \"creation_date\", \"last_update_time\"]\n",
    "# )\n",
    "# inputDF.show()\n",
    "# # Specify common DataSourceWriteOptions in the single hudiOptions variable\n",
    "hudiOptions = {\n",
    "'hoodie.table.name': 'my_hudi_table',\n",
    "'hoodie.datasource.write.recordkey.field': 'id',\n",
    "    #记录键字段。用作HoodieKey中recordKey部分的值。 实际值将通过在字段值上调用.toString()来获得。可以使用点符号指定嵌套字段，例如：a.b.c\n",
    "'hoodie.datasource.write.partitionpath.field': 'year',\n",
    "    #分区路径字段。用作HoodieKey中partitionPath部分的值。 通过调用.toString()获得实际的值\n",
    "'hoodie.datasource.write.precombine.field': 'create_time',\n",
    "'hoodie.datasource.hive_sync.enable': 'true',\n",
    "'hoodie.datasource.hive_sync.table': 'my_hudi_table',\n",
    "'hoodie.datasource.hive_sync.partition_fields': 'etldate',\n",
    "'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor'\n",
    "}\n",
    "# option('hoodie.datasource.write.operation', 'insert')\\\n",
    "#是否为写操作进行插入更新、插入或批量插入。使用bulkinsert将新数据加载到表中，之后使用upsert或insert。 \n",
    "#批量插入使用基于磁盘的写入路径来扩展以加载大量输入，而无需对其进行缓存。\n",
    "#如果是大量数据使用bulkinsert\n",
    "# # Write a DataFrame as a Hudi dataset\n",
    "df.write\\\n",
    ".format('org.apache.hudi')\\\n",
    ".option('hoodie.datasource.write.operation', 'insert')\\\n",
    ".options(**hudiOptions)\\\n",
    ".mode('overwrite')\\\n",
    ".save('s3://rupiahplus-data-warehouse/aliyun/banda_real_hudi/t_admin_audit1/')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
