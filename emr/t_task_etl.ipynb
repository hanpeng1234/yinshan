{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08959f08d5c4656bec237be7bff1450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842c2ae09c1a44a5af5b414e1e0ee8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '400' from https://172.31.7.15:18888/sessions/0/statements/0 with error payload: {\"msg\":\"requirement failed: Session isn't active.\"}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Python Demo\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "\n",
    "url = \"jdbc:postgresql://lovina-prod.ct3lmpqmpvcq.ap-southeast-1.rds.amazonaws.com:5432/lovina\"\n",
    "t_task = spark\\\n",
    "  .read\\\n",
    "  .format(\"jdbc\")\\\n",
    "  .option(\"url\", url)\\\n",
    "  .option(\"dbtable\", \"t_task\")\\\n",
    "  .option(\"user\", \"lovina\")\\\n",
    "  .option(\"password\", \"UFXXExqnXJix9b2eM9Ge\")\\\n",
    "  .option(\"partitionColumn\", \"id\")\\\n",
    "  .option(\"lowerBound\", 100000)\\\n",
    "  .option(\"upperBound\", 41840364)\\\n",
    "  .option(\"numPartitions\", 1000)\\\n",
    "  .load()\n",
    "t_task.createOrReplaceTempView(\"t\")\n",
    "bound = spark.sql(\"select max(id) max, min(id) min from t\")\n",
    "max = bound.collect()[0].max\n",
    "min = bound.collect()[0].min\n",
    "t_task = spark\\\n",
    "  .read\\\n",
    "  .format(\"jdbc\")\\\n",
    "  .option(\"url\", url)\\\n",
    "  .option(\"dbtable\", \"t_task\")\\\n",
    "  .option(\"user\", \"lovina\")\\\n",
    "  .option(\"password\", \"UFXXExqnXJix9b2eM9Ge\")\\\n",
    "  .option(\"partitionColumn\", \"id\")\\\n",
    "  .option(\"lowerBound\", min)\\\n",
    "  .option(\"upperBound\", max)\\\n",
    "  .option(\"numPartitions\", 1000)\\\n",
    "  .load()\n",
    "t_task.write.mode(\"overwrite\").orc(\"s3://rupiahplus-data-warehouse/aliyun/lovina/t_task_1\")\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8b3852ee484bfc93bc6804093b05ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1626419356965_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-3-0-119.ap-southeast-1.compute.internal:20888/proxy/application_1626419356965_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-3-0-16.ap-southeast-1.compute.internal:8042/node/containerlogs/container_1626419356965_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select  * from t_task where id >=80000000 and id <90000000\n",
      "select  * from t_task where id >=90000000 and id <100000000\n",
      "select  * from t_task where id >=100000000 and id <110000000\n",
      "select  * from t_task where id >=110000000 and id <120000000\n",
      "select  * from t_task where id >=120000000 and id <130000000"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Python Demo\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "\n",
    "url = \"jdbc:postgresql://lovina-prod.ct3lmpqmpvcq.ap-southeast-1.rds.amazonaws.com:5432/lovina\"\n",
    "partitions=[\n",
    "#     (1,10000000),\n",
    "#     (10000000,20000000),\n",
    "#     (20000000,30000000),\n",
    "#     (30000000,40000000),\n",
    "#     (40000000,50000000),\n",
    "#     (50000000,60000000),\n",
    "#     (60000000,70000000),\n",
    "#     (70000000,80000000),\n",
    "    (80000000,90000000),\n",
    "    (90000000,100000000),\n",
    "    (100000000,110000000),\n",
    "    (110000000,120000000),\n",
    "    (120000000,130000000),\n",
    "]\n",
    "for i in partitions:\n",
    "    sql=\"select  * from t_task where id >={} and id <{}\".format(i[0],i[1])\n",
    "    print(sql)\n",
    "    t_task = spark\\\n",
    "      .read\\\n",
    "      .format(\"jdbc\")\\\n",
    "      .option(\"url\", url)\\\n",
    "      .option(\"user\", \"lovina\")\\\n",
    "      .option(\"password\", \"UFXXExqnXJix9b2eM9Ge\")\\\n",
    "      .option(\"fetchsize\",10000)\\\n",
    "      .option(\"numPartitions\",1000)\\\n",
    "      .option(\"query\",sql)\\\n",
    "      .load()\n",
    "    t_task.write.mode(\"append\").orc(\"s3://rupiahplus-data-warehouse/aliyun/lovina/t_task_2\")\n",
    "# t_task = spark\\\n",
    "#   .read\\\n",
    "#   .format(\"jdbc\")\\\n",
    "#   .option(\"url\", url)\\\n",
    "#   .option(\"user\", \"lovina\")\\\n",
    "#   .option(\"password\", \"UFXXExqnXJix9b2eM9Ge\")\\\n",
    "#   .option(\"fetchsize\",10000)\\\n",
    "#   .option(\"numPartitions\",1000)\\\n",
    "#   .option(\"query\",\"select  * from t_task where id >=50000000 and id <60000000\")\\\n",
    "#   .load()\n",
    "# t_task.write.mode(\"append\").orc(\"s3://rupiahplus-data-warehouse/aliyun/lovina/t_task_2\")\n",
    "# t_task.createOrReplaceTempView(\"t\")\n",
    "# spark.sql(\"\"\"select  * from  t\"\"\").show()\n",
    "#   .option(\"partitionColumn\", \"id\")\\\n",
    "#   .option(\"lowerBound\", 100000)\\\n",
    "#   .option(\"upperBound\", 41840364)\\\n",
    "#   .option(\"numPartitions\", 1000)\\\n",
    "#   .load()\n",
    "# t_task.createOrReplaceTempView(\"t\")\n",
    "# bound = spark.sql(\"select max(id) max, min(id) min from t\")\n",
    "# max = bound.collect()[0].max\n",
    "# min = bound.collect()[0].min\n",
    "# t_task = spark\\\n",
    "#   .read\\\n",
    "#   .format(\"jdbc\")\\\n",
    "#   .option(\"url\", url)\\\n",
    "#   .option(\"dbtable\", \"t_task\")\\\n",
    "#   .option(\"user\", \"lovina\")\\\n",
    "#   .option(\"password\", \"UFXXExqnXJix9b2eM9Ge\")\\\n",
    "#   .option(\"partitionColumn\", \"id\")\\\n",
    "#   .option(\"lowerBound\", min)\\\n",
    "#   .option(\"upperBound\", max)\\\n",
    "#   .option(\"numPartitions\", 1000)\\\n",
    "#   .load()\n",
    "# t_task.write.mode(\"overwrite\").orc(\"s3://rupiahplus-data-warehouse/aliyun/lovina/t_task_1\")\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
