{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306436f8d9ac4eb8800637203326bf59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "# 合并小文件\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "# print(datetime.now(pytz.timezone(\"Asia/Shanghai\")).strftime( '%Y-%m-%d'))\n",
    "today=(datetime.now()+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "yesterday=(datetime.now(pytz.timezone(\"Asia/Shanghai\"))+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "tablemap={\"luzon_stream_etl\":\"luzon\"} #,\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.shuffle.targetPostShuffleInputSize\", \"128000000\")\n",
    "if __name__ == \"builtins\":\n",
    "    databases=spark.sql(\"show databases\")\n",
    "#     print(databases.show())\n",
    "    databases=databases.collect()\n",
    "    for row in databases:\n",
    "        databaseName=row[\"databaseName\"]\n",
    "#         print(databaseName in tablemap)\n",
    "        if databaseName != \"\" and  databaseName in tablemap :\n",
    "            databasesql=\"show tables in \"+databaseName\n",
    "#             print(databasesql)\n",
    "            tables=spark.sql(databasesql)\n",
    "            tablelist=tables.collect();\n",
    "            for row in tablelist:\n",
    "                tableName=row[\"tableName\"]\n",
    "                if tableName!='tmp':\n",
    "                    spark.catalog.refreshTable(databaseName+\".\"+tableName)\n",
    "                    sql1=\"SELECt * FROM \"+databaseName+\".\"+tableName+\" where  cast(concat(year,'-',month,'-',day) AS date) < cast(\"+'\"2020-06-10 00:00:00\"'+\"  AS date) and cast(concat(year,'-',month,'-',day) AS date) > cast(\"+'\"2019-12-30 00:00:00\"'+\"  AS date)\"\n",
    "#                   luzon数据少，分成1个文件\n",
    "                    df=spark.sql(sql1).repartition(1).createOrReplaceTempView(\"tmp\");\n",
    "                    path=\"s3://rupiahplus-data-warehouse/stream/luzon_etl/\"+tableName;\n",
    "                    spark.sql(\"\"\" SELECT * FROM tmp \"\"\").write.mode(\"overwrite\").partitionBy(\"year\",\"month\",\"day\").orc(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合并banda小文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5c3f79099640a7914b5af9fe779bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1593243875741_0003</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd5c25c11284e89956b04913f5f2a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b85c55052d6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'# from __future__ import print_function\\n\\nimport sys\\n\\nimport numpy as np\\nfrom pyspark.sql import SparkSession\\nfrom datetime import datetime,timedelta\\nimport pytz\\n# print(datetime.now(pytz.timezone(\"Asia/Shanghai\")).strftime( \\'%Y-%m-%d\\'))\\ntoday=(datetime.now()+ timedelta(-1)).strftime( \\'%Y-%m-%d\\')\\nyesterday=(datetime.now(pytz.timezone(\"Asia/Shanghai\"))+ timedelta(-1)).strftime( \\'%Y-%m-%d\\')\\ntablemap={\"banda_stream_etl\":\"banda\"} #,\\nspark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\\nspark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")\\nspark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")\\nspark.conf.set(\"spark.sql.adaptive.shuffle.targetPostShuffleInputSize\", \"128000000\")\\nif __name__ == \"builtins\":\\n    databases=spark.sql(\"show databases\")\\n#     print(databases.show())\\n    databases=databases.collect()\\n    for row in databases:\\n        databaseName=row[\"databaseName\"]\\n#         print(databaseName in tablemap)\\n        if databaseName != \"\" and  databaseName in tablemap :\\n            databasesql=\"show tables in \"+databaseName\\n#             print(databasesql)\\n            tables=spark.sql(databasesql)\\n            tablelist=tables.collect();\\n            for row in tablelist:\\n                tableName=row[\"tableName\"]\\n                if tableName!=\\'tmp\\':\\n                    spark.catalog.refreshTable(databaseName+\".\"+tableName)\\n#                     sql1=\"SELECt * FROM \"+databaseName+\".\"+tableName+\" where  cast(concat(year,\\'-\\',month,\\'-\\',day) AS date) < cast(\"+\\'\"2020-06-09 00:00:00\"\\'+\"  AS date) and  cast(concat(year,\\'-\\',month,\\'-\\',day) AS date) > cast(\"+\\'\"2020-05-01 00:00:00\"\\'+\"  AS date) \"\\n                    sql1=\"SELECt * FROM \"+databaseName+\".\"+tableName+\" where  cast(concat(year,\\'-\\',month,\\'-\\',day) AS date) < cast(\"+\\'\"2020-06-09 00:00:00\"\\'+\"  AS date)  \"\\n                    df=spark.sql(sql1).repartition(1).createOrReplaceTempView(\"tmp\");\\n                    path=\"s3://rupiahplus-data-warehouse/stream/banda_etl/\"+tableName;\\n                    spark.sql(\"\"\" SELECT * FROM tmp \"\"\").write.mode(\"overwrite\").partitionBy(\"year\",\"month\",\"day\").orc(path)\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2360\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-124>\u001b[0m in \u001b[0;36mspark\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sparkmagic/livyclientlib/exceptions.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu\"ENCOUNTERED AN INTERNAL ERROR: {}\\n\\tTraceback:\\n{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sparkmagic/livyclientlib/exceptions.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions_to_handle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Do not log! as some messages may contain private client information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sparkmagic/kernels/kernelmagics.py\u001b[0m in \u001b[0;36mspark\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mcoerce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_coerce_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplemethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplefraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sparkmagic/magics/sparkmagicsbase.py\u001b[0m in \u001b[0;36mexecute_spark\u001b[0;34m(self, cell, output_var, samplemethod, maxrows, samplefraction, session_name, coerce)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplemethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplefraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_controller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_display\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sparkmagic/livyclientlib/sparkcontroller.py\u001b[0m in \u001b[0;36mrun_command\u001b[0;34m(self, command, client_name)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0msession_to_use\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session_by_name_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_to_use\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_sqlquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqlquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sparkmagic/livyclientlib/command.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mstatement_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mu'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_statement_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             self._spark_events.emit_statement_execution_end_event(session.guid, session.kind, session.id,\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sparkmagic/livyclientlib/command.py\u001b[0m in \u001b[0;36m_get_statement_output\u001b[0;34m(self, session, statement_id)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mFINAL_STATEMENT_STATUS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'progress'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0mretries\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sparkmagic/livyclientlib/livysession.py\u001b[0m in \u001b[0;36msleep\u001b[0;34m(self, retries)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseconds_to_sleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;31m# This function will refresh the status and get the logs in a single call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "# print(datetime.now(pytz.timezone(\"Asia/Shanghai\")).strftime( '%Y-%m-%d'))\n",
    "today=(datetime.now()+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "yesterday=(datetime.now(pytz.timezone(\"Asia/Shanghai\"))+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "tablemap={\"banda_stream_etl\":\"banda\"} #,\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.shuffle.targetPostShuffleInputSize\", \"128000000\")\n",
    "if __name__ == \"builtins\":\n",
    "    databases=spark.sql(\"show databases\")\n",
    "#     print(databases.show())\n",
    "    databases=databases.collect()\n",
    "    for row in databases:\n",
    "        databaseName=row[\"databaseName\"]\n",
    "#         print(databaseName in tablemap)\n",
    "        if databaseName != \"\" and  databaseName in tablemap :\n",
    "            databasesql=\"show tables in \"+databaseName\n",
    "#             print(databasesql)\n",
    "            tables=spark.sql(databasesql)\n",
    "            tablelist=tables.collect();\n",
    "            for row in tablelist:\n",
    "                tableName=row[\"tableName\"]\n",
    "                if tableName!='tmp':\n",
    "                    spark.catalog.refreshTable(databaseName+\".\"+tableName)\n",
    "#                     sql1=\"SELECt * FROM \"+databaseName+\".\"+tableName+\" where  cast(concat(year,'-',month,'-',day) AS date) < cast(\"+'\"2020-06-09 00:00:00\"'+\"  AS date) and  cast(concat(year,'-',month,'-',day) AS date) > cast(\"+'\"2020-05-01 00:00:00\"'+\"  AS date) \"\n",
    "                    sql1=\"SELECt * FROM \"+databaseName+\".\"+tableName+\" where  cast(concat(year,'-',month,'-',day) AS date) < cast(\"+'\"2020-06-09 00:00:00\"'+\"  AS date)  \"\n",
    "                    df=spark.sql(sql1).repartition(1).createOrReplaceTempView(\"tmp\");\n",
    "                    path=\"s3://rupiahplus-data-warehouse/stream/banda_etl/\"+tableName;\n",
    "                    spark.sql(\"\"\" SELECT * FROM tmp \"\"\").write.mode(\"overwrite\").partitionBy(\"year\",\"month\",\"day\").orc(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试合并小文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297d7e07a36846c6b3b94bad9af9e3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# spark.conf.set(\"hive.exec.dynamic.partition\",\"true\");\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "# spark.conf.set(\"hive.default.fileformat\", \"Orc\")\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")\n",
    "# spark.conf.set(\"spark.sql.adaptive.shuffle.targetPostShuffleInputSize\", \"128000000\")\n",
    "# spark.conf.set(\"mapreduce.input.fileinputformat.input.dir.recursive\",\"false\")\n",
    "spark.catalog.refreshTable(\"`han_test`.`demo`\")\n",
    "df=spark.sql(\"\"\"\n",
    "SELECt\n",
    "*\n",
    "FROM `han_test`.`demo`\n",
    "where  cast(concat(year,\"-\",month,\"-\",day) AS date) < cast('2020-03-31 00:00:00'  AS date) \n",
    "\"\"\").repartition(10).createOrReplaceTempView(\"tmp\");\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    " *\n",
    "FROM tmp\n",
    "\"\"\").write.mode(\"overwrite\").partitionBy(\"year\",\"month\",\"day\").orc(\"s3://rupiahplus-data-warehouse/stream/demo/\")\n",
    "# WHERE cast(concat(year,\"-\",month,\"-\",day) AS date) < cast('2020-03-26 00:00:00'  AS date) \n",
    "#  where cast(etldate AS timestamp) >= cast('2020-03-02 00:00:00'  AS timestamp) \n",
    "# df.write.mode(\"overwrite\").partitionBy(\"year\",\"month\",\"day\").orc(\"s3://rupiahplus-data-warehouse/stream/demo/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合并coupon小文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给daily_etl添加2020年的partitions\n",
    "banda\n",
    "luzon\n",
    "coupon\n",
    "notification\n",
    "protoss\n",
    "credinex_risk\n",
    "arkham\n",
    "credinex_account\n",
    "credinex_hive\n",
    "telemarket\n",
    "lovina\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402b9def19fa41a58ff9ddb91d28d853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7</td><td>application_1625629849770_0009</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-3-0-77.ap-southeast-1.compute.internal:20888/proxy/application_1625629849770_0009/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-3-0-86.ap-southeast-1.compute.internal:8042/node/containerlogs/container_1625629849770_0009_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_brick\n",
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "from datetime import datetime,timedelta\n",
    "start=datetime(2020,12,30)\n",
    "end=datetime(2022,1,1)\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "import pyspark.sql.functions as F\n",
    "# \",\n",
    "# tablemap={\"arkham_stream_etl\":\"arkham\",\"credinex_account_stream_etl\":\"credinex_account\",\n",
    "#           \"credinex_hive_stream_etl\":\"credinex_hive\",\"credinex_repeater_stream_etl\":\"credinex_repeater\",\"protoss_stream_etl\":\"protoss\"} #,\n",
    "tablemap={\n",
    "    \"banda_stream_etl\":\"banda\",\n",
    "#     \"lovina_stream_etl\":\"lovina\",\n",
    "#     \"coupon_stream_etl\":\"coupon\",\n",
    "#     \"notification_stream_etl\":\"notification\",\n",
    "#     \"telemarket_stream_etl\":\"telemarket\",\n",
    "#     \"arkham_stream_etl\":\"arkham\",\n",
    "#     \"credinex_account_stream_etl\":\"credinex_account\",\n",
    "#     \"credinex_hive_stream_etl\":\"credinex_hive\",\n",
    "#     \"credinex_repeater_stream_etl\":\"credinex_repeater\",\n",
    "#     \"protoss_stream_etl\":\"protoss\"\n",
    "} #,\n",
    "databases=spark.sql(\"show databases\")\n",
    "dateList=[]\n",
    "def gen_dates(b_date, days):\n",
    "    day = timedelta(days=1)\n",
    "    for i in range(days):\n",
    "        yield b_date + day*i\n",
    "def getDateList(start=None, end=None):\n",
    "    \"\"\"\n",
    "    获取日期列表\n",
    "    :param start: 开始日期\n",
    "    :param end: 结束日期\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if start is None:\n",
    "        start = datetime.strptime(\"2000-01-01\", \"%Y-%m-%d\")\n",
    "    if end is None:\n",
    "        end = datetime.now()\n",
    "    data = []\n",
    "#     partitionsql = \"ALTER TABLE `\" + database+ \"`.\" + tableNm+ \"  ADD IF NOT EXISTS \"\n",
    "    for d in gen_dates(start, (end-start).days):\n",
    "        data.append(d.strftime('%Y-%m-%d'))\n",
    "#         datelist=d.strftime('%Y-%m-%d').split(\"-\")\n",
    "#         partition=\" PARTITION (year=\" + datelist[0] + \",month=\" + datelist[1] + \",day=\" + datelist[2] + \") \"\n",
    "#         partitionsql=partitionsql+partition\n",
    "#         spark.sql(partitionsql)\n",
    "    return data\n",
    "dateList=getDateList(start,end)\n",
    "databases=databases.collect()\n",
    "def getpartitionSql(database,tableNm):\n",
    "    partitionsql = \"ALTER TABLE `\" + database+ \"`.\" + tableNm+ \"  ADD IF NOT EXISTS \"\n",
    "    for date in dateList:\n",
    "        datels=date.split(\"-\")\n",
    "        partition=\" PARTITION (year=\" + datels[0] + \",month=\" + datels[1] + \",day=\" + datels[2] + \") \"\n",
    "        partitionsql=partitionsql+partition\n",
    "    return partitionsql\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Demo\") \\\n",
    "    .config(\"hive.metastore.client.factory.class\",\n",
    "            \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"hive.exec.dynamic.partition\",\"true\");\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "for databaseName in tablemap:\n",
    "    databasesql=\"show tables in \"+databaseName\n",
    "    tables=spark.sql(databasesql)\n",
    "    tablelist=tables.collect();\n",
    "    for row in tablelist:\n",
    "        tableName=row[\"tableName\"]\n",
    "        if tableName=='t_password_update_record':\n",
    "            print(tableName)\n",
    "    #             tableName=row[\"tableName\"]\n",
    "            sql=getpartitionSql(databaseName,tableName)\n",
    "    #         print(sql)\n",
    "            spark.sql(sql)\n",
    "    #             setPartitions(start,end,\"partitions_etl\",tableName)\n",
    "    #                 spark.catalog.refreshTable(databaseName+\".\"+tableName)\n",
    "    #                 sql1=\"SELECT * FROM \"+databaseName+\".\"+tableName\n",
    "    #                 path=\"s3://rupiahplus-data-warehouse/stream/demo/\"+tableName;\n",
    "    #                 spark.sql(sql1).write.mode(\"overwrite\").partitionBy(\"year\",\"month\",\"day\").orc(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "删除partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c9e917fa3042438231d04d62cb98f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96065e3a09c6447580b292fad96ad1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '400' from https://10.3.0.74:18888/sessions/2/statements/10 with error payload: {\"msg\":\"requirement failed: Session isn't active.\"}\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime,timedelta\n",
    "start=datetime(2017,1,1)\n",
    "end=datetime(2020,1,1)\n",
    "# \",\n",
    "# tablemap={\"arkham_stream_etl\":\"arkham\",\"credinex_account_stream_etl\":\"credinex_account\",\n",
    "#           \"credinex_hive_stream_etl\":\"credinex_hive\",\"credinex_repeater_stream_etl\":\"credinex_repeater\",\"protoss_stream_etl\":\"protoss\"} #,\n",
    "tablemap={\n",
    "    \"banda_stream_etl\":\"banda\",\n",
    "#     \"lovina_stream_etl\":\"lovina\",\n",
    "#     \"coupon_stream_etl\":\"coupon\",\n",
    "#     \"notification_stream_etl\":\"notification\",\n",
    "#     \"telemarket_stream_etl\":\"telemarket\",\n",
    "#     \"arkham_stream_etl\":\"arkham\",\n",
    "#     \"credinex_account_stream_etl\":\"credinex_account\",\n",
    "#     \"credinex_hive_stream_etl\":\"credinex_hive\",\n",
    "#     \"credinex_repeater_stream_etl\":\"credinex_repeater\",\n",
    "#     \"protoss_stream_etl\":\"protoss\"\n",
    "} #,\n",
    "databases=spark.sql(\"show databases\")\n",
    "dateList=[]\n",
    "def gen_dates(b_date, days):\n",
    "    day = timedelta(days=1)\n",
    "    for i in range(days):\n",
    "        yield b_date + day*i\n",
    "def getDateList(start=None, end=None):\n",
    "    \"\"\"\n",
    "    获取日期列表\n",
    "    :param start: 开始日期\n",
    "    :param end: 结束日期\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if start is None:\n",
    "        start = datetime.strptime(\"2000-01-01\", \"%Y-%m-%d\")\n",
    "    if end is None:\n",
    "        end = datetime.now()\n",
    "    data = []\n",
    "#     partitionsql = \"ALTER TABLE `\" + database+ \"`.\" + tableNm+ \"  ADD IF NOT EXISTS \"\n",
    "    for d in gen_dates(start, (end-start).days):\n",
    "        data.append(d.strftime('%Y-%m-%d'))\n",
    "#         datelist=d.strftime('%Y-%m-%d').split(\"-\")\n",
    "#         partition=\" PARTITION (year=\" + datelist[0] + \",month=\" + datelist[1] + \",day=\" + datelist[2] + \") \"\n",
    "#         partitionsql=partitionsql+partition\n",
    "#         spark.sql(partitionsql)\n",
    "    return data\n",
    "dateList=getDateList(start,end)\n",
    "databases=databases.collect()\n",
    "def getpartitionSql(database,tableNm):\n",
    "    partitionsql = \"ALTER TABLE `\" + database+ \"`.\" + tableNm+ \"  DROP IF EXISTS \"\n",
    "    for date in dateList:\n",
    "        datels=date.split(\"-\")\n",
    "        partition=\" PARTITION (year=\" + datels[0] + \",month=\" + datels[1] + \",day=\" + datels[2] + \") ,\"\n",
    "        partitionsql=partitionsql+partition\n",
    "    return partitionsql[0:len(partitionsql)-1]\n",
    "spark.conf.set(\"hive.exec.dynamic.partition\",\"true\"); \n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "for databaseName in tablemap:\n",
    "    databasesql=\"show tables in \"+databaseName\n",
    "    tables=spark.sql(databasesql)\n",
    "    tablelist=tables.collect();\n",
    "    for row in tablelist:\n",
    "        tableName=row[\"tableName\"]\n",
    "        print(tableName)\n",
    "#             tableName=row[\"tableName\"]\n",
    "        sql=getpartitionSql(databaseName,tableName)\n",
    "#         print(sql)\n",
    "        spark.sql(sql)\n",
    "#             setPartitions(start,end,\"partitions_etl\",tableName)\n",
    "#                 spark.catalog.refreshTable(databaseName+\".\"+tableName)\n",
    "#                 sql1=\"SELECT * FROM \"+databaseName+\".\"+tableName\n",
    "#                 path=\"s3://rupiahplus-data-warehouse/stream/demo/\"+tableName;\n",
    "#                 spark.sql(sql1).write.mode(\"overwrite\").partitionBy(\"year\",\"month\",\"day\").orc(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b764a95ea66040288ecbe933a9708d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tzinfo argument must be None or of a tzinfo subclass, not type 'int'\n",
      "Traceback (most recent call last):\n",
      "TypeError: tzinfo argument must be None or of a tzinfo subclass, not type 'int'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime,timedelta\n",
    "\n",
    "start=datetime.now().strftime('%Y-%m-%d')\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将banda_etl复制到demo库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc41dc6930947b6a7dbbc877f0e9770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1589957054770_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-9-153.ap-southeast-1.compute.internal:20888/proxy/application_1589957054770_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-10-59.ap-southeast-1.compute.internal:8042/node/containerlogs/container_1589957054770_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "# print(datetime.now(pytz.timezone(\"Asia/Shanghai\")).strftime( '%Y-%m-%d'))\n",
    "today=(datetime.now()+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "yesterday=(datetime.now(pytz.timezone(\"Asia/Shanghai\"))+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "tablemap={\"banda_stream_etl\":\"banda\"} #,\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.shuffle.targetPostShuffleInputSize\", \"128000000\")\n",
    "if __name__ == \"builtins\":\n",
    "    databases=spark.sql(\"show databases\")\n",
    "#     print(databases.show())\n",
    "    databases=databases.collect()\n",
    "    for row in databases:\n",
    "        databaseName=row[\"databaseName\"]\n",
    "#         print(databaseName in tablemap)\n",
    "        if databaseName != \"\" and  databaseName in tablemap :\n",
    "            databasesql=\"show tables in \"+databaseName\n",
    "#             print(databasesql)\n",
    "            tables=spark.sql(databasesql)\n",
    "            tablelist=tables.collect();\n",
    "            for row in tablelist:\n",
    "                tableName=row[\"tableName\"]\n",
    "                spark.catalog.refreshTable(databaseName+\".\"+tableName)\n",
    "                sql1=\"SELECT * FROM \"+databaseName+\".\"+tableName\n",
    "                path=\"s3://rupiahplus-data-warehouse/stream/demo/\"+tableName;\n",
    "                spark.sql(sql1).write.mode(\"overwrite\").partitionBy(\"year\",\"month\",\"day\").orc(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
