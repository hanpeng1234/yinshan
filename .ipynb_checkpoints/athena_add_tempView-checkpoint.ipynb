{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2e7739335443beb0a85cebe53d74f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "import copy\n",
    "dbmap={\n",
    "#     \"banda\":{\n",
    "#         \"increment_database\":\"banda_stream_etl\",\n",
    "#         \"database\":\"banda-etl-s3\",\n",
    "#         \"realTime_path\":\"banda\"\n",
    "#     },\n",
    "#     \"luzon\":{\n",
    "#          \"increment_database\":\"luzon_stream_etl\",\n",
    "#          \"database\":\"luzon_etl_s3\",\n",
    "#          \"realTime_path\":\"luzon\"\n",
    "#     },\n",
    "#     \"telemarket\":{\n",
    "#          \"increment_database\":\"telemarket_stream_etl\",\n",
    "#          \"database\":\"telemarket_etl_s3\",\n",
    "#          \"realTime_path\":\"telemarket\"\n",
    "#     },\n",
    "#      \"coupon\":{\n",
    "#         \"increment_database\":\"coupon_stream_etl\",\n",
    "#         \"database\":\"coupon_etl_s3\",\n",
    "#         \"realTime_path\":\"coupon\"\n",
    "#     },\n",
    "#     \"notification\":{\n",
    "#         \"increment_database\":\"notification_stream_etl\",\n",
    "#         \"database\":\"notification_etl_s3\",\n",
    "#         \"realTime_path\":\"notification\"\n",
    "#     },\n",
    "#     \"arkham\":{\n",
    "#         \"increment_database\":\"arkham_stream_etl\",\n",
    "#         \"database\":\"arkham_etl_s3\",\n",
    "#         \"realTime_path\":\"arkham\"\n",
    "#     },\n",
    "#     \"protoss\":{\n",
    "#         \"increment_database\":\"protoss_stream_etl\",\n",
    "#         \"database\":\"protoss_etl_s3\",\n",
    "#         \"realTime_path\":\"protoss\"\n",
    "#     },\n",
    "#     \"credinex_account\":{\n",
    "#         \"increment_database\":\"credinex_account_stream_etl\",\n",
    "#         \"database\":\"credinex_account_etl_s3\",\n",
    "#         \"realTime_path\":\"credinex_account\"\n",
    "#     },\n",
    "#     \"credinex_repeater\":{\n",
    "#         \"increment_database\":\"credinex_repeater_stream_etl\",\n",
    "#         \"database\":\"credinex_repeater_etl_s3\",\n",
    "#         \"realTime_path\":\"credinex_repeaters\"\n",
    "#     },\n",
    "#     \"credinex_hive\":{\n",
    "#         \"increment_database\":\"credinex_hive_stream_etl\",\n",
    "#         \"database\":\"credinex_hive_etl_s3\",\n",
    "#         \"realTime_path\":\"credinex_hive\"\n",
    "#     },\n",
    "#     \"lovina\":{\n",
    "#         \"increment_database\":\"lovina_stream_etl\",\n",
    "#         \"database\":\"lovina_etl_s3\",\n",
    "#         \"realTime_path\":\"lovina\"\n",
    "#     }\n",
    "}\n",
    "# increment_database=\"banda_stream_etl\"\n",
    "# # tableNm=\"t_loan_app\"\n",
    "# database=\"banda-etl-s3\"\n",
    "nowdate=(datetime.now()).strftime('%Y-%m-%d')\n",
    "yesterday=(datetime.now()+ timedelta(-1)).strftime('%Y-%m-%d')\n",
    "sqltemp=\"\"\"\n",
    "CREATE OR REPLACE VIEW {tableNm}_stream AS \n",
    "with {update_snapshot_table} as (\n",
    "  select *\n",
    "    from (\n",
    "      SELECT * ,\n",
    "         row_number() OVER (PARTITION BY id ORDER BY parse_datetime(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "      FROM \"{increment_database}\".\"{tableNm}\"\n",
    "      where date(year || '-' || month || '-' || day) = current_date\n",
    "      )\n",
    "    where row_num = 1\n",
    "  )\n",
    "select  {select_sql} \n",
    "from \"{database}\".\"{tableNm}\" l\n",
    "left join {update_snapshot_table}\n",
    "on l.id = {update_snapshot_table}.id\n",
    "where if({update_snapshot_table}.kind is null ,'', {update_snapshot_table}.kind )<> 'delete'\n",
    "union\n",
    "select {update_snapshot_sql} from (\n",
    "  select *\n",
    "  from \"{increment_database}\".\"{tableNm}\"\n",
    "  where\n",
    "    date(year || '-' || month || '-' || day) = current_date \n",
    "    and kind = 'insert'\n",
    "  ) new\n",
    "left join {update_snapshot_table}\n",
    "on new.id = {update_snapshot_table}.id  where if({update_snapshot_table}.kind is null ,'', {update_snapshot_table}.kind )<> 'delete'; \"\"\"\n",
    "def getTableColum(b,update_snapshot_table):\n",
    "    colum=\"\"\n",
    "    snapshot_colum=\"\"\n",
    "    sqlstrtemp=\"if({update_snapshot_table}.id is null, {colNm}     , {snapshot_colNm})  {ascolNm} ,\"\n",
    "    for index in range(len(b)):\n",
    "        if(index==1 or (index>2 and index<len(b)-8)):   \n",
    "            sqlstr=copy.copy(sqlstrtemp)\n",
    "            colum=colum+sqlstr.format(colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],False),snapshot_colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],True),ascolNm=b[index][\"col_name\"],update_snapshot_table=update_snapshot_table)\n",
    "            snapshot_colum=snapshot_colum+setSnapshotDef(b[index][\"data_type\"],b[index][\"col_name\"]).format(update_snapshot_table=update_snapshot_table)+\" ,\"\n",
    "    return colum[0:len(colum)-2],snapshot_colum[0:len(snapshot_colum)-2]\n",
    "def setDef(type,table_col,is_snapshot):\n",
    "    if is_snapshot:\n",
    "        return \"{update_snapshot_table}.\"+table_col\n",
    "    else:\n",
    "        return \"l.\"+table_col\n",
    "def setSnapshotDef(type,table_col):\n",
    "    return \"{update_snapshot_table}.\"+table_col\n",
    "def execute(tablerow,dbtype):\n",
    "#     print(tablerow,dbtype)\n",
    "    tableNm=tablerow[\"tableName\"]\n",
    "#     if tableNm=='t_user_install_info':\n",
    "    update_snapshot_table=\"update_snapshot_\"+dbtype+\"_\"+tableNm\n",
    "    spark.catalog.refreshTable(\"`\"+dbmap[dbtype][\"database\"]+\"`.\"+tableNm)\n",
    "    col=spark.sql(\"desc `\"+dbmap[dbtype][\"increment_database\"]+\"`.\"+tableNm).collect()\n",
    "    #     #真正的列,增量的列\n",
    "    real_col,snapshot_col=getTableColum(col,update_snapshot_table)\n",
    "#     sql=copy.copy(sqltemp)\n",
    "#     real_sql=sql.format(increment_database=dbmap[dbtype][\"increment_database\"],tableNm=tableNm,nowdate=nowdate,select_sql=real_col.format(update_snapshot_table=update_snapshot_table),update_snapshot_sql=snapshot_col.format(update_snapshot_table=update_snapshot_table),database=dbmap[dbtype][\"database\"],yesterday=yesterday,update_snapshot_table=update_snapshot_table)\n",
    "#     tablepath=\"s3://rupiahplus-data-warehouse/aliyun/\"+dbmap[dbtype][\"realTime_path\"]+\"/\"+tableNm\n",
    "#     print(real_sql)\n",
    "#         spark.sql(real_sql).drop(\"etlindex\").write.mode(\"overwrite\").orc(tablepath)\n",
    "#         spark.sql(real_sql).where(\"id==1731052\").show()\n",
    "#     spark.sql(real_sql).drop(\"etlindex\").write.mode(\"overwrite\").format(\"orc\").saveAsTable(\"etl_s3_temp.\"+dbtype+\"_\"+tableNm);\n",
    "#     spark.sql(\"select  * from  \"+\"etl_s3_temp.\"+dbtype+\"_\"+tableNm).write.mode(\"overwrite\").orc(tablepath)\n",
    "# __main__\n",
    "if __name__ == \"builtins\":\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Python Demo\")\\\n",
    "        .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "    spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "    spark.conf.set(\"spark.files.overwrite\",\"true\")\n",
    "#     ,Apache Arrow：一个跨平台的在内存中以列式存储的数据层，用来加速大数据分析速度。其可以一次性传入更大块的数据，pyspark中已经有载入该模块，需要打开该设置：\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "#     创建表时自动删除已存在的目录\n",
    "    spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "    for dbtype in dbmap:\n",
    "        databasesql=\"show tables in \"+dbmap[dbtype][\"increment_database\"]\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "    #         for row in tablelist:\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(execute, table,dbtype) for table in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d3a8b3896942328502e3670d6e7b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling pyathena-2.0.0:\n",
      "  Successfully uninstalled pyathena-2.0.0\n",
      "\n",
      "Uninstalling pandas-1.1.4:\n",
      "  Successfully uninstalled pandas-1.1.4\n",
      "\n",
      "Collecting pyathena\n",
      "  Using cached https://files.pythonhosted.org/packages/f5/27/53fc42c07c8ccee31173e37599bb63dc6d53ade00660d12cf3cb83e1d1e3/PyAthena-2.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: tenacity>=4.1.0 in /mnt/tmp/1606893858830-0/lib/python3.7/site-packages (from pyathena)\n",
      "Requirement already satisfied: boto3>=1.4.4 in /mnt/tmp/1606893858830-0/lib/python3.7/site-packages (from pyathena)\n",
      "Requirement already satisfied: botocore>=1.5.52 in /mnt/tmp/1606893858830-0/lib/python3.7/site-packages (from pyathena)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/site-packages (from tenacity>=4.1.0->pyathena)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3>=1.4.4->pyathena)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /mnt/tmp/1606893858830-0/lib/python3.7/site-packages (from boto3>=1.4.4->pyathena)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /mnt/tmp/1606893858830-0/lib/python3.7/site-packages (from botocore>=1.5.52->pyathena)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /mnt/tmp/1606893858830-0/lib/python3.7/site-packages (from botocore>=1.5.52->pyathena)\n",
      "Installing collected packages: pyathena\n",
      "Successfully installed pyathena-2.0.0\n",
      "\n",
      "Collecting pandas\n",
      "  Using cached https://files.pythonhosted.org/packages/bf/4c/cb7da76f3a5e077e545f9cf8575b8f488a4e8ad60490838f89c5cdd5bb57/pandas-1.1.4-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib64/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /mnt/tmp/1606893858830-0/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.1.4\n",
      "\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019619\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019640\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019652\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019662\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019671\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019681\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019690\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019699\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019709\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019719\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019729\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019738\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019748\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019757\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019767\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019777\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019786\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019796\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019805\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019814\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019824\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019833\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019842\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019852\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019862\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019871\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019880\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019890\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019899\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019908\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019917\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019927\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019936\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019945\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019954\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019963\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019972\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019981\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.019991\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.020000\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.020009\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.020019\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.020028\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.020037\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.020046\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.020055\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.020064\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.020073\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.020083\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.020092\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.020101\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.020110\n",
      "None\n",
      "end----- 2020-12-02 08:07:37.020119\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828149\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828170\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828181\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828190\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828200\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828213\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828223\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828232\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828242\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828251\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828260\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828269\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828278\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828287\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828296\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828305\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828314\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828323\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828332\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828341\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828350\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828359\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828368\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828378\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828387\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828396\n",
      "None\n",
      "end----- 2020-12-02 08:08:09.828406\n",
      "None\n",
      "end----- 2020-12-02 08:08:20.640528\n",
      "None\n",
      "end----- 2020-12-02 08:08:20.640549\n",
      "None\n",
      "end----- 2020-12-02 08:08:20.640560\n",
      "None\n",
      "end----- 2020-12-02 08:08:20.640569\n",
      "None\n",
      "end----- 2020-12-02 08:08:20.640579\n",
      "None\n",
      "end----- 2020-12-02 08:08:20.640588\n",
      "None\n",
      "end----- 2020-12-02 08:08:20.640597\n",
      "None\n",
      "end----- 2020-12-02 08:08:20.640607\n",
      "None\n",
      "end----- 2020-12-02 08:08:25.501789\n",
      "None\n",
      "end----- 2020-12-02 08:08:25.501810\n",
      "None\n",
      "end----- 2020-12-02 08:08:25.501821\n",
      "None\n",
      "end----- 2020-12-02 08:08:33.908706\n",
      "None\n",
      "end----- 2020-12-02 08:08:33.908726\n",
      "None\n",
      "end----- 2020-12-02 08:08:33.908736\n",
      "None\n",
      "end----- 2020-12-02 08:08:33.908746\n",
      "None\n",
      "end----- 2020-12-02 08:08:33.908755\n",
      "None\n",
      "end----- 2020-12-02 08:08:33.908764\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316409\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316431\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316443\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316452\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316462\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316471\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316480\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316490\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316499\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316508\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316517\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316527\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316536\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316545\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316554\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316564\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316573\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316583\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316592\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316601\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316611\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316620\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316629\n",
      "None\n",
      "end----- 2020-12-02 08:09:03.316638\n",
      "None\n",
      "end----- 2020-12-02 08:09:08.200845\n",
      "None\n",
      "end----- 2020-12-02 08:09:08.200864\n",
      "None\n",
      "end----- 2020-12-02 08:09:08.200875\n",
      "None\n",
      "end----- 2020-12-02 08:09:19.007986\n",
      "None\n",
      "end----- 2020-12-02 08:09:19.008007\n",
      "None\n",
      "end----- 2020-12-02 08:09:19.008018\n",
      "None\n",
      "end----- 2020-12-02 08:09:19.008028\n",
      "None\n",
      "end----- 2020-12-02 08:09:19.008038\n",
      "None\n",
      "end----- 2020-12-02 08:09:19.008048\n",
      "None\n",
      "end----- 2020-12-02 08:09:19.008057\n",
      "None\n",
      "end----- 2020-12-02 08:09:19.008067\n",
      "None\n",
      "end----- 2020-12-02 08:09:25.035857\n",
      "None\n",
      "end----- 2020-12-02 08:09:25.035882\n",
      "None\n",
      "end----- 2020-12-02 08:09:25.035893\n",
      "None\n",
      "end----- 2020-12-02 08:09:25.035902\n",
      "None\n",
      "end----- 2020-12-02 08:09:32.427432\n",
      "None\n",
      "end----- 2020-12-02 08:09:32.427453\n",
      "None\n",
      "end----- 2020-12-02 08:09:32.427464\n",
      "None\n",
      "end----- 2020-12-02 08:09:32.427473\n",
      "None\n",
      "end----- 2020-12-02 08:09:32.427483\n",
      "None\n",
      "end----- 2020-12-02 08:09:36.147827\n",
      "None\n",
      "end----- 2020-12-02 08:09:36.147849"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime,timedelta\n",
    "import copy\n",
    "import json\n",
    "import pytz\n",
    "import hmac\n",
    "def install():\n",
    "    sc.uninstall_package('pyathena')\n",
    "    sc.uninstall_package('pandas')\n",
    "    sc.install_pypi_package(\"pyathena\")\n",
    "    sc.install_pypi_package(\"pandas\")\n",
    "install()\n",
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "from hashlib import sha256 \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import types as T\n",
    "conn = connect(aws_access_key_id='AKIA4OO3YD6F6H3BAHEZ',\n",
    "               aws_secret_access_key='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',\n",
    "               s3_staging_dir='s3://aws-athena-query-results-ap-southeast-1-855696220043',\n",
    "               region_name='ap-southeast-1')\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Python Demo\")\\\n",
    "    .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "spark.conf.set(\"spark.files.overwrite\",\"true\")\n",
    "#     ,Apache Arrow：一个跨平台的在内存中以列式存储的数据层，用来加速大数据分析速度。其可以一次性传入更大块的数据，pyspark中已经有载入该模块，需要打开该设置：\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "#     创建表时自动删除已存在的目录\n",
    "spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "dbmap={\n",
    "      \"banda\":{\n",
    "        \"increment_database\":\"banda_stream_etl\",\n",
    "        \"database\":\"banda-etl-s3\",\n",
    "        \"realTime_path\":\"banda\"\n",
    "    },\n",
    "    \"luzon\":{\n",
    "         \"increment_database\":\"luzon_stream_etl\",\n",
    "         \"database\":\"luzon_etl_s3\",\n",
    "         \"realTime_path\":\"luzon\"\n",
    "    },\n",
    "    \"telemarket\":{\n",
    "         \"increment_database\":\"telemarket_stream_etl\",\n",
    "         \"database\":\"telemarket_etl_s3\",\n",
    "         \"realTime_path\":\"telemarket\"\n",
    "    },\n",
    "     \"coupon\":{\n",
    "        \"increment_database\":\"coupon_stream_etl\",\n",
    "        \"database\":\"coupon_etl_s3\",\n",
    "        \"realTime_path\":\"coupon\"\n",
    "    },\n",
    "    \"notification\":{\n",
    "        \"increment_database\":\"notification_stream_etl\",\n",
    "        \"database\":\"notification_etl_s3\",\n",
    "        \"realTime_path\":\"notification\"\n",
    "    },\n",
    "    \"arkham\":{\n",
    "        \"increment_database\":\"arkham_stream_etl\",\n",
    "        \"database\":\"arkham_etl_s3\",\n",
    "        \"realTime_path\":\"arkham\"\n",
    "    },\n",
    "    \"protoss\":{\n",
    "        \"increment_database\":\"protoss_stream_etl\",\n",
    "        \"database\":\"protoss_etl_s3\",\n",
    "        \"realTime_path\":\"protoss\"\n",
    "    },\n",
    "    \"credinex_account\":{\n",
    "        \"increment_database\":\"credinex_account_stream_etl\",\n",
    "        \"database\":\"credinex_account_etl_s3\",\n",
    "        \"realTime_path\":\"credinex_account\"\n",
    "    },\n",
    "    \"credinex_repeater\":{\n",
    "        \"increment_database\":\"credinex_repeater_stream_etl\",\n",
    "        \"database\":\"credinex_repeater_etl_s3\",\n",
    "        \"realTime_path\":\"credinex_repeaters\"\n",
    "    },\n",
    "    \"credinex_hive\":{\n",
    "        \"increment_database\":\"credinex_hive_stream_etl\",\n",
    "        \"database\":\"credinex_hive_etl_s3\",\n",
    "        \"realTime_path\":\"credinex_hive\"\n",
    "    },\n",
    "    \"lovina\":{\n",
    "        \"increment_database\":\"lovina_stream_etl\",\n",
    "        \"database\":\"lovina_etl_s3\",\n",
    "        \"realTime_path\":\"lovina\"\n",
    "    }\n",
    "}\n",
    "nowdate=(datetime.now()).strftime('%Y-%m-%d')\n",
    "yesterday=(datetime.now()+ timedelta(-1)).strftime('%Y-%m-%d')\n",
    "# where date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "sqltemp=\"\"\"\n",
    "CREATE OR REPLACE VIEW \"{database}\".{tableNm}_stream AS \n",
    "with {update_snapshot_table} as (\n",
    "  select *\n",
    "    from (\n",
    "      SELECT * ,\n",
    "         row_number() OVER (PARTITION BY id ORDER BY parse_datetime(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "         FROM \"{increment_database}\".\"{tableNm}\"\n",
    "         where date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "      )\n",
    "    where row_num = 1\n",
    "  )\n",
    "select  {select_sql} \n",
    "from \"{database}\".\"{tableNm}\" l\n",
    "left join {update_snapshot_table}\n",
    "on l.id = {update_snapshot_table}.id\n",
    "where if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete'\n",
    "union\n",
    "select {update_snapshot_sql} from (\n",
    "  select *\n",
    "  from \"{increment_database}\".\"{tableNm}\"\n",
    "  where\n",
    "    date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "    and kind = 'insert'\n",
    "  ) new\n",
    "left join {update_snapshot_table}\n",
    "on new.id = {update_snapshot_table}.id where if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete' \"\"\"\n",
    "def get_secret_obj():\n",
    "    df=spark.read.text(\"s3://rupiahplus-configs/etl/data_secrt/col.json\").collect()\n",
    "    keymap=''\n",
    "    for  row in df:\n",
    "        keymap=keymap+row['value']\n",
    "    json_content=json.loads(keymap)\n",
    "    return json_content\n",
    "colmap= get_secret_obj()   \n",
    "def getTableColum(b,update_snapshot_table,dbtype,tableNm):\n",
    "    colum=\"\"\n",
    "    snapshot_colum=\"\"\n",
    "    sqlstrtemp=\"if({update_snapshot_table}.id is null, {colNm}     , {snapshot_colNm})  {ascolNm} ,\"\n",
    "    for index in range(len(b)):\n",
    "        if((index==1 or (index>2 and index<len(b)-7)) and (colmap.get(dbtype)==None or colmap.get(dbtype)!=None and (colmap[dbtype].get(tableNm)==None or colmap[dbtype].get(tableNm)!=None  and \n",
    "                                                          b[index][\"col_name\"] not in colmap[dbtype].get(tableNm)))) :\n",
    "            sqlstr=copy.copy(sqlstrtemp)\n",
    "            colum=colum+sqlstr.format(colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],False),snapshot_colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],True),ascolNm=b[index][\"col_name\"],update_snapshot_table=update_snapshot_table)\n",
    "            snapshot_colum=snapshot_colum+setSnapshotDef(b[index][\"data_type\"],b[index][\"col_name\"]).format(update_snapshot_table=update_snapshot_table)+\" ,\"\n",
    "    return colum[0:len(colum)-2],snapshot_colum[0:len(snapshot_colum)-2]\n",
    "def setDef(type,table_col,is_snapshot):\n",
    "    if is_snapshot:\n",
    "        return \"{update_snapshot_table}.\"+table_col\n",
    "    else:\n",
    "        return \"l.\"+table_col\n",
    "def setSnapshotDef(type,table_col):\n",
    "    return \"{update_snapshot_table}.\"+table_col\n",
    "def execute(tablerow,dbtype):\n",
    "    tableName=tablerow\n",
    "    update_snapshot_table=\"update_snapshot_\"+dbtype+\"_\"+tableName\n",
    "    col=spark.sql(\"desc `\"+dbmap[dbtype][\"increment_database\"]+\"`.\"+tableName).collect()\n",
    "    real_col,snapshot_col=getTableColum(col,update_snapshot_table,dbtype,tableName)\n",
    "    sql=copy.copy(sqltemp)\n",
    "    real_sql=sql.format(increment_database=dbmap[dbtype][\"increment_database\"],tableNm=tableName,nowdate=nowdate,select_sql=real_col.format(update_snapshot_table=update_snapshot_table),update_snapshot_sql=snapshot_col.format(update_snapshot_table=update_snapshot_table),database=dbmap[dbtype][\"database\"],yesterday=yesterday,update_snapshot_table=update_snapshot_table)\n",
    "#     print(real_sql)\n",
    "    pd.read_sql(real_sql, conn)\n",
    "if __name__ == \"builtins\":\n",
    "    for dbtype in dbmap:\n",
    "        databasesql=\"show tables in \"+dbmap[dbtype][\"increment_database\"]\n",
    "        df=pd.read_sql(databasesql, conn)\n",
    "        tablelist=df[\"tab_name\"]\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(execute, table,dbtype) for table in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())\n",
    "                print(\"end-----\",datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485dcb03b74d495a859b449e7084e2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling pyathena-2.0.0:\n",
      "  Successfully uninstalled pyathena-2.0.0\n",
      "\n",
      "Uninstalling pandas-1.1.4:\n",
      "  Successfully uninstalled pandas-1.1.4\n",
      "\n",
      "Collecting pyathena\n",
      "  Using cached https://files.pythonhosted.org/packages/f5/27/53fc42c07c8ccee31173e37599bb63dc6d53ade00660d12cf3cb83e1d1e3/PyAthena-2.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: tenacity>=4.1.0 in /mnt/tmp/1606810797628-0/lib/python3.7/site-packages (from pyathena)\n",
      "Requirement already satisfied: boto3>=1.4.4 in /mnt/tmp/1606810797628-0/lib/python3.7/site-packages (from pyathena)\n",
      "Requirement already satisfied: botocore>=1.5.52 in /mnt/tmp/1606810797628-0/lib/python3.7/site-packages (from pyathena)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/site-packages (from tenacity>=4.1.0->pyathena)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3>=1.4.4->pyathena)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /mnt/tmp/1606810797628-0/lib/python3.7/site-packages (from boto3>=1.4.4->pyathena)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /mnt/tmp/1606810797628-0/lib/python3.7/site-packages (from botocore>=1.5.52->pyathena)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /mnt/tmp/1606810797628-0/lib/python3.7/site-packages (from botocore>=1.5.52->pyathena)\n",
      "Installing collected packages: pyathena\n",
      "Successfully installed pyathena-2.0.0\n",
      "\n",
      "Collecting pandas\n",
      "  Using cached https://files.pythonhosted.org/packages/bf/4c/cb7da76f3a5e077e545f9cf8575b8f488a4e8ad60490838f89c5cdd5bb57/pandas-1.1.4-cp37-cp37m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib64/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /mnt/tmp/1606810797628-0/lib/python3.7/site-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.1.4\n",
      "\n",
      "                    etldate        id  ...               instance  year\n",
      "0   2020-11-30 17:05:13.168  20662649  ...  job-instance-adapundi  2020\n",
      "1   2020-11-30 17:05:39.131  20662676  ...  job-instance-adapundi  2020\n",
      "2   2020-11-30 17:05:44.133  20662681  ...  job-instance-adapundi  2020\n",
      "3   2020-11-30 17:05:45.124  20662682  ...  job-instance-adapundi  2020\n",
      "4   2020-11-30 17:06:29.771  20662706  ...     job-instance-1-245  2020\n",
      "..                      ...       ...  ...                    ...   ...\n",
      "95  2020-12-01 03:02:30.312  20673508  ...     job-instance-1-245  2020\n",
      "96  2020-12-01 03:05:09.161  20673531  ...  job-instance-adapundi  2020\n",
      "97  2020-12-01 03:05:19.216  20673542  ...  job-instance-adapundi  2020\n",
      "98  2020-12-01 03:05:37.180  20673561  ...  job-instance-adapundi  2020\n",
      "99  2020-12-01 03:06:29.720  20673591  ...     job-instance-1-245  2020\n",
      "\n",
      "[100 rows x 11 columns]"
     ]
    }
   ],
   "source": [
    "def install():\n",
    "    sc.uninstall_package('pyathena')\n",
    "    sc.uninstall_package('pandas')\n",
    "    sc.install_pypi_package(\"pyathena\")\n",
    "    sc.install_pypi_package(\"pandas\")\n",
    "install()\n",
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "conn = connect(aws_access_key_id='AKIA4OO3YD6F6H3BAHEZ',\n",
    "               aws_secret_access_key='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',\n",
    "               s3_staging_dir='s3://aws-athena-query-results-ap-southeast-1-855696220043',\n",
    "               region_name='ap-southeast-1')\n",
    "sql_01=\"\"\"\n",
    "select  * from \"banda-etl-s3\".t_job_metric  limit 100\n",
    "\"\"\"\n",
    "pd.read_sql(sql_01, conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3d406db9bc4a96b6e5f730b60ef5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "{'t_ai_rudder_schedule_mobile': ['mobile'], 't_mobile_tag_rel': ['mobile'], 't_code': ['send_to'], 't_personal_info': ['credential_no', 'backup_mobile'], 't_review_blacklist': ['value'], 't_contact': ['mobile'], 't_customer': ['mobile'], 't_employment': ['company_phone'], 't_loan_app': ['credential_no'], 't_login_log': ['mobile'], 't_record_personal_info': ['credential_no', 'backup_mobile'], 't_record_employment': ['company_phone'], 't_record_contact': ['mobile'], 't_record_file': ['path'], 't_sms': ['sendto']}"
     ]
    }
   ],
   "source": [
    "a=['3','4','5']\n",
    "print('6' not in a )\n",
    "\n",
    "print(colmap.get(\"banda\"))\n",
    "b=[0 for i in range(20)]\n",
    "index=3\n",
    "dbtype='banda'\n",
    "tableNm='t_admin'\n",
    "if((index==1 or (index>2 and index<len(b)-7)) and (colmap.get(dbtype)==None or\n",
    "                                                         ( colmap.get(dbtype)!=None and colmap[dbtype].get(tableNm)!=None  and \n",
    "                                                          b[index][\"col_name\"] not in colmap[dbtype].get(tableNm)))):\n",
    "    \n",
    "    print(true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
