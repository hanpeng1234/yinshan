{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with update_snapshot as (\n",
    "  -- 只包含了当天所有update类型的最终数据。\n",
    "  select *\n",
    "    from (\n",
    "      SELECT * ,\n",
    "         row_number() OVER (PARTITION BY id ORDER BY date_parse(etldate,'%Y-%m-%d %H:%i:%s.%f') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "      FROM banda_stream_etl.t_loan_app\n",
    "      where date(year || '-' || month || '-' || day) <= date(current_date) and date(year || '-' || month || '-' || day) >= date(current_date) \n",
    "      )\n",
    "    where row_num = 1\n",
    "  )\n",
    "select\n",
    "-- 通过将历史数据与当天的update数据结合，来更新历史数据。\n",
    "l.id,\n",
    "l.customer_id,\n",
    "if(update_snapshot.id is null, l.status              , update_snapshot.status              ) status              ,\n",
    "if(update_snapshot.id is null, l.sub_status          , update_snapshot.sub_status          ) sub_status          ,\n",
    "if(update_snapshot.id is null, l.full_name           , update_snapshot.full_name           ) full_name           ,\n",
    "if(update_snapshot.id is null, l.credential_no       , update_snapshot.credential_no       ) credential_no       ,\n",
    "if(update_snapshot.id is null, l.credential_type     , update_snapshot.credential_type     ) credential_type     ,\n",
    "if(update_snapshot.id is null, l.loan_type           , update_snapshot.loan_type           ) loan_type           ,\n",
    "if(update_snapshot.id is null, l.amount              , update_snapshot.amount              ) amount              ,\n",
    "if(update_snapshot.id is null, l.repayment_type      , update_snapshot.repayment_type      ) repayment_type      ,\n",
    "if(update_snapshot.id is null, l.service_fee         , update_snapshot.service_fee         ) service_fee         ,\n",
    "if(update_snapshot.id is null, l.pre_service_fee     , update_snapshot.pre_service_fee     ) pre_service_fee     ,\n",
    "if(update_snapshot.id is null, l.interest_rate       , update_snapshot.interest_rate       ) interest_rate       ,\n",
    "if(update_snapshot.id is null, l.period              , update_snapshot.period              ) period              ,\n",
    "if(update_snapshot.id is null, l.period_unit         , update_snapshot.period_unit         ) period_unit         ,\n",
    "if(update_snapshot.id is null, l.effective_time      , update_snapshot.effective_time      ) effective_time      ,\n",
    "if(update_snapshot.id is null, l.apply_for           , update_snapshot.apply_for           ) apply_for           ,\n",
    "if(update_snapshot.id is null, l.apply_channel       , update_snapshot.apply_channel       ) apply_channel       ,\n",
    "if(update_snapshot.id is null, l.apply_platform      , update_snapshot.apply_platform      ) apply_platform      ,\n",
    "if(update_snapshot.id is null, l.comment             , update_snapshot.comment             ) comment             ,\n",
    "if(update_snapshot.id is null, l.reviewer_id         , update_snapshot.reviewer_id         ) reviewer_id         ,\n",
    "if(update_snapshot.id is null, l.create_time         , update_snapshot.create_time         ) create_time         ,\n",
    "if(update_snapshot.id is null, l.update_time         , update_snapshot.update_time         ) update_time         ,\n",
    "if(update_snapshot.id is null, l.paid_off_mode       , update_snapshot.paid_off_mode       ) paid_off_mode       ,\n",
    "if(update_snapshot.id is null, l.parent_id           , update_snapshot.parent_id           ) parent_id           ,\n",
    "if(update_snapshot.id is null, l.apply_purpose       , update_snapshot.apply_purpose       ) apply_purpose       ,\n",
    "if(update_snapshot.id is null, l.imei                , update_snapshot.imei                ) imei                ,\n",
    "if(update_snapshot.id is null, l.ip                  , update_snapshot.ip                  ) ip                  ,\n",
    "if(update_snapshot.id is null, l.grace_period_rate   , update_snapshot.grace_period_rate   ) grace_period_rate   ,\n",
    "if(update_snapshot.id is null, l.overdue_rate        , update_snapshot.overdue_rate        ) overdue_rate        ,\n",
    "if(update_snapshot.id is null, l.haircut             , update_snapshot.haircut             ) haircut             ,\n",
    "if(update_snapshot.id is null, l.haircut_issue_status, update_snapshot.haircut_issue_status) haircut_issue_status,\n",
    "if(update_snapshot.id is null, l.haircut_rate        , update_snapshot.haircut_rate        ) haircut_rate        ,\n",
    "if(update_snapshot.id is null, l.product_name        , update_snapshot.product_name        ) product_name        ,\n",
    "if(update_snapshot.id is null, l.current_lpay_id     , update_snapshot.current_lpay_id     ) current_lpay_id     ,\n",
    "if(update_snapshot.id is null, l.android_id          , update_snapshot.android_id          ) android_id          ,\n",
    "if(update_snapshot.id is null, l.initial_loan_id     , update_snapshot.initial_loan_id     ) initial_loan_id      \n",
    "from \"banda-etl-s3\".t_loan_app l\n",
    "left join update_snapshot\n",
    "on l.id = update_snapshot.id\n",
    "where update_snapshot.kind<> 'delete'\n",
    "union\n",
    "select\n",
    "-- 获取今天的insert数据，并更新至最新状态。\n",
    "update_snapshot.id                  ,\n",
    "update_snapshot.customer_id         ,\n",
    "update_snapshot.status              ,\n",
    "update_snapshot.sub_status          ,\n",
    "update_snapshot.full_name           ,\n",
    "update_snapshot.credential_no       ,\n",
    "update_snapshot.credential_type     ,\n",
    "update_snapshot.loan_type           ,\n",
    "update_snapshot.amount              ,\n",
    "update_snapshot.repayment_type      ,\n",
    "update_snapshot.service_fee         ,\n",
    "update_snapshot.pre_service_fee     ,\n",
    "update_snapshot.interest_rate       ,\n",
    "update_snapshot.period              ,\n",
    "update_snapshot.period_unit         ,\n",
    "update_snapshot.effective_time      ,\n",
    "update_snapshot.apply_for           ,\n",
    "update_snapshot.apply_channel       ,\n",
    "update_snapshot.apply_platform      ,\n",
    "update_snapshot.comment             ,\n",
    "update_snapshot.reviewer_id         ,\n",
    "update_snapshot.create_time         ,\n",
    "update_snapshot.update_time         ,\n",
    "update_snapshot.paid_off_mode       ,\n",
    "update_snapshot.parent_id           ,\n",
    "update_snapshot.apply_purpose       ,\n",
    "update_snapshot.imei                ,\n",
    "update_snapshot.ip                  ,\n",
    "update_snapshot.grace_period_rate   ,\n",
    "update_snapshot.overdue_rate        ,\n",
    "update_snapshot.haircut             ,\n",
    "update_snapshot.haircut_issue_status,\n",
    "update_snapshot.haircut_rate        ,\n",
    "update_snapshot.product_name        ,\n",
    "update_snapshot.current_lpay_id     ,\n",
    "update_snapshot.android_id          ,\n",
    "update_snapshot.initial_loan_id \n",
    "from (\n",
    "  select *\n",
    "  from banda_stream_etl.t_loan_app\n",
    "  where\n",
    "    date(year || '-' || month || '-' || day) = current_date\n",
    "    and kind = 'insert'\n",
    "  ) new\n",
    "left join update_snapshot\n",
    "on new.id = update_snapshot.id\n",
    "where update_snapshot.kind<> 'delete'\n",
    "limit 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "banda的daily_etl代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb606155a39470c8bf471f5e260869a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_admin\n",
      "t_admin_audit\n",
      "t_admin_group_rel\n",
      "t_af_bill\n",
      "t_ai_rudder_schedule_mobile\n",
      "t_auto_review_loan\n",
      "t_bankcard\n",
      "t_channel_details\n",
      "t_clear_detail_log\n",
      "t_code\n",
      "t_collection_audit\n",
      "t_collection_blacklist\n",
      "t_contact\n",
      "t_coupon_send\n",
      "t_customer\n",
      "t_customer_app\n",
      "t_customer_device_info\n",
      "t_customer_install_info\n",
      "t_customer_tag_rel\n",
      "t_employment\n",
      "t_engine_rule_detail\n",
      "t_file\n",
      "t_group\n",
      "t_invitation\n",
      "t_job_metric\n",
      "t_loan_app\n",
      "t_loan_app_review_summary\n",
      "t_loan_app_status_log\n",
      "t_loan_issue\n",
      "t_loan_log\n",
      "t_loan_tag_rel\n",
      "t_login_log\n",
      "t_lpay\n",
      "t_lpay_deposit\n",
      "t_message\n",
      "t_personal_info\n",
      "t_record_bankcard\n",
      "t_record_contact\n",
      "t_record_employment\n",
      "t_record_file\n",
      "t_record_personal_info\n",
      "t_review_blacklist\n",
      "t_review_step_execution\n",
      "t_send_coupon_rel\n",
      "t_sms\n",
      "t_test_customer\n",
      "t_thirdparty_data\n",
      "t_tongdun_data\n",
      "t_virtual_account\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*\n",
    "# from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime,timedelta\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "import random\n",
    "import pytz\n",
    "nowdate=(datetime.now()).strftime('%Y-%m-%d')\n",
    "# print(datetime.now(pytz.timezone(\"Asia/Shanghai\")).strftime( '%Y-%m-%d'))\n",
    "beforeyesterday=(datetime.now()+ timedelta(-1)).strftime('%Y-%m-%d')+\" \"+(datetime.now()-timedelta(minutes=60)).strftime('%H:00:00')\n",
    "yesterday=(datetime.now()+ timedelta(-1)).strftime('%Y-%m-%d')\n",
    "partitionlist=yesterday.split(\"-\",3)\n",
    "# yesterday=yesterday+ \" 23:59:59\"\n",
    "tablemap={\"banda_stream_etl\":\"banda\"} \n",
    "database='banda-etl-s3'\n",
    "update_snapshotsql =\" select * from ( SELECT * , row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num FROM `{increment_database}`.`{tableNm}` where date(year || '-' || month || '-' || day) <='{nowdate}' and date(year || '-' || month || '-' || day) >='{yesterday}'  and   date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') >= (SELECT max(date_format(etldate, 'yyyy-MM-dd HH:mm:ss.SSS')) from  `{database}`.`{tableNm}`)) where row_num = 1\"\n",
    "def getTableColum(b):\n",
    "    colum=[]\n",
    "    for index in range(len(b)):\n",
    "        if(index==1 or ( index>2 and index<len(b)-8)):   \n",
    "            colum.append(b[index][\"col_name\"])\n",
    "    return colum\n",
    "def setDef(type,table_col):\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull(\"+table_col+\",0) as \"+table_col\n",
    "    else:\n",
    "        return table_col\n",
    "def rm_temp_table():\n",
    "    tables=spark.catalog.listTables()\n",
    "    for table in tables :\n",
    "        if table.isTemporary==True:\n",
    "            spark.catalog.dropTempView(table.name)\n",
    "def execute_sql(databaseName,row):\n",
    "    try:\n",
    "        tableName=row[\"tableName\"]\n",
    "        database='banda-etl-s3'\n",
    "        tablecolum=\"desc \" +databaseName+\".\"+tableName;\n",
    "        tableSchema= spark.sql(tablecolum).collect()\n",
    "        snapshotsql=update_snapshotsql.format(increment_database=databaseName,tableNm=tableName,database=database,nowdate=nowdate,yesterday=yesterday)\n",
    "#       tempNm=\"update_snapshot_\"+tableName\n",
    "#       etltable=\"etl_\"+tableName\n",
    "#       今天的增量数据\n",
    "        snapshot_df=spark.sql(snapshotsql)\n",
    "#       历史数据\n",
    "        etltable=spark.sql(\"select * from `\"+database+\"`.\"+tableName )\n",
    "#       更该字段类型\n",
    "        df1_r = snapshot_df.select(*(col(x).alias(\"update_snapshot_\"+x) for x in snapshot_df.columns))\n",
    "        df2_r = etltable.select(*(col(x).alias(\"etl_\"+x) for x in etltable.columns))\n",
    "#       join\n",
    "        df2=df2_r.join(df1_r, col('update_snapshot_id') == col('etl_id'), \"left\").fillna(\"\", subset=['update_snapshot_kind']).where(\"update_snapshot_kind!= 'delete'\")\n",
    "#       更新历史数据\n",
    "        df3=df2.selectExpr(*(\"if(update_snapshot_\"+x+\" is null,etl_\"+x+\",update_snapshot_\"+x+\") as \"+x for x in getTableColum(tableSchema)))\n",
    "#       今天的新增数据\n",
    "        nowDf=spark.sql(\"select * from \"+databaseName+\".\"+tableName).filter(\"date(year || '-' || month || '-' || day) <= date('\"+nowdate+\"') and date(year || '-' || month || '-' || day) >= date('\"+yesterday+\"')\")\n",
    "        nowDf.filter(\"date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') >= (SELECT max(date_format(etldate, 'yyyy-MM-dd HH:mm:ss.SSS')) from  `\"+database+\"`.`\"+tableName+\"`) and kind= 'insert'\")\n",
    "        nowDf=nowDf.join(df1_r,col('update_snapshot_id') == col('id'),\"left\").etltable=spark.sql(\"select * from `\"+database+\"`.\"+tableName )\\\n",
    "        .selectExpr(*(\"update_snapshot_\"+x+\" as \"+x for x in getTableColum(tableSchema)))\\\n",
    "#         .filter(\"update_snapshot_kind<> 'delete'\")\n",
    "        tablepath=\"s3://rupiahplus-data-warehouse/aliyun/\"+tablemap[databaseName]+\"_real/\"+row[\"tableName\"]\n",
    "    #     print(tablepath)\n",
    "        realdf=df3.intersect(nowDf).write.mode(\"overwrite\").format(\"orc\").saveAsTable(\"etl_s3_temp.\"+tablemap[databaseName]+\"_\"+tableName);\n",
    "        spark.sql(\"select  * from  \"+\"etl_s3_temp.\"+tablemap[databaseName]+\"_\"+tableName).write.mode(\"overwrite\").orc(tablepath)\n",
    "    except BaseException as e:\n",
    "        raise e\n",
    "# print(__name__)builtins    __main__\n",
    "if __name__ == \"builtins\":\n",
    "#     spark = SparkSession\\\n",
    "#         .builder\\\n",
    "#         .appName(\"Python Demo\")\\\n",
    "#         .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "#         .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "#         .enableHiveSupport()\\\n",
    "#         .getOrCreate()\n",
    "    spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "    spark.conf.set(\"spark.files.overwrite\",\"true\")\n",
    "#     ,Apache Arrow：一个跨平台的在内存中以列式存储的数据层，用来加速大数据分析速度。其可以一次性传入更大块的数据，pyspark中已经有载入该模块，需要打开该设置：\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "    #     创建表时自动删除已存在的目录\n",
    "    spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "#     databases=spark.sql(\"show databases\")\n",
    "#     databases=databases.collect()\n",
    "    rm_temp_table()\n",
    "    for databaseName in tablemap:\n",
    "#         print(databaseName)\n",
    "        databasesql=\"show tables in \"+databaseName\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "#         print(tablelist)\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor2:\n",
    "            futures_result=futures.wait([executor2.submit(execute_sql,databaseName, tableNm) for tableNm in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())\n",
    "\n",
    "#         for tableNm in tablelist:\n",
    "#                 df3.union(nowDf).filter(\"id=101175\").show(1,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "luzon和其他表的daily代码.未更改\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*\n",
    "# from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime,timedelta\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col\n",
    "import random\n",
    "import pytz\n",
    "nowdate=(datetime.now()).strftime('%Y-%m-%d')\n",
    "# print(datetime.now(pytz.timezone(\"Asia/Shanghai\")).strftime( '%Y-%m-%d'))\n",
    "beforeyesterday=(datetime.now()+ timedelta(-1)).strftime('%Y-%m-%d')+\" \"+(datetime.now()-timedelta(minutes=60)).strftime('%H:00:00')\n",
    "yesterday=(datetime.now()+ timedelta(-1)).strftime('%Y-%m-%d')\n",
    "partitionlist=yesterday.split(\"-\",3)\n",
    "# yesterday=yesterday+ \" 23:59:59\"\n",
    "tablemap={\"luzon_stream_etl\":\"luzon\",\"telemarket_stream_etl\":\"telemarket\"} \n",
    "update_snapshotsql =\" select * from ( SELECT * , row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num FROM `{increment_database}`.`{tableNm}` where date(year || '-' || month || '-' || day) <='{nowdate}' and date(year || '-' || month || '-' || day) >='{yesterday}'  and   date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') >= (SELECT max(date_format(etldate, 'yyyy-MM-dd HH:mm:ss.SSS')) from  `{database}`.`{tableNm}`)) where row_num = 1\"\n",
    "def getTableColum(b):\n",
    "    colum=[]\n",
    "    for index in range(len(b)):\n",
    "        if(index==1 or ( index>2 and index<len(b)-8)):   \n",
    "            colum.append(b[index][\"col_name\"])\n",
    "    return colum\n",
    "def setDef(type,table_col):\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull(\"+table_col+\",0) as \"+table_col\n",
    "    else:\n",
    "        return table_col\n",
    "def rm_temp_table():\n",
    "    tables=spark.catalog.listTables()\n",
    "    for table in tables :\n",
    "        if table.isTemporary==True:\n",
    "            spark.catalog.dropTempView(table.name)\n",
    "def execute_sql(databaseName,tabletype,row):\n",
    "    try:\n",
    "        tableName=row[\"tableName\"]\n",
    "        print(tableName)\n",
    "        tablecolum=\"desc \" +databaseName+\".\"+tableName;\n",
    "        tableSchema= spark.sql(tablecolum).collect()\n",
    "        database=tabletype+\"_etl_s3\"\n",
    "        snapshotsql=update_snapshotsql.format(increment_database=databaseName,tableNm=tableName,database=database,nowdate=nowdate,yesterday=yesterday)\n",
    "#       tempNm=\"update_snapshot_\"+tableName\n",
    "#       etltable=\"etl_\"+tableName\n",
    "#       今天的增量数据\n",
    "        snapshot_df=spark.sql(snapshotsql)\n",
    "#       历史数据\n",
    "        etltable=spark.sql(\"select * from `\"+database+\"`.\"+tableName )\n",
    "#       更该字段类型\n",
    "        df1_r = snapshot_df.select(*(col(x).alias(\"update_snapshot_\"+x) for x in snapshot_df.columns))\n",
    "        df2_r = etltable.select(*(col(x).alias(\"etl_\"+x) for x in etltable.columns))\n",
    "#       join\n",
    "        df2=df2_r.join(df1_r, col('update_snapshot_id') == col('etl_id'), \"left\")\n",
    "#       更新历史数据\n",
    "        df3=df2.selectExpr(*(\"if(update_snapshot_\"+x+\" is null,etl_\"+x+\",update_snapshot_\"+x+\") as \"+x for x in getTableColum(tableSchema)))\n",
    "#       今天的新增数据\n",
    "        nowDf=spark.sql(\"select * from \"+databaseName+\".\"+tableName).filter(\"date(year || '-' || month || '-' || day) <= date('\"+nowdate+\"') and date(year || '-' || month || '-' || day) >= date('\"+yesterday+\"')\")\n",
    "        nowDf.filter(\"date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') >= (SELECT max(date_format(etldate, 'yyyy-MM-dd HH:mm:ss.SSS')) from  `\"+database+\"`.`\"+tableName+\"`) and kind= 'insert'\")\n",
    "        nowDf=nowDf.join(df1_r,col('update_snapshot_id') == col('id'),\"left\").selectExpr(*(\"update_snapshot_\"+x+\" as \"+x for x in getTableColum(tableSchema)))\\\n",
    "        .filter(\"update_snapshot_kind<> 'delete'\")\n",
    "        tablepath=\"s3://rupiahplus-data-warehouse/aliyun/\"+tablemap[databaseName]+\"_real/\"+row[\"tableName\"]\n",
    "    #     print(tablepath)\n",
    "        realdf=df3.union(nowDf).distinct().write.mode(\"overwrite\").format(\"orc\").saveAsTable(\"etl_s3_temp.\"+tablemap[databaseName]+\"_\"+tableName);\n",
    "        spark.sql(\"select  * from  \"+\"etl_s3_temp.\"+tablemap[databaseName]+\"_\"+tableName).write.mode(\"overwrite\").orc(tablepath)\n",
    "    except BaseException as e:\n",
    "        raise e\n",
    "# print(__name__)builtins    __main__\n",
    "if __name__ == \"builtins\":\n",
    "#     spark = SparkSession\\\n",
    "#         .builder\\\n",
    "#         .appName(\"Python Demo\")\\\n",
    "#         .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "#         .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "#         .enableHiveSupport()\\\n",
    "#         .getOrCreate()\n",
    "    spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "    spark.conf.set(\"spark.files.overwrite\",\"true\")\n",
    "#     ,Apache Arrow：一个跨平台的在内存中以列式存储的数据层，用来加速大数据分析速度。其可以一次性传入更大块的数据，pyspark中已经有载入该模块，需要打开该设置：\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "#     databases=spark.sql(\"show databases\")\n",
    "#     databases=databases.collect()\n",
    "    rm_temp_table()\n",
    "    for databaseName in tablemap:\n",
    "#         print(databaseName)\n",
    "        databasesql=\"show tables in \"+databaseName\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "#         print(tablelist)\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor2:\n",
    "            futures_result=futures.wait([executor2.submit(execute_sql,databaseName,tablemap[databaseName] ,tableNm) for tableNm in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())\n",
    "\n",
    "#         for tableNm in tablelist:\n",
    "#                 df3.union(nowDf).filter(\"id=101175\").show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986a3474ce2545ba84d04c3f9d93d471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[a: string]"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4b46f444164864952efb18f64fca96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table(name='call_log', database='default', description=None, tableType='EXTERNAL', isTemporary=False)\n",
      "Table(name='sampledb', database='default', description=None, tableType='EXTERNAL', isTemporary=False)\n",
      "Table(name='t_customer', database='default', description=None, tableType=None, isTemporary=False)\n",
      "Table(name='update_snapshot_t_admin', database=None, description=None, tableType='TEMPORARY', isTemporary=True)"
     ]
    }
   ],
   "source": [
    "tables=spark.catalog.listTables()\n",
    "for table in tables :\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158f680986d94ab7954fcb44c36f93ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>11</td><td>application_1595214595250_0012</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-7-249.ap-southeast-1.compute.internal:20888/proxy/application_1595214595250_0012/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-15-119.ap-southeast-1.compute.internal:8042/node/containerlogs/container_1595214595250_0012_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------+------+--------------+--------------------+--------------------+-----+------+------------------+--------------------+--------------------+------------------+--------------+--------------------+-------------------+--------------------+------------------+--------------------+----+-----+---+-------+\n",
      "|  kind|             etldate|etlindex|    id|        mobile|            password|           full_name| type|status|failed_login_count|         create_time|         update_time|call_center_id_3cx|team_leader_id|       password_salt|         entry_time|               token|auto_call_id_yixun|   password_set_time|year|month|day|row_num|\n",
      "+------+--------------------+--------+------+--------------+--------------------+--------------------+-----+------+------------------+--------------------+--------------------+------------------+--------------+--------------------+-------------------+--------------------+------------------+--------------------+----+-----+---+-------+\n",
      "|update|2020-07-20 08:03:...|       0|101770|  082006121825|1256e571000ea50e5...|    Indah Miranti-Q0|ADMIN|ACTIVE|              null|2020-07-16 08:48:...|2020-07-20 08:03:...|              2002|        100066|facd17ea792f403fb...|2020-07-15 17:00:00|                null|              null|2020-07-16 08:48:...|2020|   07| 20|      1|\n",
      "|update|2020-07-20 08:00:...|       0|100385| 0852559341760|e9e8e7696074385c7...|   Jisman Planit -TL|ADMIN|ACTIVE|              null|2018-05-31 01:17:...|2020-07-20 08:00:...|              1100|          null|89f704ea500b47409...|2018-05-30 10:00:00|eyJhbGciOiJIUzUxM...|              1400|2020-06-28 06:27:...|2020|   07| 20|      1|\n",
      "|update|2020-07-20 07:19:...|       0|101668|08212533273611|368d73fa046ff14ee...|Adang beny tanadi-Q3|ADMIN|ACTIVE|              null|2020-04-30 11:01:...|2020-07-20 07:19:...|              1130|        100085|39a151df8e4943239...|2020-04-29 17:00:00|                null|              null|2020-06-28 05:20:...|2020|   07| 20|      1|\n",
      "|update|2020-07-20 07:58:...|       0|100406|       MBA1728|84377db2aed8432ef...|                 MBA|ADMIN|ACTIVE|              null|2018-06-08 02:29:...|2020-07-20 07:58:...|              null|          null|5b43d9c4da0c4a42b...|2018-06-06 17:00:00|                null|              null|2020-07-20 07:58:...|2020|   07| 20|      1|\n",
      "+------+--------------------+--------+------+--------------+--------------------+--------------------+-----+------+------------------+--------------------+--------------------+------------------+--------------+--------------------+-------------------+--------------------+------------------+--------------------+----+-----+---+-------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    " FROM  (SELECT * ,\n",
    "         row_number()\n",
    "        OVER (PARTITION BY id ORDER BY  date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "    FROM `banda_stream_etl`.`t_admin`\n",
    "    WHERE date(year || '-' || month || '-' || day) =date('2020-07-20')\n",
    "    and date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') >= (SELECT max(date_format(etldate, 'yyyy-MM-dd HH:mm:ss.SSS')) from  `banda-etl-s3`.`t_admin`))\n",
    "WHERE row_num = 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccf648130e6400184df49afe7dc3565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------+--------+------+------------+--------------------------------+----------+-----+------+------------------+-----------------------+-----------------------+------------------+--------------+--------------------------------+-------------------+-----+------------------+-----------------------+----+-----+---+-------+\n",
      "|kind  |etldate                |etlindex|id    |mobile      |password                        |full_name |type |status|failed_login_count|create_time            |update_time            |call_center_id_3cx|team_leader_id|password_salt                   |entry_time         |token|auto_call_id_yixun|password_set_time      |year|month|day|row_num|\n",
      "+------+-----------------------+--------+------+------------+--------------------------------+----------+-----+------+------------------+-----------------------+-----------------------+------------------+--------------+--------------------------------+-------------------+-----+------------------+-----------------------+----+-----+---+-------+\n",
      "|update|2020-07-20 09:52:29.158|0       |101514|008118188870|8161263de00f423b06fa99ad36ea72b5|Minerva-Q0|ADMIN|ACTIVE|null              |2020-03-20 08:55:47.298|2020-07-20 09:52:29.158|null              |101494        |478f620ae33a46c9befda0c2fe3513dd|2020-03-19 17:00:00|null |null              |2020-05-28 02:46:26.178|2020|07   |20 |1      |\n",
      "+------+-----------------------+--------+------+------------+--------------------------------+----------+-----+------+------------------+-----------------------+-----------------------+------------------+--------------+--------------------------------+-------------------+-----+------------------+-----------------------+----+-----+---+-------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    " FROM  (SELECT * ,\n",
    "         row_number()\n",
    "        OVER (PARTITION BY id ORDER BY  date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "    FROM `banda_stream_etl`.`t_admin`\n",
    "    WHERE date(year || '-' || month || '-' || day) <= date('2020-07-24') and date(year || '-' || month || '-' || day) >= date('2020-07-23')\n",
    "    and   date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') >= (SELECT max(date_format(etldate, 'yyyy-MM-dd HH:mm:ss.SSS')) from  `banda-etl-s3`.`t_admin`))\n",
    "WHERE row_num = 1\n",
    "\"\"\").show(100,False)\n",
    "#    SELECT max(date_format('2020-07-01 12:22:23', 'yyyy-MM-dd HH:mm:ss.SSS'))FROM `banda-etl-s3`.`t_admin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1ca74b8dbb4cb895c70e353094e3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+\n",
      "|max(date_format(CAST(etldate AS TIMESTAMP), yyyy-MM-dd HH:mm:ss.SSS))|\n",
      "+---------------------------------------------------------------------+\n",
      "|2020-07-20 08:51:15.820                                              |\n",
      "+---------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT max(date_format(etldate, 'yyyy-MM-dd HH:mm:ss.SSS')) from  `banda-etl-s3`.`t_admin`\"\"\").show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd032db27a54ae281c7404e49d5aeb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-20\n",
      "2020-07-19\n",
      "date(year || '-' || month || '-' || day) <= date('2020-07-20') and date(year || '-' || month || '-' || day) >= date('2020-07-19'))"
     ]
    }
   ],
   "source": [
    "from datetime import datetime,timedelta\n",
    "nowdate=(datetime.now()).strftime('%Y-%m-%d')\n",
    "yesterday=(datetime.now()+ timedelta(-1)).strftime('%Y-%m-%d')\n",
    "print(nowdate)\n",
    "print(yesterday)\n",
    "\n",
    "print(\"date(year || '-' || month || '-' || day) <= date('\"+nowdate+\"') and date(year || '-' || month || '-' || day) >= date('\"+yesterday+\"'))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d48ed1b28941629daef1c8468c95b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banda_stream_etl\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "\n",
    "# print(datetime.now(pytz.timezone(\"Asia/Shanghai\")).strftime( '%Y-%m-%d'))\n",
    "today=(datetime.now()+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "yesterday=(datetime.now(pytz.timezone(\"Asia/Shanghai\"))+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "tablemap={\"banda_stream_etl\":\"banda_real\"} #,\n",
    "# ,\"banda_stream_etl\":\"1\"\n",
    "tablesql_1=\" FROM  (   SELECT    *  , row_number() OVER (PARTITION BY id  ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC  , if(etlindex is null ,0,etlindex)  desc)  row_num FROM  \"\n",
    "tablesql_2=\" ORDER BY id  ASC )  a  WHERE a.row_num = 1 AND a.kind <> 'delete' \"\n",
    "# def getTableColum(b):\n",
    "#     colum=\"\";\n",
    "#     for index in range(len(b)):\n",
    "#         if(index>0 and index<len(b)-8):   \n",
    "#             colum=colum+b[index][\"col_name\"]+\", \"\n",
    "#     return colum+\"year,month,day\"\n",
    "def getTableColum(b):\n",
    "    colum=\"\"\n",
    "    for index in range(len(b)):\n",
    "        if(index>0 and index<len(b)-8):   \n",
    "            colum=colum+setDef(b[index][\"data_type\"],b[index][\"col_name\"])+\", \"\n",
    "    return colum[0:len(colum)-2]\n",
    "def setDef(type,table_col):\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull(\"+table_col+\",0) as \"+table_col\n",
    "    else:\n",
    "        return table_col\n",
    "def make_col_temptable(databaseName,row):\n",
    "    tableName=row[\"tableName\"]\n",
    "    if tableName=='t_coupon_send':\n",
    "        tablecolum=\"desc \" +databaseName+\".\"+tableName;\n",
    "        tableSchema= spark.sql(tablecolum).collect()\n",
    "        tablepath=\"s3://rupiahplus-data-warehouse/aliyun/\"+tablemap[databaseName]+\"_real/\"+row[\"tableName\"]\n",
    "        spark.catalog.refreshTable(databaseName+\".\"+tableName)\n",
    "        sql=\"select \"+ getTableColum(tableSchema)+ tablesql_1+databaseName+\".\"+tableName+tablesql_2\n",
    "    #     print(sql)\n",
    "        spark.sql(sql).drop(\"etlindex\").write.mode(\"overwrite\").orc(tablepath)\n",
    "#     builtins\n",
    "if __name__ == \"builtins\":\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Python Demo\")\\\n",
    "        .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "#     spark.conf.set(\"spark.scheduler.mode\",\"FAIR\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "    for databaseName in tablemap:\n",
    "        print(databaseName)\n",
    "        databasesql=\"show tables in \"+databaseName\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(make_col_temptable,databaseName, tableNm) for tableNm in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按小时的更新(近实时)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c1652c3cc34b5d9a21462665fb5240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"cannot resolve '`bb.etldate`' given input columns: [bb.deposit_channel, bb.out_deposit_no, bb.arrived_amount, bb.create_time, bb.cleared_amount, bb.reconcile_status, bb.deposit_method, bb.id, bb.status, bb.type, bb.deposit_amount, bb.lpay_id, bb.payment_code, bb.customer_id, bb.transaction_time, bb.update_time, bb.subtype]; line 7 pos 86;\\n'Project [*]\\n+- 'Filter ('row_num = 1)\\n   +- 'SubqueryAlias `__auto_generated_subquery_name`\\n      +- 'Project [*, row_number() windowspecdefinition('id, 'date_format('etldate, yyyy-MM-dd HH:mm:ss.SSS) DESC NULLS LAST, 'if(isnull('etlindex), 0, 'etlindex) DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#9806]\\n         +- 'Filter (((cast(concat(concat(concat(concat(year#9829, -), month#9830), -), day#9831) as date) <= cast(2020-07-30 as date)) && (cast(concat(concat(concat(concat(year#9829, -), month#9830), -), day#9831) as date) >= cast(2020-07-29 as date))) && (date_format(cast(etldate#9810 as timestamp), yyyy-MM-dd HH:mm:ss.SSS, Some(UTC)) >= scalar-subquery#9807 []))\\n            :  +- 'Project [unresolvedalias('max('date_format('bb.etldate, yyyy-MM-dd HH:mm:ss.SSS)), None)]\\n            :     +- SubqueryAlias `bb`\\n            :        +- SubqueryAlias `banda_realtime_etl`.`t_lpay_deposit`\\n            :           +- Relation[id#9832L,lpay_id#9833L,customer_id#9834L,status#9835,deposit_amount#9836,arrived_amount#9837,cleared_amount#9838,deposit_channel#9839,out_deposit_no#9840,payment_code#9841,deposit_method#9842,reconcile_status#9843,create_time#9844,update_time#9845,type#9846,subtype#9847,transaction_time#9848] orc\\n            +- SubqueryAlias `banda_stream_etl`.`t_lpay_deposit`\\n               +- Relation[kind#9809,etldate#9810,etlindex#9811,id#9812L,lpay_id#9813L,customer_id#9814L,status#9815,deposit_amount#9816,arrived_amount#9817,cleared_amount#9818,deposit_channel#9819,out_deposit_no#9820,payment_code#9821,deposit_method#9822,reconcile_status#9823,create_time#9824,update_time#9825,type#9826,subtype#9827,transaction_time#9828,year#9829,month#9830,day#9831] orc\\n\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 767, in sql\n",
      "    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: \"cannot resolve '`bb.etldate`' given input columns: [bb.deposit_channel, bb.out_deposit_no, bb.arrived_amount, bb.create_time, bb.cleared_amount, bb.reconcile_status, bb.deposit_method, bb.id, bb.status, bb.type, bb.deposit_amount, bb.lpay_id, bb.payment_code, bb.customer_id, bb.transaction_time, bb.update_time, bb.subtype]; line 7 pos 86;\\n'Project [*]\\n+- 'Filter ('row_num = 1)\\n   +- 'SubqueryAlias `__auto_generated_subquery_name`\\n      +- 'Project [*, row_number() windowspecdefinition('id, 'date_format('etldate, yyyy-MM-dd HH:mm:ss.SSS) DESC NULLS LAST, 'if(isnull('etlindex), 0, 'etlindex) DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS row_num#9806]\\n         +- 'Filter (((cast(concat(concat(concat(concat(year#9829, -), month#9830), -), day#9831) as date) <= cast(2020-07-30 as date)) && (cast(concat(concat(concat(concat(year#9829, -), month#9830), -), day#9831) as date) >= cast(2020-07-29 as date))) && (date_format(cast(etldate#9810 as timestamp), yyyy-MM-dd HH:mm:ss.SSS, Some(UTC)) >= scalar-subquery#9807 []))\\n            :  +- 'Project [unresolvedalias('max('date_format('bb.etldate, yyyy-MM-dd HH:mm:ss.SSS)), None)]\\n            :     +- SubqueryAlias `bb`\\n            :        +- SubqueryAlias `banda_realtime_etl`.`t_lpay_deposit`\\n            :           +- Relation[id#9832L,lpay_id#9833L,customer_id#9834L,status#9835,deposit_amount#9836,arrived_amount#9837,cleared_amount#9838,deposit_channel#9839,out_deposit_no#9840,payment_code#9841,deposit_method#9842,reconcile_status#9843,create_time#9844,update_time#9845,type#9846,subtype#9847,transaction_time#9848] orc\\n            +- SubqueryAlias `banda_stream_etl`.`t_lpay_deposit`\\n               +- Relation[kind#9809,etldate#9810,etlindex#9811,id#9812L,lpay_id#9813L,customer_id#9814L,status#9815,deposit_amount#9816,arrived_amount#9817,cleared_amount#9818,deposit_channel#9819,out_deposit_no#9820,payment_code#9821,deposit_method#9822,reconcile_status#9823,create_time#9824,update_time#9825,type#9826,subtype#9827,transaction_time#9828,year#9829,month#9830,day#9831] orc\\n\"\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102848d501b54365a292f013146a70ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://rupiahplus-data-warehouse/aliyun/banda_real/t_file/\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "import copy\n",
    "dbmap={\n",
    "    \"banda_real\":{\n",
    "        \"increment_database\":\"banda_stream_etl\",\n",
    "        \"database\":\"banda_realtime_etl\",\n",
    "        \"realTime_path\":\"banda_real\"\n",
    "        \n",
    "    },\n",
    "#     \"luzon\":{\n",
    "#          \"increment_database\":\"luzon_stream_etl\",\n",
    "#          \"database\":\"luzon_etl_s3\",\n",
    "#          \"realTime_path\":\"luzon_real\"\n",
    "#     },\n",
    "#     \"telemarket\":{\n",
    "#          \"increment_database\":\"telemarket_stream_etl\",\n",
    "#          \"database\":\"telemarket_etl_s3\",\n",
    "#          \"realTime_path\":\"telemarket\"\n",
    "#     }\n",
    "}\n",
    "# increment_database=\"banda_stream_etl\"\n",
    "# # tableNm=\"t_loan_app\"\n",
    "# database=\"banda-etl-s3\"\n",
    "nowdate=(datetime.now()).strftime('%Y-%m-%d')\n",
    "yesterday=(datetime.now()+ timedelta(-1)).strftime('%Y-%m-%d')\n",
    "sqltemp=\"\"\"with {update_snapshot_table} as (\n",
    "  select *\n",
    "    from (\n",
    "      SELECT * ,\n",
    "         row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "      FROM `{increment_database}`.`{tableNm}`\n",
    "      where date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "      )\n",
    "    where row_num = 1\n",
    "  )\n",
    "select  {select_sql} \n",
    "from `{database}`.`{tableNm}` l\n",
    "left join {update_snapshot_table}\n",
    "on l.id = {update_snapshot_table}.id\n",
    "\n",
    "union\n",
    "select {update_snapshot_sql} from (\n",
    "  select *\n",
    "  from `{increment_database}`.`{tableNm}`\n",
    "  where\n",
    "    date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "    and kind = 'insert'\n",
    "  ) new\n",
    "left join {update_snapshot_table}\n",
    "on new.id = {update_snapshot_table}.id where {update_snapshot_table}.kind<> 'delete' \"\"\"\n",
    "def getTableColum(b,update_snapshot_table):\n",
    "    colum=\" \"\n",
    "    snapshot_colum=\" \"\n",
    "    sqlstrtemp=\"if({update_snapshot_table}.id is null, {colNm}     , {snapshot_colNm})  {ascolNm} ,\"\n",
    "    for index in range(len(b)):\n",
    "        if(index==1 or (index>2 and index<len(b)-7)):   \n",
    "            sqlstr=copy.copy(sqlstrtemp)\n",
    "            colum=colum+sqlstr.format(colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],False),snapshot_colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],True),ascolNm=b[index][\"col_name\"],update_snapshot_table=update_snapshot_table)\n",
    "            snapshot_colum=snapshot_colum+setSnapshotDef(b[index][\"data_type\"],b[index][\"col_name\"]).format(update_snapshot_table=update_snapshot_table)+\" ,\"\n",
    "    return colum[0:len(colum)-2],snapshot_colum[0:len(snapshot_colum)-2]\n",
    "def setDef(type,table_col,is_snapshot):\n",
    "    if(type[:7] == 'decimal'):\n",
    "        if is_snapshot:\n",
    "            return \"ifnull({update_snapshot_table}.\"+table_col+\",0)  \"\n",
    "        else:\n",
    "            return \"ifnull(l.\"+table_col+\",0) \"\n",
    "    else:\n",
    "        if is_snapshot:\n",
    "            return \"{update_snapshot_table}.\"+table_col\n",
    "        else:\n",
    "            return \"l.\"+table_col\n",
    "def setSnapshotDef(type,table_col):\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull({update_snapshot_table}.\"+table_col+\",0) \"+table_col\n",
    "    else:\n",
    "        return \"{update_snapshot_table}.\"+table_col\n",
    "def execute(tablerow,dbtype):\n",
    "#     print(tablerow,dbtype)\n",
    "    tableNm=tablerow[\"tableName\"]\n",
    "    if tableNm=='t_file':\n",
    "        update_snapshot_table=\"update_snapshot_\"+dbtype+\"_\"+tableNm\n",
    "        spark.catalog.refreshTable(\"`\"+dbmap[dbtype][\"database\"]+\"`.\"+tableNm)\n",
    "        col=spark.sql(\"desc `\"+dbmap[dbtype][\"increment_database\"]+\"`.\"+tableNm).collect()\n",
    "        #     #真正的列,增量的列\n",
    "        real_col,snapshot_col=getTableColum(col,update_snapshot_table)\n",
    "        sql=copy.copy(sqltemp)\n",
    "        real_sql=sql.format(increment_database=dbmap[dbtype][\"increment_database\"],tableNm=tableNm,nowdate=nowdate,select_sql=real_col.format(update_snapshot_table=update_snapshot_table),update_snapshot_sql=snapshot_col.format(update_snapshot_table=update_snapshot_table),database=dbmap[dbtype][\"database\"],yesterday=yesterday,update_snapshot_table=update_snapshot_table)\n",
    "        spark.sql(real_sql).drop(\"etlindex\").write.mode(\"overwrite\").partitionBy(\"year\").format(\"orc\").saveAsTable(\"etl_s3_temp.\"+dbtype+\"_\"+tableNm);\n",
    "        #     spark.sql(\" insert overwrite table `\"+dbmap[dbtype][\"database\"]+\"`.`\"+tableNm+\"`  select  * from  \"+\"etl_s3_temp.\"+dbtype+\"_\"+tableNm)\n",
    "        tablepath='s3://rupiahplus-data-warehouse/aliyun/'+dbmap[dbtype][\"realTime_path\"]+'/'+tableNm+\"/\"\n",
    "        spark.sql(\"select  * from  \"+\"etl_s3_temp.\"+dbtype+\"_\"+tableNm).write.mode(\"overwrite\").partitionBy(\"year\").orc(tablepath)\n",
    "if __name__ == \"builtins\":\n",
    "#     builtins\n",
    "    spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Python Demo\")\\\n",
    "    .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "    spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "    spark.conf.set(\"spark.files.overwrite\",\"true\")\n",
    "    spark.conf.set(\"spark.scheduler.mode\",\"FAIR\")\n",
    "    \n",
    "#     ,Apache Arrow：一个跨平台的在内存中以列式存储的数据层，用来加速大数据分析速度。其可以一次性传入更大块的数据，pyspark中已经有载入该模块，需要打开该设置：\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "#     创建表时自动删除已存在的目录\n",
    "    spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "    for dbtype in dbmap:\n",
    "        databasesql=\"show tables in \"+dbmap[dbtype][\"increment_database\"]\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "    #         for row in tablelist:\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(execute, table,dbtype) for table in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06af3eaeae364887abc4009209f4e0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-3:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "import copy\n",
    "dbmap={\n",
    "    \"banda\":{\n",
    "        \"increment_database\":\"banda_stream_etl\",\n",
    "        \"database\":\"banda-etl-s3\",\n",
    "        \"realTime_path\":\"banda\"\n",
    "        \n",
    "    },\n",
    "#     \"luzon\":{\n",
    "#          \"increment_database\":\"luzon_stream_etl\",\n",
    "#          \"database\":\"luzon_etl_s3\",\n",
    "#          \"realTime_path\":\"luzon\"\n",
    "#     },\n",
    "#     \"telemarket\":{\n",
    "#          \"increment_database\":\"telemarket_stream_etl\",\n",
    "#          \"database\":\"telemarket_etl_s3\",\n",
    "#          \"realTime_path\":\"telemarket\"\n",
    "#     }\n",
    "}\n",
    "# increment_database=\"banda_stream_etl\"\n",
    "# # tableNm=\"t_loan_app\"\n",
    "# database=\"banda-etl-s3\"\n",
    "nowdate=(datetime.now()).strftime('%Y-%m-%d')\n",
    "yesterday=(datetime.now()+ timedelta(-1)).strftime('%Y-%m-%d')\n",
    "sqltemp=\"\"\"with {update_snapshot_table} as (\n",
    "  select *\n",
    "    from (\n",
    "      SELECT * ,\n",
    "         row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "      FROM `{increment_database}`.`{tableNm}`\n",
    "      where date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "      )\n",
    "    where row_num = 1\n",
    "  )\n",
    "select  {select_sql} \n",
    "from `{database}`.`{tableNm}` l\n",
    "left join {update_snapshot_table}\n",
    "on l.id = {update_snapshot_table}.id\n",
    "where if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete'\n",
    "union\n",
    "select {update_snapshot_sql} from (\n",
    "  select *\n",
    "  from `{increment_database}`.`{tableNm}`\n",
    "  where\n",
    "    date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "    and kind = 'insert'\n",
    "  ) new\n",
    "left join {update_snapshot_table}\n",
    "on new.id = {update_snapshot_table}.id where  if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete' \"\"\"\n",
    "def getTableColum(b,update_snapshot_table):\n",
    "    colum=\"\"\n",
    "    snapshot_colum=\"\"\n",
    "    sqlstrtemp=\"if({update_snapshot_table}.id is null, {colNm}     , {snapshot_colNm})  {ascolNm} ,\"\n",
    "    for index in range(len(b)):\n",
    "        if(index==1 or (index>2 and index<len(b)-8)):   \n",
    "            sqlstr=copy.copy(sqlstrtemp)\n",
    "            colum=colum+sqlstr.format(colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],False),snapshot_colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],True),ascolNm=b[index][\"col_name\"],update_snapshot_table=update_snapshot_table)\n",
    "            snapshot_colum=snapshot_colum+setSnapshotDef(b[index][\"data_type\"],b[index][\"col_name\"]).format(update_snapshot_table=update_snapshot_table)+\" ,\"\n",
    "    return colum[0:len(colum)-2],snapshot_colum[0:len(snapshot_colum)-2]\n",
    "def setDef(type,table_col,is_snapshot):\n",
    "    if(type[:7] == 'decimal'):\n",
    "        if is_snapshot:\n",
    "            return \"ifnull({update_snapshot_table}.\"+table_col+\",0)  \"\n",
    "        else:\n",
    "            return \"ifnull(l.\"+table_col+\",0) \"\n",
    "    else:\n",
    "        if is_snapshot:\n",
    "            return \"{update_snapshot_table}.\"+table_col\n",
    "        else:\n",
    "            return \"l.\"+table_col\n",
    "def setSnapshotDef(type,table_col):\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull({update_snapshot_table}.\"+table_col+\",0) \"+table_col\n",
    "    else:\n",
    "        return \"{update_snapshot_table}.\"+table_col\n",
    "def execute(tablerow,dbtype):\n",
    "#     print(tablerow,dbtype)\n",
    "    tableNm=tablerow[\"tableName\"]\n",
    "    update_snapshot_table=\"update_snapshot_\"+dbtype+\"_\"+tableNm\n",
    "    spark.catalog.refreshTable(\"`\"+dbmap[dbtype][\"database\"]+\"`.\"+tableNm)\n",
    "    col=spark.sql(\"desc `\"+dbmap[dbtype][\"increment_database\"]+\"`.\"+tableNm).collect()\n",
    "    #     #真正的列,增量的列\n",
    "    real_col,snapshot_col=getTableColum(col,update_snapshot_table)\n",
    "    sql=copy.copy(sqltemp)\n",
    "    real_sql=sql.format(increment_database=dbmap[dbtype][\"increment_database\"],tableNm=tableNm,nowdate=nowdate,select_sql=real_col.format(update_snapshot_table=update_snapshot_table),update_snapshot_sql=snapshot_col.format(update_snapshot_table=update_snapshot_table),database=dbmap[dbtype][\"database\"],yesterday=yesterday,update_snapshot_table=update_snapshot_table)\n",
    "    tablepath=\"s3://rupiahplus-data-warehouse/aliyun/\"+dbmap[dbtype][\"realTime_path\"]+\"/\"+tableNm\n",
    "#         print(real_sql)\n",
    "#         spark.sql(real_sql).drop(\"etlindex\").write.mode(\"overwrite\").orc(tablepath)\n",
    "#         spark.sql(real_sql).where(\"id==1731052\").show()\n",
    "    spark.sql(real_sql).drop(\"etlindex\").write.mode(\"overwrite\").format(\"orc\").saveAsTable(\"etl_s3_temp.\"+dbtype+\"_\"+tableNm);\n",
    "    spark.sql(\"select  * from  \"+\"etl_s3_temp.\"+dbtype+\"_\"+tableNm).write.mode(\"overwrite\").orc(tablepath)\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Python Demo\")\\\n",
    "        .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "    spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "    spark.conf.set(\"spark.files.overwrite\",\"true\")\n",
    "#     ,Apache Arrow：一个跨平台的在内存中以列式存储的数据层，用来加速大数据分析速度。其可以一次性传入更大块的数据，pyspark中已经有载入该模块，需要打开该设置：\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "#     创建表时自动删除已存在的目录\n",
    "    spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "    for dbtype in dbmap:\n",
    "        databasesql=\"show tables in \"+dbmap[dbtype][\"increment_database\"]\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "        print(tableList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1ed5a149f14c6ba562acab92f45963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'RuntimeConfig' object is not iterable\n",
      "Traceback (most recent call last):\n",
      "TypeError: 'RuntimeConfig' object is not iterable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf\n",
    "for s in (spark.conf):\n",
    "    print(s)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
