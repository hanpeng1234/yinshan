{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线程同步增量banda到近实时etl  23min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c8a3a00bca45f68c21b447f3d5ccac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread cell_monitor-23:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.7/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in cell_monitor\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/awseditorssparkmonitoringwidget-1.0-py3.7.egg/awseditorssparkmonitoringwidget/cellmonitor.py\", line 154, in <listcomp>\n",
      "    job_group_filtered_jobs = [job for job in jobs_data if job['jobGroup'] == str(statement_id)]\n",
      "KeyError: 'jobGroup'\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid syntax (<stdin>, line 2)\n",
      "  File \"<stdin>\", line 2\n",
      "    for table in tables if table.isTemporary==True:\n",
      "                                                  ^\n",
      "SyntaxError: invalid syntax\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tables=spark.catalog.listTables()\n",
    "for table in tables :\n",
    "    if table.isTemporary==True:\n",
    "        spark.catalog.dropTempView(table.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "\n",
    "# print(datetime.now(pytz.timezone(\"Asia/Shanghai\")).strftime( '%Y-%m-%d'))\n",
    "today=(datetime.now()+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "yesterday=(datetime.now(pytz.timezone(\"Asia/Shanghai\"))+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "tablemap={\"banda_stream_etl\":\"banda_real\"} #,\n",
    "# ,\"banda_stream_etl\":\"1\"\n",
    "tablesql_1=\" FROM  (   SELECT    *  , row_number() OVER (PARTITION BY id  ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC  , if(etlindex is null ,0,etlindex)  desc)  row_num FROM  \"\n",
    "tablesql_2=\"  ORDER BY id  ASC )  a  WHERE a.row_num = 1 AND a.kind <> 'delete' \"\n",
    "# def getTableColum(b):\n",
    "#     colum=\"\";\n",
    "#     for index in range(len(b)):\n",
    "#         if(index>0 and index<len(b)-8):   \n",
    "#             colum=colum+b[index][\"col_name\"]+\", \"\n",
    "#     return colum+\"year,month,day\"\n",
    "def getTableColum(b):\n",
    "    colum=\"\"\n",
    "    for index in range(len(b)):\n",
    "        if(index>0 and index<len(b)-8):   \n",
    "            colum=colum+setDef(b[index][\"data_type\"],b[index][\"col_name\"])+\", \"\n",
    "    return colum+\"year,month,day \"\n",
    "def setDef(type,table_col):\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull(\"+table_col+\",0) as \"+table_col\n",
    "    else:\n",
    "        return table_col\n",
    "#     带分区！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
    "def make_col_temptable(databaseName,row):\n",
    "    tableName=row[\"tableName\"]\n",
    "    tablecolum=\"desc \" +databaseName+\".\"+tableName;\n",
    "    tableSchema= spark.sql(tablecolum).collect()\n",
    "    tablepath=\"s3://rupiahplus-data-warehouse/aliyun/\"+tablemap[databaseName]+\"/\"+row[\"tableName\"]\n",
    "    spark.catalog.refreshTable(databaseName+\".\"+tableName)\n",
    "    sql=\"select \"+ getTableColum(tableSchema)+ tablesql_1+databaseName+\".\"+tableName+tablesql_2\n",
    "#     print(sql)\n",
    "#  带分区！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
    "    spark.sql(sql).drop(\"etlindex\").write.mode(\"overwrite\").partitionBy(\"year\",\"month\",\"day\").orc(tablepath)\n",
    "#      带分区！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Python Demo\")\\\n",
    "        .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "#     spark.conf.set(\"spark.scheduler.mode\",\"FAIR\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "    for databaseName in tablemap:\n",
    "        print(databaseName)\n",
    "        databasesql=\"show tables in \"+databaseName\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futur es.wait([executor.submit(make_col_temptable,databaseName, tableNm) for tableNm in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())\n",
    "           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同步增量带分区"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "banda全量导表到增量表带分区的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线程同步全量banda到近实时etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f773e1ed7844029ee3d61c6a78105b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1600940220938_0005</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-3-207.ap-southeast-1.compute.internal:20888/proxy/application_1600940220938_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-3-51.ap-southeast-1.compute.internal:8042/node/containerlogs/container_1600940220938_0005_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banda_stream_etl\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "\n",
    "# print(datetime.now(pytz.timezone(\"Asia/Shanghai\")).strftime( '%Y-%m-%d'))\n",
    "today=(datetime.now()+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "yesterday=(datetime.now(pytz.timezone(\"Asia/Shanghai\"))+ timedelta(-1)).strftime( '%Y-%m-%d')\n",
    "tablemap={\"banda_stream_etl\":\"banda\"} #,\n",
    "# ,\"banda_stream_etl\":\"1\"\n",
    "tablesql_1=\" FROM  (   SELECT    *  , row_number() OVER (PARTITION BY id  ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC  , if(etlindex is null ,0,etlindex)  desc)  row_num FROM  \"\n",
    "tablesql_2=\" ORDER BY id  ASC )  a  WHERE a.row_num = 1 AND a.kind <> 'delete' \"\n",
    "# def getTableColum(b):\n",
    "#     colum=\"\";\n",
    "#     for index in range(len(b)):\n",
    "#         if(index>0 and index<len(b)-8):   \n",
    "#             colum=colum+b[index][\"col_name\"]+\", \"\n",
    "#     return colum+\"year,month,day\"\n",
    "def getTableColum(b):\n",
    "    colum=\"\"\n",
    "    for index in range(len(b)):\n",
    "        if(index>0 and index<len(b)-7):   \n",
    "            colum=colum+setDef(b[index][\"data_type\"],b[index][\"col_name\"])+\", \"\n",
    "    return colum[0:len(colum)-2]\n",
    "def setDef(type,table_col):\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull(\"+table_col+\",0) as \"+table_col\n",
    "    else:\n",
    "        return table_col\n",
    "def make_col_temptable(databaseName,row):\n",
    "    tableName=row[\"tableName\"]\n",
    "    if tableName=='t_af_bill':\n",
    "        tablecolum=\"desc \" +databaseName+\".\"+tableName;\n",
    "        tableSchema= spark.sql(tablecolum).collect()\n",
    "        tablepath=\"s3://rupiahplus-data-warehouse/aliyun/\"+tablemap[databaseName]+\"/\"+row[\"tableName\"]\n",
    "        spark.catalog.refreshTable(databaseName+\".\"+tableName)\n",
    "        sql=\"select \"+ getTableColum(tableSchema)+ tablesql_1+databaseName+\".\"+tableName+tablesql_2\n",
    "    #     print(sql)\n",
    "        spark.sql(sql).drop(\"etlindex\").write.mode(\"overwrite\").partitionBy(\"year\").orc(tablepath)\n",
    "#     builtins\n",
    "if __name__ == \"builtins\":\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Python Demo\")\\\n",
    "        .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "#     spark.conf.set(\"spark.scheduler.mode\",\"FAIR\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "    for databaseName in tablemap:\n",
    "        print(databaseName)\n",
    "        databasesql=\"show tables in \"+databaseName\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(make_col_temptable,databaseName, tableNm) for tableNm in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7db11987c44f36bc7b535632ffdb7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n",
      "|database|tableName |isTemporary|\n",
      "+--------+----------+-----------+\n",
      "|default |call_log  |false      |\n",
      "|default |sampledb  |false      |\n",
      "|default |t_customer|false      |\n",
      "+--------+----------+-----------+"
     ]
    }
   ],
   "source": [
    "# from concurrent import futures\n",
    "# def make_col_temptable(a):\n",
    "#     print(a) \n",
    "# for b in [1,2,3,4,5,6,7,8]:\n",
    "#     with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "# #         futures.wait([executor.submit(make_col_temptable,a) for a in [1,2,3,4,6]]) \n",
    "# a=[[2,3],[1,4],[2,4,5],[1,3,5,7]]\n",
    "# for b in a:for c in b:\n",
    "#     print c\n",
    "spark.sql(\"\"\"show tables\"\"\").show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.scheduler.mode\",\"FAIR\")\n",
    "for a in spark.sparkContext.getConf().getAll():\n",
    "#     futures.wait([executor.submit(make_col_temptable,databaseName, tableNm) for tableNm in tablelist])\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368115419a414373aa45fd58e6f26b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-02 23:59:59\n",
      "2020-06-01 07:00:00"
     ]
    }
   ],
   "source": [
    "beforeyesterday=(datetime.now()+ timedelta(-1)).strftime('%Y-%m-%d')+\" \"+(datetime.now()-timedelta(minutes=60)).strftime('%H:00:00')\n",
    "yesterday=(datetime.now()).strftime('%Y-%m-%d')\n",
    "partitionlist=yesterday.split(\"-\",3)\n",
    "yesterday=yesterday+ \" 23:59:59\"\n",
    "print(yesterday)\n",
    "print(beforeyesterday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67efee1fd17d4f62b367faeff3465c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n",
      "|database| tableName|isTemporary|\n",
      "+--------+----------+-----------+\n",
      "| default|  call_log|      false|\n",
      "| default|  sampledb|      false|\n",
      "| default|t_customer|      false|\n",
      "+--------+----------+-----------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"show tables\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照小时更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "import copy\n",
    "dbmap={\n",
    "    \"banda\":{\n",
    "        \"increment_database\":\"banda_stream_etl\",\n",
    "        \"database\":\"banda-etl-s3\",\n",
    "        \"realTime_path\":\"banda\"\n",
    "        \n",
    "    },\n",
    "#     \"luzon\":{\n",
    "#          \"increment_database\":\"luzon_stream_etl\",\n",
    "#          \"database\":\"luzon_etl_s3\",\n",
    "#          \"realTime_path\":\"luzon\"\n",
    "#     },\n",
    "#     \"telemarket\":{\n",
    "#          \"increment_database\":\"telemarket_stream_etl\",\n",
    "#          \"database\":\"telemarket_etl_s3\",\n",
    "#          \"realTime_path\":\"telemarket\"\n",
    "#     }\n",
    "}\n",
    "# increment_database=\"banda_stream_etl\"\n",
    "# # tableNm=\"t_loan_app\"\n",
    "# database=\"banda-etl-s3\"\n",
    "nowdate=(datetime.now()).strftime('%Y-%m-%d')\n",
    "yesterday=(datetime.now()+ timedelta(-1)).strftime('%Y-%m-%d')\n",
    "sqltemp=\"\"\"with {update_snapshot_table} as (\n",
    "  select *\n",
    "    from (\n",
    "      SELECT * ,\n",
    "         row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "      FROM `{increment_database}`.`{tableNm}`\n",
    "      where date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "      )\n",
    "    where row_num = 1\n",
    "  )\n",
    "select  {select_sql} \n",
    "from `{database}`.`{tableNm}` l\n",
    "left join {update_snapshot_table}\n",
    "on l.id = {update_snapshot_table}.id\n",
    "where if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete'\n",
    "union\n",
    "select {update_snapshot_sql} from (\n",
    "  select *\n",
    "  from `{increment_database}`.`{tableNm}`\n",
    "  where\n",
    "    date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "    and kind = 'insert'\n",
    "  ) new\n",
    "left join {update_snapshot_table}\n",
    "on new.id = {update_snapshot_table}.id where if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete' \"\"\"\n",
    "def getTableColum(b,update_snapshot_table):\n",
    "    colum=\"\"\n",
    "    snapshot_colum=\"\"\n",
    "    sqlstrtemp=\"if({update_snapshot_table}.id is null, {colNm}     , {snapshot_colNm})  {ascolNm} ,\"\n",
    "    for index in range(len(b)):\n",
    "        if(index==1 or (index>2 and index<len(b)-7)):   \n",
    "            sqlstr=copy.copy(sqlstrtemp)\n",
    "            colum=colum+sqlstr.format(colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],False),snapshot_colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],True),ascolNm=b[index][\"col_name\"],update_snapshot_table=update_snapshot_table)\n",
    "            snapshot_colum=snapshot_colum+setSnapshotDef(b[index][\"data_type\"],b[index][\"col_name\"]).format(update_snapshot_table=update_snapshot_table)+\" ,\"\n",
    "    return colum[0:len(colum)-2],snapshot_colum[0:len(snapshot_colum)-2]\n",
    "def setDef(type,table_col,is_snapshot):\n",
    "    if(type[:7] == 'decimal'):\n",
    "        if is_snapshot:\n",
    "            return \"ifnull({update_snapshot_table}.\"+table_col+\",0)  \"\n",
    "        else:\n",
    "            return \"ifnull(l.\"+table_col+\",0) \"\n",
    "    else:\n",
    "        if is_snapshot:\n",
    "            return \"{update_snapshot_table}.\"+table_col\n",
    "        else:\n",
    "            return \"l.\"+table_col\n",
    "def setSnapshotDef(type,table_col):\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull({update_snapshot_table}.\"+table_col+\",0) \"+table_col\n",
    "    else:\n",
    "        return \"{update_snapshot_table}.\"+table_col\n",
    "def execute(tablerow,dbtype):\n",
    "#     print(tablerow,dbtype)\n",
    "    tableNm=tablerow[\"tableName\"]\n",
    "    update_snapshot_table=\"update_snapshot_\"+dbtype+\"_\"+tableNm\n",
    "    spark.catalog.refreshTable(\"`\"+dbmap[dbtype][\"database\"]+\"`.\"+tableNm)\n",
    "    col=spark.sql(\"desc `\"+dbmap[dbtype][\"increment_database\"]+\"`.\"+tableNm).collect()\n",
    "    #     #真正的列,增量的列\n",
    "    real_col,snapshot_col=getTableColum(col,update_snapshot_table)\n",
    "    sql=copy.copy(sqltemp)\n",
    "    real_sql=sql.format(increment_database=dbmap[dbtype][\"increment_database\"],tableNm=tableNm,nowdate=nowdate,select_sql=real_col.format(update_snapshot_table=update_snapshot_table),update_snapshot_sql=snapshot_col.format(update_snapshot_table=update_snapshot_table),database=dbmap[dbtype][\"database\"],yesterday=yesterday,update_snapshot_table=update_snapshot_table)\n",
    "    tablepath=\"s3://rupiahplus-data-warehouse/aliyun/\"+dbmap[dbtype][\"realTime_path\"]+\"/\"+tableNm\n",
    "    spark.sql(real_sql).drop(\"etlindex\").write.mode(\"overwrite\").partitionBy(\"year\").format(\"orc\").saveAsTable(\"etl_s3_temp.\"+dbtype+\"_\"+tableNm);\n",
    "    tablepath='s3://rupiahplus-data-warehouse/aliyun/'+dbmap[dbtype][\"realTime_path\"]+'/'+tableNm+\"/\"\n",
    "    spark.sql(\"select  * from  \"+\"etl_s3_temp.\"+dbtype+\"_\"+tableNm).write.mode(\"overwrite\").partitionBy(\"year\").orc(tablepath)\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Python Demo\")\\\n",
    "        .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "    spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\",20)\n",
    "    spark.conf.set(\"spark.files.overwrite\",\"true\")\n",
    "#     ,Apache Arrow：一个跨平台的在内存中以列式存储的数据层，用来加速大数据分析速度。其可以一次性传入更大块的数据，pyspark中已经有载入该模块，需要打开该设置：\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "#     创建表时自动删除已存在的目录\n",
    "    spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "    for dbtype in dbmap:\n",
    "        databasesql=\"show tables in \"+dbmap[dbtype][\"increment_database\"]\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "    #         for row in tablelist:\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(execute, table,dbtype) for table in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  $$$$$$$$$\n",
    "\n",
    "\n",
    ">带加密列的每小时更新\n",
    ">  $$$$$$$$$$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427e6b9d4d1c4e7b8029471fa49169a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start----- 2021-05-13 07:58:58.142540\n",
      "with update_snapshot_banda_t_engine_rule_detail as (\n",
      "  select *\n",
      "    from (\n",
      "      SELECT * ,\n",
      "         row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
      "         FROM `banda_stream_etl`.`t_engine_rule_detail`\n",
      "         where date(year || '-' || month || '-' || day) <=date('2021-05-13') and date(year || '-' || month || '-' || day) >= date('2021-05-12')\n",
      "      )\n",
      "    where row_num = 1\n",
      "  )\n",
      "select  if(update_snapshot_banda_t_engine_rule_detail.id is null, l.etldate     , update_snapshot_banda_t_engine_rule_detail.etldate)  etldate ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.id     , update_snapshot_banda_t_engine_rule_detail.id)  id ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.loan_id     , update_snapshot_banda_t_engine_rule_detail.loan_id)  loan_id ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.code     , update_snapshot_banda_t_engine_rule_detail.code)  code ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.value     , update_snapshot_banda_t_engine_rule_detail.value)  value ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.result     , update_snapshot_banda_t_engine_rule_detail.result)  result ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.rule_name     , update_snapshot_banda_t_engine_rule_detail.rule_name)  rule_name ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.description     , update_snapshot_banda_t_engine_rule_detail.description)  description ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.auto_review_loan_id     , update_snapshot_banda_t_engine_rule_detail.auto_review_loan_id)  auto_review_loan_id ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.create_time     , update_snapshot_banda_t_engine_rule_detail.create_time)  create_time ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.year     , update_snapshot_banda_t_engine_rule_detail.year)  year \n",
      "from `banda-etl-s3`.`t_engine_rule_detail` l\n",
      "left join update_snapshot_banda_t_engine_rule_detail\n",
      "on l.id = update_snapshot_banda_t_engine_rule_detail.id\n",
      "where if(update_snapshot_banda_t_engine_rule_detail.kind is not null,update_snapshot_banda_t_engine_rule_detail.kind,'') <> 'delete'\n",
      "union\n",
      "select update_snapshot_banda_t_engine_rule_detail.etldate ,update_snapshot_banda_t_engine_rule_detail.id ,update_snapshot_banda_t_engine_rule_detail.loan_id ,update_snapshot_banda_t_engine_rule_detail.code ,update_snapshot_banda_t_engine_rule_detail.value ,update_snapshot_banda_t_engine_rule_detail.result ,update_snapshot_banda_t_engine_rule_detail.rule_name ,update_snapshot_banda_t_engine_rule_detail.description ,update_snapshot_banda_t_engine_rule_detail.auto_review_loan_id ,update_snapshot_banda_t_engine_rule_detail.create_time ,update_snapshot_banda_t_engine_rule_detail.year from (\n",
      "  select *\n",
      "  from `banda_stream_etl`.`t_engine_rule_detail`\n",
      "  where\n",
      "    date(year || '-' || month || '-' || day) <=date('2021-05-13') and date(year || '-' || month || '-' || day) >= date('2021-05-12')\n",
      "    and kind = 'insert'\n",
      "  ) new\n",
      "left join update_snapshot_banda_t_engine_rule_detail\n",
      "on new.id = update_snapshot_banda_t_engine_rule_detail.id where if(update_snapshot_banda_t_engine_rule_detail.kind is not null,update_snapshot_banda_t_engine_rule_detail.kind,'') <> 'delete' \n",
      "None\n",
      "end----- 2021-05-13 07:59:02.987684\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.987726\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.987748\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.987768\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.987787\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.987807\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.987826\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.987844\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.987862\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.987882\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.987901\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.987923\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.987943\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.987962\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.987983\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988003\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988026\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988047\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988065\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988083\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988102\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988133\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988153\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988173\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988193\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988213\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988232\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988250\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988269\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988287\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988305\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988323\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988340\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988359\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988377\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988397\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988419\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988437\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988457\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988475\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988493\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988511\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988529\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988547\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988566\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988584\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988602\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988621\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988639\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988658\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988675\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988694\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988712\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988730\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988748\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988765\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988783\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988800\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988819\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988837\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988855\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988873\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988892\n",
      "None\n",
      "end----- 2021-05-13 07:59:02.988911"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from concurrent import futures\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime,timedelta\n",
    "import copy\n",
    "import json\n",
    "import pytz\n",
    "import hmac\n",
    "from hashlib import sha256 \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import types as T\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Python Demo\")\\\n",
    "    .config(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\",\"nonstrict\");\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",100)\n",
    "spark.conf.set(\"spark.files.overwrite\",\"true\")\n",
    "#     ,Apache Arrow：一个跨平台的在内存中以列式存储的数据层，用来加速大数据分析速度。其可以一次性传入更大块的数据，pyspark中已经有载入该模块，需要打开该设置：\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "#     创建表时自动删除已存在的目录\n",
    "# spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation\",\"true\")\n",
    "dbmap={\n",
    "    \"banda\":{\n",
    "        \"increment_database\":\"banda_stream_etl\",\n",
    "        \"database\":\"banda-etl-s3\",\n",
    "        \"realTime_path\":\"banda\"\n",
    "    }\n",
    "}\n",
    "nowdate=(datetime.now()).strftime('%Y-%m-%d')\n",
    "yesterday=(datetime.now()+ timedelta(-1)).strftime('%Y-%m-%d')\n",
    "sqltemp=\"\"\"with {update_snapshot_table} as (\n",
    "  select *\n",
    "    from (\n",
    "      SELECT * ,\n",
    "         row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "         FROM `{increment_database}`.`{tableNm}`\n",
    "         where date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "      )\n",
    "    where row_num = 1\n",
    "  )\n",
    "select  {select_sql} \n",
    "from `{database}`.`{tableNm}` l\n",
    "left join {update_snapshot_table}\n",
    "on l.id = {update_snapshot_table}.id\n",
    "where if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete'\n",
    "union\n",
    "select {update_snapshot_sql} from (\n",
    "  select *\n",
    "  from `{increment_database}`.`{tableNm}`\n",
    "  where\n",
    "    date(year || '-' || month || '-' || day) <=date('{nowdate}') and date(year || '-' || month || '-' || day) >= date('{yesterday}')\n",
    "    and kind = 'insert'\n",
    "  ) new\n",
    "left join {update_snapshot_table}\n",
    "on new.id = {update_snapshot_table}.id where if({update_snapshot_table}.kind is not null,{update_snapshot_table}.kind,'') <> 'delete' \"\"\"\n",
    "def get_secret_obj():\n",
    "    df=spark.read.text(\"s3://rupiahplus-configs/etl/data_secrt/col.json\").collect()\n",
    "    keymap=''\n",
    "    for  row in df:\n",
    "        keymap=keymap+row['value']\n",
    "    json_content=json.loads(keymap)\n",
    "    return json_content\n",
    "colmap= get_secret_obj()   \n",
    "hmac_key=colmap[\"hmac_key\"]\n",
    "mobileType=colmap[\"mobileType\"]\n",
    "def getRelmobile(colNm,str):\n",
    "    if colNm in mobileType:\n",
    "        relmobile = str.strip().replace(\"+\", \"\").replace(\"-\", \"\").replace(\" \", \"\").replace('\"', '')\n",
    "        if (relmobile.startswith(\"62\")):\n",
    "            relmobile = relmobile.replace(\"62\", \"0\",1)\n",
    "        if relmobile.startswith(\"0\") == False:\n",
    "            relmobile = \"0\" + relmobile;\n",
    "        return relmobile\n",
    "    return str\n",
    "# hmac_256加密\n",
    "def hmac_sha256(key,colNm,value):\n",
    "    if value!=None and value!='':\n",
    "#         处理手机号\n",
    "        rtn=getRelmobile(colNm,value)\n",
    "        h = hmac.new(key.encode('utf-8'),digestmod=sha256)\n",
    "        h.update(rtn.encode('utf-8'))\n",
    "        h_str = h.hexdigest()\n",
    "        return h_str\n",
    "spark.udf.register(\"hmac_sha256\",hmac_sha256,T.StringType())\n",
    "def getTableColum(b,update_snapshot_table,dbtype,tableNm):\n",
    "    colum=\"\"\n",
    "    snapshot_colum=\"\"\n",
    "    sqlstrtemp=\"if({update_snapshot_table}.id is null, {colNm}     , {snapshot_colNm})  {ascolNm} ,\"\n",
    "    sqlstr_secret_temp=\" if({update_snapshot_table}.id is null, {colNm}     , hmac_sha256('{hmac_key}','{ascolNm}',{snapshot_colNm})) {ascolNm}_x , \"\n",
    "    for index in range(len(b)):\n",
    "        if(index==1 or (index>2 and index<len(b)-7)):\n",
    "#             print(index,b[index])\n",
    "#             在year前面加上加密列\n",
    "            if(index==len(b)-8) and colmap[dbtype]!=None and colmap[dbtype].get(tableNm)!=None:\n",
    "                for secret_col in colmap[dbtype][tableNm]:\n",
    "                    sqlstr=copy.copy(sqlstr_secret_temp)\n",
    "                    colum=colum+sqlstr.format(colNm=setDef(\"string\",secret_col,False),snapshot_colNm=setDef(\"string\",secret_col,True),ascolNm=secret_col,update_snapshot_table=update_snapshot_table,hmac_key=hmac_key)\n",
    "                    snapshot_colum=snapshot_colum+setsecret_SnapshotDef(secret_col).format(update_snapshot_table=update_snapshot_table,hmac_key=hmac_key)+\" ,\"\n",
    "            sqlstr=copy.copy(sqlstrtemp)\n",
    "            colum=colum+sqlstr.format(colNm=isX(dbtype,tableNm,setDef(b[index][\"data_type\"],b[index][\"col_name\"],False)),snapshot_colNm=setDef(b[index][\"data_type\"],b[index][\"col_name\"],True),ascolNm=b[index][\"col_name\"],update_snapshot_table=update_snapshot_table)\n",
    "            snapshot_colum=snapshot_colum+setSnapshotDef(b[index][\"data_type\"],b[index][\"col_name\"]).format(update_snapshot_table=update_snapshot_table)+\" ,\"\n",
    "    return colum[0:len(colum)-2],snapshot_colum[0:len(snapshot_colum)-2]\n",
    "def isX(type,table,column):\n",
    "    columnList=column.split(\".\")\n",
    "    if len(columnList)>0 and colmap[type].get(table)!=None and columnList[len(columnList)-1] in colmap[type][table]:\n",
    "        return column+\"_x\"\n",
    "    return column\n",
    "def setDef(type,table_col,is_snapshot):\n",
    "    table_col_x=copy.copy(table_col)\n",
    "    if(type[:7] == 'decimal'):\n",
    "        if is_snapshot:\n",
    "            return \"ifnull({update_snapshot_table}.\"+table_col_x+\",0)  \"\n",
    "        else:\n",
    "            return \"ifnull(l.\"+table_col_x+\",0) \"\n",
    "    else:\n",
    "        if is_snapshot:\n",
    "            return \"{update_snapshot_table}.\"+table_col_x\n",
    "        else:\n",
    "            return \"l.\"+table_col_x\n",
    "def setSnapshotDef(type,table_col):\n",
    "    table_col_x=copy.copy(table_col)\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull({update_snapshot_table}.\"+table_col_x+\",0) \"+table_col\n",
    "    else:\n",
    "        return \"{update_snapshot_table}.\"+table_col_x\n",
    "def setsecret_SnapshotDef(table_col):\n",
    "    table_col_x=copy.copy(table_col)\n",
    "#     print(\"hmac_sha256('{hmac_key}','\"+table_col_x+\"', {update_snapshot_table}.\"+table_col_x+\") \"+ table_col_x+\"_x\")\n",
    "    return \"hmac_sha256('{hmac_key}','\"+table_col_x+\"', {update_snapshot_table}.\"+table_col_x+\") \"+ table_col_x+\"_x\"\n",
    "#     return \"sha2({update_snapshot_table}.\"+table_col+\",256) \" +table_col+\"_x\"\n",
    "def execute(tablerow,dbtype):\n",
    "#     print(tablerow,dbtype)\n",
    "    tableName=tablerow[\"tableName\"]\n",
    "    if tableName=='t_engine_rule_detail':\n",
    "        update_snapshot_table=\"update_snapshot_\"+dbtype+\"_\"+tableName\n",
    "        spark.catalog.refreshTable(\"`\"+dbmap[dbtype][\"database\"]+\"`.\"+tableName)\n",
    "        col=spark.sql(\"desc `\"+dbmap[dbtype][\"increment_database\"]+\"`.\"+tableName).collect()\n",
    "        #     #真正的列,增量的列\n",
    "        real_col,snapshot_col=getTableColum(col,update_snapshot_table,dbtype,tableName)\n",
    "        sql=copy.copy(sqltemp)\n",
    "        real_sql=sql.format(increment_database=dbmap[dbtype][\"increment_database\"],tableNm=tableName,nowdate=nowdate,select_sql=real_col.format(update_snapshot_table=update_snapshot_table),update_snapshot_sql=snapshot_col.format(update_snapshot_table=update_snapshot_table),database=dbmap[dbtype][\"database\"],yesterday=yesterday,update_snapshot_table=update_snapshot_table,hmac_key=hmac_key)\n",
    "        print(real_sql)\n",
    "    #     df=spark.sql(real_sql).drop(\"etlindex\")\n",
    "#     if colmap[dbtype].get(tableName)!=None :\n",
    "#         for i in colmap[dbtype][tableName]:\n",
    "#             df=df.withColumn(\"col_x\",F.col(i+\"_x\")).withColumn(i+\"_x\",F.col(i)).withColumn(i,F.col(\"col_x\"))\n",
    "#     if colmap[\"extraColumn\"]!=None and colmap[\"extraColumn\"].get(dbtype)!=None and colmap[\"extraColumn\"][dbtype].get(tableName)!=None and len(colmap[\"extraColumn\"][dbtype][tableName])>0:\n",
    "#         colmap[\"extraColumn\"][dbtype][tableName].insert(0,\"*\")\n",
    "#         df=df.selectExpr(colmap[\"extraColumn\"][dbtype][tableName])\n",
    "#         df=df.withColumn(\"year_new\",F.col(\"year\")).drop(\"year\").withColumnRenamed(\"year_new\",\"year\")\n",
    "#     df.drop(\"col_x\").write.option(\"path\", \"s3://rupiahplus-data-warehouse/stream/etl_s3_temp/\"+dbtype+\"_\"+tableName).mode(\"overwrite\").partitionBy(\"year\").format(\"orc\").saveAsTable(\"etl_s3_temp.\"+dbtype+\"_\"+tableName);\n",
    "#     tablepath='s3://rupiahplus-data-warehouse/aliyun/'+dbmap[dbtype][\"realTime_path\"]+'/'+tableName+\"/\"\n",
    "#     spark.sql(\"select  * from  \"+\"etl_s3_temp.\"+dbtype+\"_\"+tableName).write.mode(\"overwrite\").partitionBy(\"year\").orc(tablepath)\n",
    "if __name__ == \"builtins\":\n",
    "    print(\"start-----\",datetime.now())\n",
    "    for dbtype in dbmap:\n",
    "        databasesql=\"show tables in \"+dbmap[dbtype][\"increment_database\"]\n",
    "        tables=spark.sql(databasesql)\n",
    "        tablelist=tables.collect();\n",
    "    #         for row in tablelist:\n",
    "        executor=None\n",
    "        with futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures_result=futures.wait([executor.submit(execute, table,dbtype) for table in tablelist])\n",
    "            for  future in futures_result[0]:\n",
    "                print(future.result())\n",
    "                print(\"end-----\",datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e658e903bfb74a5e9887fa2449afe67c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| count(1)|\n",
      "+---------+\n",
      "|168935759|\n",
      "+---------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "with update_snapshot_banda_t_engine_rule_detail as (\n",
    "  select *\n",
    "    from (\n",
    "      SELECT * ,\n",
    "         row_number() OVER (PARTITION BY id ORDER BY date_format(etldate,'yyyy-MM-dd HH:mm:ss.SSS') DESC, if(etlindex is NULL, 0, etlindex) desc) row_num\n",
    "         FROM `banda_stream_etl`.`t_engine_rule_detail`\n",
    "         where date(year || '-' || month || '-' || day) <=date('2021-05-13') and date(year || '-' || month || '-' || day) >= date('2021-05-12')\n",
    "      )\n",
    "    where row_num = 1\n",
    "  )\n",
    "select count(1) from (select  if(update_snapshot_banda_t_engine_rule_detail.id is null, l.etldate     , update_snapshot_banda_t_engine_rule_detail.etldate)  etldate ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.id     , update_snapshot_banda_t_engine_rule_detail.id)  id ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.loan_id     , update_snapshot_banda_t_engine_rule_detail.loan_id)  loan_id ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.code     , update_snapshot_banda_t_engine_rule_detail.code)  code ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.value     , update_snapshot_banda_t_engine_rule_detail.value)  value ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.result     , update_snapshot_banda_t_engine_rule_detail.result)  result ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.rule_name     , update_snapshot_banda_t_engine_rule_detail.rule_name)  rule_name ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.description     , update_snapshot_banda_t_engine_rule_detail.description)  description ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.auto_review_loan_id     , update_snapshot_banda_t_engine_rule_detail.auto_review_loan_id)  auto_review_loan_id ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.create_time     , update_snapshot_banda_t_engine_rule_detail.create_time)  create_time ,if(update_snapshot_banda_t_engine_rule_detail.id is null, l.year     , update_snapshot_banda_t_engine_rule_detail.year)  year \n",
    "from `banda-etl-s3`.`t_engine_rule_detail` l\n",
    "left join update_snapshot_banda_t_engine_rule_detail\n",
    "on l.id = update_snapshot_banda_t_engine_rule_detail.id\n",
    "where if(update_snapshot_banda_t_engine_rule_detail.kind is not null,update_snapshot_banda_t_engine_rule_detail.kind,'') <> 'delete'\n",
    "union\n",
    "select update_snapshot_banda_t_engine_rule_detail.etldate ,update_snapshot_banda_t_engine_rule_detail.id ,update_snapshot_banda_t_engine_rule_detail.loan_id ,update_snapshot_banda_t_engine_rule_detail.code ,update_snapshot_banda_t_engine_rule_detail.value ,update_snapshot_banda_t_engine_rule_detail.result ,update_snapshot_banda_t_engine_rule_detail.rule_name ,update_snapshot_banda_t_engine_rule_detail.description ,update_snapshot_banda_t_engine_rule_detail.auto_review_loan_id ,update_snapshot_banda_t_engine_rule_detail.create_time ,update_snapshot_banda_t_engine_rule_detail.year from (\n",
    "  select *\n",
    "  from `banda_stream_etl`.`t_engine_rule_detail`\n",
    "  where\n",
    "    date(year || '-' || month || '-' || day) <=date('2021-05-13') and date(year || '-' || month || '-' || day) >= date('2021-05-12')\n",
    "    and kind = 'insert'\n",
    "  ) new\n",
    "left join update_snapshot_banda_t_engine_rule_detail\n",
    "on new.id = update_snapshot_banda_t_engine_rule_detail.id where if(update_snapshot_banda_t_engine_rule_detail.kind is not null,update_snapshot_banda_t_engine_rule_detail.kind,'') <> 'delete' ) as tmp\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
