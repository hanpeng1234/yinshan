{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e0a38896b44a15978ee0657a6f117c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 39, in hmac_sha256\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/column.py\", line 690, in __nonzero__\n",
      "    raise ValueError(\"Cannot convert column into bool: please use '&' for 'and', '|' for 'or', \"\n",
      "ValueError: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import pyspark.sql.types as T\n",
    "import pytz\n",
    "import hashlib\n",
    "import pytz\n",
    "import re\n",
    "import hmac\n",
    "import json\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timedelta\n",
    "from hashlib import sha256 \n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.column import Column\n",
    "# 替换手机号\n",
    "def get_secret_obj():\n",
    "    df=spark.read.text(\"s3://rupiahplus-configs/etl/data_secrt/col.json\").collect()\n",
    "    keymap=''\n",
    "    for  row in df:\n",
    "        keymap=keymap+row['value']\n",
    "    json_content=json.loads(keymap)\n",
    "    return json_content\n",
    "colmap= get_secret_obj()\n",
    "mobileType=colmap[\"mobileType\"]\n",
    "def getRelmobile(colNm,str):\n",
    "    if colNm in mobileType:\n",
    "        relmobile = str.strip().replace(\"+\", \"\").replace(\"-\", \"\").replace(\" \", \"\").replace('\"', '')\n",
    "        if (relmobile.startswith(\"62\")):\n",
    "            relmobile = relmobile.replace(\"62\", \"0\",1)\n",
    "        if relmobile.startswith(\"0\") == False:\n",
    "            relmobile = \"0\" + relmobile;\n",
    "        return relmobile\n",
    "    return str\n",
    "# hmac_256加密\n",
    "def hmac_sha256(key,colNm,value):\n",
    "    if value!=None and value!='':\n",
    "        rtn=getRelmobile(colNm,value)\n",
    "        h = hmac.new(key.encode('utf-8'),digestmod=sha256)\n",
    "        h.update(rtn.encode('utf-8'))\n",
    "        h_str = h.hexdigest()\n",
    "        return h_str\n",
    "    return \"\"\n",
    "spark.udf.register(\"hmac_sha256\",hmac_sha256,T.StringType())\n",
    "def getTableColum(b):\n",
    "    colum=\"\"\n",
    "    for index in range(len(b)):\n",
    "        if(index>0 and index<len(b)-8):   \n",
    "            colum=colum+setDef(b[index][\"data_type\"],b[index][\"col_name\"])+\", \"\n",
    "    return colum[0:len(colum)-2]\n",
    "def setDef(type,table_col):\n",
    "    if(type[:7] == 'decimal'):\n",
    "        return \"ifnull(\"+table_col+\",0) as \"+table_col\n",
    "    else:\n",
    "        return table_col\n",
    "def add_secret_col(dbType,tableNm,col):\n",
    "    secret_Col=[]\n",
    "    if colmap.get(dbType)!=None and colmap[dbType]!=None and colmap[dbType].get(tableNm)!=None:\n",
    "        secret_Col=colmap[tablemap[databaseName]][tableNm]\n",
    "    sh256Col =col\n",
    "    hmac_key=colmap[\"hmac_key\"]\n",
    "#     sql=\"select hmac_sha256('{hmac_key}','123456789') as sh2\".format(hmac_key=hmac_key)\n",
    "    for i in secret_Col:\n",
    "        sh256Col=sh256Col+ \", hmac_sha256('{hmac_key}','\"+i+\"', \"+i+\") \"+ i+\"_x\"\n",
    "df=spark.read.format(\"csv\").load(\"s3://rupiahplus-configs/yanzhengmweishiy_副.csv\")\n",
    "# df.createOrReplaceTempView(\"tempViewName\")  \n",
    "path=\"s3://rupiahplus-configs/yanzhengmweishiy_副_加密.csv\"\n",
    "# spark.sql(\"select _c0, hmac_sha256('xUIAZ3grtPOxaPNK','mobile',_c0) mobile1 from  tempViewName\").write.mode(\"overwrite\").csv(path)\n",
    "df.withColumn(\"mobile\",hmac_sha256(lit(hmac_key),lit(\"mobile\"),F.col('_c0'))).show()\n",
    "\n",
    "# keymap=''\n",
    "# for  row in df:\n",
    "#     keymap=keymap+row['value']\n",
    "# json_content=json.loads(keymap)\n",
    "# return json_content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
