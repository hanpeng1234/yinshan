{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d37f3279394d7192374ac54ca373a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                    Version\n",
      "-------------------------- -------\n",
      "aiobotocore                1.1.2\n",
      "aiohttp                    3.6.3\n",
      "aioitertools               0.7.0\n",
      "async-timeout              3.0.1\n",
      "attrs                      20.2.0\n",
      "beautifulsoup4             4.8.0\n",
      "boto                       2.49.0\n",
      "boto3                      1.15.17\n",
      "botocore                   1.18.17\n",
      "chardet                    3.0.4\n",
      "docutils                   0.15.2\n",
      "fsspec                     0.8.4\n",
      "idna                       2.10\n",
      "idna-ssl                   1.1.0\n",
      "jmespath                   0.9.4\n",
      "lxml                       4.4.1\n",
      "multidict                  4.7.6\n",
      "mysqlclient                1.4.4\n",
      "nltk                       3.4.5\n",
      "nose                       1.3.4\n",
      "numpy                      1.14.5\n",
      "pandas                     0.25.1\n",
      "pip                        20.2.3\n",
      "py-dateutil                2.2\n",
      "python-dateutil            2.8.1\n",
      "python36-sagemaker-pyspark 1.2.4\n",
      "pytz                       2019.2\n",
      "PyYAML                     3.11\n",
      "s3fs                       0.5.1\n",
      "s3transfer                 0.3.3\n",
      "setuptools                 50.3.1\n",
      "six                        1.12.0\n",
      "soupsieve                  1.9.3\n",
      "typing-extensions          3.7.4.3\n",
      "urllib3                    1.25.10\n",
      "wheel                      0.35.1\n",
      "windmill                   1.6\n",
      "wrapt                      1.12.1\n",
      "yarl                       1.5.1"
     ]
    }
   ],
   "source": [
    "sc.list_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c458b3d39974bc0ac5a7d657e4c5e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>2</td><td>application_1602817165865_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-11-195.ap-southeast-1.compute.internal:20888/proxy/application_1602817165865_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-5-234.ap-southeast-1.compute.internal:8042/node/containerlogs/container_1602817165865_0003_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==0.25.1\n",
      "  Using cached pandas-0.25.1-cp36-cp36m-manylinux1_x86_64.whl (10.5 MB)\n",
      "Collecting python-dateutil>=2.6.1\n",
      "  Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.6/site-packages (from pandas==0.25.1) (1.14.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas==0.25.1) (2019.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas==0.25.1) (1.12.0)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-0.25.1 python-dateutil-2.8.1"
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"pandas==0.25.1\")\n",
    "# sc.install_pypi_package(\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d0e289362348d983fded3c777fbcc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 's3://rupiahplus-configs/etl/data_secrt/col.json'\n",
      "Traceback (most recent call last):\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 's3://rupiahplus-configs/etl/data_secrt/col.json'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import  pandas as pd\n",
    "# import  boto3\n",
    "# s3_client = boto3.client('s3')\n",
    "# response = s3_client.get_object(Bucket=\"rupiahplus-configs\",Key=\"etl/data_secrt/col.json\")\n",
    "# file = response[\"Body\"]\n",
    "\n",
    "# pd.read_json(\"s3://rupiahplus-configs/etl/data_secrt/col.json\")\n",
    "import json\n",
    "import pandas as pd\n",
    "data = json.load(open('s3://rupiahplus-configs/etl/data_secrt/col.json'))\n",
    "\n",
    "# df = pd.DataFrame(data[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b123b1301247420bb351a99e818576a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t_admin_audit': ['admin_id', 'access_mode'], 't_auto_review_loan': ['mobile', 'error_message']}"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "# s3://rupiahplus-configs/etl/data_secrt/format.1602818198787.json\n",
    "content_object = s3.Object('rupiahplus-configs', 'etl/data_secrt/col.json')\n",
    "file_content = content_object.get()['Body'].read().decode('utf-8')\n",
    "json_content = json.loads(file_content)\n",
    "print(json_content[\"luzon\"])\n",
    "# >> Something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2111d2a4f084af9beb7672a021d44d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\\nreferenced columns only include the internal corrupt record column\\n(named _corrupt_record by default). For example:\\nspark.read.schema(schema).json(file).filter($\"_corrupt_record\".isNotNull).count()\\nand spark.read.schema(schema).json(file).select(\"_corrupt_record\").show().\\nInstead, you can cache or save the parsed results and then send the same query.\\nFor example, val df = spark.read.schema(schema).json(file).cache() and then\\ndf.filter($\"_corrupt_record\".isNotNull).count().;'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 380, in show\n",
      "    print(self._jdf.showString(n, 20, vertical))\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.AnalysisException: 'Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\\nreferenced columns only include the internal corrupt record column\\n(named _corrupt_record by default). For example:\\nspark.read.schema(schema).json(file).filter($\"_corrupt_record\".isNotNull).count()\\nand spark.read.schema(schema).json(file).select(\"_corrupt_record\").show().\\nInstead, you can cache or save the parsed results and then send the same query.\\nFor example, val df = spark.read.schema(schema).json(file).cache() and then\\ndf.filter($\"_corrupt_record\".isNotNull).count().;'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.json('s3://rupiahplus-configs/etl/data_secrt/format.1602818198787.json')\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
