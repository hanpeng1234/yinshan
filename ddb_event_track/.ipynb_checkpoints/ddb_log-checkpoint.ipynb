{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e92eac7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20a0ea808c44d5cbf2008b92cc3ed3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "   with loan_detail as (\n",
    "     select id as loan_id,android_id,loan_create_time,review_end_time from ys_wide_table.loan loan\n",
    "   where  android_id  is not null \n",
    "   and android_id <> ''\n",
    "   and date(loan_create_time) >=date '2020-12-01'\n",
    "   group by id ,android_id,loan_create_time,review_end_time\n",
    "   order by id,android_id ,loan_create_time desc\n",
    "   )\n",
    "   \n",
    "   \n",
    "   SELECT  diff.android_id ,\n",
    "       diff.page_id,\n",
    "        count(1) AS countNum,\n",
    "         max(diff.difftime)as maxNum ,\n",
    "         min(diff.difftime) AS minNum,\n",
    "         avg(diff.difftime) AS avgNum,\n",
    "         sum(diff.difftime) AS sumNum\n",
    "FROM \n",
    "    (SELECT *,\n",
    "    unix_timestamp(tmp.leave_time,'yyyy-MM-dd HH:mm:ss.SSS')-unix_timestamp(tmp.log_time,'yyyy-MM-dd HH:mm:ss.SSS') as difftime \n",
    "    FROM \n",
    "        (SELECT log.android_id,\n",
    "         log.action_type,\n",
    "         log.runid,\n",
    "         log.log_time ,\n",
    "         log.page_id,\n",
    "         if( LEAD(log.action_type,  1) OVER(partition by log.page_Id ORDER BY  log.log_time)='page_leave_event',LEAD(log.log_time, 1) \n",
    "            OVER(partition by log.page_Id ORDER BY  log.log_time), null) AS leave_time\n",
    " from \n",
    "      loan_detail loan\n",
    " left  join  ddb_event_track_s3.id_adapundi_event_track track\n",
    "   on loan.android_id =track.android_id\n",
    "   and date(timestamp(track.create_time/1000))>= date(loan.loan_create_time)\n",
    "   and date(timestamp(track.create_time/1000))>= if(loan.review_end_time is null ,date(loan.loan_create_time+  interval '1' day),date(loan.review_end_time+  interval '1' day))\n",
    " left join ddb_event_track_s3.id_adapundi_event_track_log log\n",
    "    on track.android_id=log.android_id\n",
    "    and track.run_id=log.runid\n",
    "    and log.action_type IN ('page_entrance_event','page_leave_event')\n",
    "  ORDER BY  log.log_time) tmp\n",
    "  WHERE tmp.leave_time is NOT NULL ) diff\n",
    "    GROUP BY  diff.android_id,diff.page_id\n",
    "\"\"\").write.mode(\"overwrite\").orc(\"s3://rupiahplus-data-warehouse/stream/event_track/adapundi_page_stat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579b1ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "   with loan_detail as (\n",
    "     select id as loan_id,android_id,loan_create_time,review_end_time from ys_wide_table.loan loan\n",
    "   where  android_id  is not null \n",
    "   and android_id <> ''\n",
    "   and date(loan_create_time) >=date '2020-12-01'\n",
    "   group by id ,android_id,loan_create_time,review_end_time\n",
    "   order by id,android_id ,loan_create_time desc\n",
    "   )\n",
    "   \n",
    "   \n",
    " SELECT *,\n",
    "    unix_timestamp(tmp.leave_time,'yyyy-MM-dd HH:mm:ss.SSS')-unix_timestamp(tmp.log_time,'yyyy-MM-dd HH:mm:ss.SSS') as difftime \n",
    "    FROM \n",
    "        (SELECT log.android_id,\n",
    "         log.action_type,\n",
    "         log.runid,\n",
    "         log.log_time ,\n",
    "         log.page_id,\n",
    "         if( LEAD(log.action_type,  1) OVER(partition by log.page_Id ORDER BY  log.log_time)='page_leave_event',LEAD(log.log_time, 1) \n",
    "            OVER(partition by log.page_Id ORDER BY  log.log_time), null) AS leave_time\n",
    " from \n",
    "      loan_detail loan\n",
    " left  join  ddb_event_track_s3.id_adapundi_event_track track\n",
    "   on loan.android_id =track.android_id\n",
    "   and date(timestamp(track.create_time/1000))>= date(loan.loan_create_time)\n",
    "   and date(timestamp(track.create_time/1000))>= if(loan.review_end_time is null ,date(loan.loan_create_time+  interval '1' day),date(loan.review_end_time+  interval '1' day))\n",
    " left join ddb_event_track_s3.id_adapundi_event_track_log log\n",
    "    on track.android_id=log.android_id\n",
    "    and track.run_id=log.runid\n",
    "    and log.action_type IN ('page_entrance_event','page_leave_event')\n",
    "  ORDER BY  log.log_time) tmp\n",
    "  WHERE tmp.leave_time is NOT NULL \n",
    "\"\"\").write.mode(\"overwrite\").orc(\"s3://rupiahplus-data-warehouse/stream/event_track/adapundi_page_event_detail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b9e647",
   "metadata": {},
   "outputs": [],
   "source": [
    " select  * from ys_wide_table.loan loan\n",
    "  left join ddb_event_track_s3.id_adapundi_event_track track\n",
    "   on loan\n",
    "  where  loan_create_time >=date '2020-12-01'\n",
    "  and  date(review_end_time + interval '7' hour) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bcdad33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12438393c3f042c7b975fa5178642efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|loan_create_time|\n",
      "+----------------+\n",
      "|      2019-08-14|\n",
      "+----------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "select date( loan_create_time)  from ys_wide_table.loan limit 1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed224ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns',None)\n",
    "\n",
    "\n",
    "data=pd.read_excel(r'D:\\py_job\\LM\\mode\\other\\log.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "# >> data processing\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data['log_id_deal']=data['log_id']\n",
    "data['log_id_deal']=data['log_id_deal'].str.slice(-3)\n",
    "\n",
    "data['log_id_deal']=data['log_id_deal'].map(lambda x: str(x).lstrip('_').rstrip('_')).astype(float)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data.sort_values(by=['runid','log_time','log_id_deal'],ascending=True,inplace=True)  #默认 axis=0 跨行\n",
    "\n",
    "df=data.reset_index()\n",
    "\n",
    "\n",
    "a=df['runid'].count()\n",
    "\n",
    "\n",
    "temp=[]\n",
    "\n",
    "\n",
    "for i in range(a):\n",
    "  if df['action_type'][i]=='page_entrance_event' and df['action_type'][i+1]=='page_leave_event' and df['runid'][i]==df['runid'][i+1] :\n",
    "      temp.append(df['log_time'][i+1])\n",
    "  else:\n",
    "      temp.append(0)\n",
    "\n",
    "\n",
    "\n",
    "data02=pd.DataFrame(temp,columns=['end_date'])\n",
    "\n",
    "data_new=pd.concat([df,data02],axis=1)              # axis=0代表往跨行（down)，而axis=1代表跨列（across)\n",
    "\n",
    "\n",
    "data_new.to_excel(r'D:\\py_job\\LM\\mode\\other\\test.xlsx',index=False)\n",
    "\n",
    "\n",
    "data_new=data_new.drop(data_new[data_new['end_date']==0].index)\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "data_new['log_time']=pd.to_datetime(data_new['log_time'])\n",
    "data_new['end_date']=pd.to_datetime(data_new['end_date'])\n",
    "\n",
    "data_new['diff_time'] = (data_new['end_date'] - data_new['log_time']).dt.seconds\n",
    "\n",
    "data_new['diff_time'] = round(data_new['diff_time'])\n",
    "\n",
    "data_new.to_excel(r'D:\\py_job\\LM\\mode\\other\\log_duration.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a40b77e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4140d3463c16423a823baee136cba25f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+---------------------------------------------------------------+-------------------------------------------------------------------------------------+\n",
      "|unix_timestamp(2021-02-02 12:12:12.893, yyyy-MM-dd HH:mm:ss.SSS)|unix_timestamp(2021-02-02 12:12:20.33, yyyy-MM-dd HH:mm:ss.SSS)|datediff(CAST(2021-02-02 12:12:12.893 AS DATE), CAST(2021-02-02 12:12:20.33 AS DATE))|\n",
      "+----------------------------------------------------------------+---------------------------------------------------------------+-------------------------------------------------------------------------------------+\n",
      "|                                                      1612267932|                                                     1612267940|                                                                                    0|\n",
      "+----------------------------------------------------------------+---------------------------------------------------------------+-------------------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "  select unix_timestamp('2021-02-02 12:12:12.893','yyyy-MM-dd HH:mm:ss.SSS') ,unix_timestamp('2021-02-02 12:12:20.33','yyyy-MM-dd HH:mm:ss.SSS')  , DATEDIFF('2021-02-02 12:12:12.893','2021-02-02 12:12:20.33')\n",
    "\"\"\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
